\chapter{Rappresentazione dei Dati}
La \emph{rappresentazione dei dati} rappresenta il modo in cui le osservazioni vengono codificate per poter essere analizzate, confrontate e trasformate. Un insieme di variabili descrittive può essere interpretato non solo come una collezione di valori eterogenei, ma come una struttura matematica coerente: ogni osservazione diventa un punto in uno \textbf{spazio delle features}, le cui dimensioni corrispondono alle variabili considerate. Questa prospettiva consente di tradurre concetti qualitativi come somiglianza, variazione o struttura in relazioni geometriche, aprendo la strada all'uso sistematico di tecniche matematiche e computazionali per l'analisi dei dati.

\section{Spazio di feature}
Da qui, si può estrapolare una definizione più formale di \textbf{rappresentazione dei dati}:
\begin{nicequote}
    La rappresentazione dei dati in uno spazio può essere formalizzata come una funzione di rappresentazione
    \[
    f : \mathcal{X} \rightarrow \mathbb{R}^d,
    \]
    che associa a ogni osservazione $x \in \mathcal{X}$ un vettore $f(x)$ appartenente a uno spazio vettoriale detto \emph{spazio delle feature} o \emph{spazio di rappresentazione}.
\end{nicequote}

\subsection{Feature extraction}
Ad esempio, un’immagine digitale può essere inizialmente descritta come una griglia bidimensionale di pixel.
Attraverso una rappresentazione vettoriale, tale immagine può essere interpretata come un punto in uno spazio di dimensione pari al numero di pixel, in cui ogni coordinata corrisponde all’intensità di un singolo pixel.
In questo caso, la feature extraction coincide con l’operazione di \emph{flattening} dell’immagine in un vettore. In contesti più generali, le feature possono invece catturare proprietà più astratte dei dati, come statistiche locali, pattern strutturali o relazioni semantiche, riducendo la dimensionalità e migliorando la separabilità delle osservazioni nello spazio delle feature.

Un aspetto centrale nella rappresentazione dei dati è la scelta delle \emph{feature}. 
Spesso, infatti, i dati grezzi non sono immediatamente adatti a essere interpretati come vettori in uno spazio utile per l’analisi: è necessario trasformarli in una forma più strutturata e informativa. Questo processo prende il nome di \textbf{feature extraction} e consiste nel definire una mappatura che, a partire dai dati originali, produca una rappresentazione vettoriale più compatta o più significativa per il compito considerato.

\begin{nicequote}
    La \emph{feature extraction} può essere formalizzata come una funzione di rappresentazione
    \[
    f : \mathcal{X} \rightarrow \mathbb{R}^d,
    \]
    che associa a ogni osservazione $x \in \mathcal{X}$ un vettore di feature $f(x)$ appartenente allo \emph{spazio delle feature}.
    La scelta della funzione $f$ induce una specifica geometria dello spazio di rappresentazione e determina, di conseguenza, le relazioni di similarità tra le osservazioni.
\end{nicequote}

Gli spazi $\mathbb{R}^d$ (e, più in generale, $\mathbb{R}^m$) vengono indicati come \emph{feature spaces} o \emph{representation spaces}, in quanto spazi vettoriali a cui appartengono le rappresentazioni dei dati, dette anche \emph{feature vectors}.
Il processo di mappatura dei dati mediante una funzione di rappresentazione prende il nome di \emph{data representation}.

\subsection{Proprietà dello spazio di feature}
Poiché le osservazioni $x$ sono vettori che vivono in un certo spazio vettoriale, \textbf{tutte le proprietà} di tali vettori e di tali spazi dell'algebra lineare \textbf{si applicano anche alle osservazioni}. 

\subsubsection*{Norme}
Un esempio di proprietà utili è la possibilità di definire delle \emph{norme} sugli spazi vettoriali, che permettono di misurare la \emph{lunghezza} o la \emph{magnitudine} di un vettore.

\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.7\textwidth]{images/norme.png}
    \caption{Rappresentazione grafica delle norme $L_1$, $L_2$ e $L_\infty$ in una circonferenza unitaria su uno spazio bidimensionale.}
    \label{fig:norme}
\end{figure}

Una norma è una funzione che associa a ogni vettore $v$ un numero reale non negativo, denotato come $\|v\|$, che rappresenta la lunghezza del vettore. Le norme più comuni sono:
\begin{description}
    \item[Norma $L_p$.] - Dato uno spazio vettoriale $S$, una norma $L_p$ è definita come
    \[
    \| x \|_p = \left( \sum_{i=1}^{d} |x_i|^p \right)^{1/p},
    \]
    dove $x$ è un vettore a $d$ dimensioni, $x_i$ è la $i$-esima componente del vettore e $p \geq 1$ è un parametro che determina il tipo di norma.
    \item[Norma Euclidea ($L_2$).] - Utilizzando $p=2$ come parametro della norma $L_p$, si ottiene la norma Euclidea, definita come
    \[
    \| x \|_2 = \left( \sum_{i=1}^{d} x_i^2 \right)^{1/2}.
    \]
    Questa norma misura la distanza "diretta" tra l'origine e il punto rappresentato dal vettore $x$ nello spazio Euclideo.
    \item[Norma Manhattan ($L_1$).] - Utilizzando $p=1$ come parametro della norma $L_p$, si ottiene la norma Manhattan, definita come
    \[
    \| x \|_1 = \sum_{i=1}^{d} |x_i|.
    \]
    Questa norma misura la distanza totale percorsa lungo gli assi cartesiani per raggiungere il punto rappresentato dal vettore $x$.
    \item[Norma di Massimo ($L_\infty$).] - Utilizzando $p \to \infty$ come parametro della norma $L_p$, si ottiene la norma di Massimo, definita come
    \[
    \| x \|_\infty = \max_{i=1,\ldots,d} |x_i|.
    \]
    Questa norma misura la massima distanza lungo una singola dimensione del vettore $x$.
\end{description}

Si possono visualizzare le differenze tra queste norme utilizzando una \textbf{circonferenza unitaria} in uno spazio bidimensionale, come mostrato nella Figura~\ref{fig:norme}.


\section{Metriche}
Le norme servono a definire dei modi di misurare la lunghezza dei vettori, chiamate \emph{distanze}. Un problema delle distanze è che funziona solo su spazi euclidei, mentre in molti casi i dati non vivono in spazi euclidei. Per questo motivo, si utilizzano delle \emph{metriche}, che sono delle funzioni che misurano la distanza tra due punti in uno spazio, indipendentemente dal fatto che lo spazio sia euclideo o meno.

\begin{nicequote}
    Dato uno spazio $S$, una funzione
    \[
    m: S \times S \rightarrow \mathbb{R}
    \]
    è una metrica se soddisfa le seguenti proprietà $\forall x, y, z \in S$:
    \begin{itemize}
        \item \textbf{Non-negatività:} $m(x, y) \geq 0$ e $m(x, y) = 0$ se e solo se $x = y$.
        \item \textbf{Simmetria:} $m(x, y) = m(y, x)$.
        \item \textbf{Disuguaglianza triangolare:} $m(x, z) \leq m(x, y) + m(y, z)$.
    \end{itemize}
\end{nicequote}

\subsection{Metriche euclidee}
Le metriche più comuni sono quelle basate sulle norme, che misurano la distanza tra due punti in uno spazio euclideo. Dalla definizione di norma $L_p$, si può derivare la metrica $L_p$ come:
\[
L_p(x, y) = \| x - y \|_p = \left( \sum_{i=1}^{d} |x_i - y_i|^p \right)^{1/p}
\]

\noindent
Da questo si derivano le metriche:
\begin{description}
    \item[Metrica Euclidea ($L_2$).] - Utilizzando $p=2$ come parametro della metrica $L_p$, si ottiene la metrica Euclidea, definita come
    \[
    L_2(x, y) = \left( \sum_{i=1}^{d} (x_i - y_i)^2 \right)^{1/2}.
    \]
    Questa metrica misura la distanza "diretta" tra i punti rappresentati dai vettori $x$ e $y$ nello spazio Euclideo.
    \item[Metrica Manhattan ($L_1$).] - Utilizzando $p=1$ come parametro della metrica $L_p$, si ottiene la metrica Manhattan, definita come
    \[
    L_1(x, y) = \sum_{i=1}^{d} |x_i - y_i|.
    \]
    Questa metrica misura la distanza totale percorsa lungo gli assi cartesiani per raggiungere il punto rappresentato dal vettore $y$ partendo dal punto rappresentato dal vettore $x$.
    \item[Metrica di Massimo ($L_\infty$).] - Utilizzando $p \to \infty$ come parametro della metrica $L_p$, si ottiene la metrica di Massimo, definita come
    \[
    L_\infty(x, y) = \max_{i=1,\ldots,d} |x_i - y_i|.
    \]
    Questa metrica misura la massima distanza lungo una singola dimensione tra i vettori $x$ e $y$.
\end{description}

\begin{figure}[htbp]
    \centering
    \includegraphics[width=1\textwidth]{images/metrics_comparison.png}
    \caption{Confronto geometrico tra le metriche $L_2$, $L_1$ e $L_\infty$ tra due punti nel piano. 
    La distanza $L_2$ (Euclidea) corrisponde al segmento rettilineo che li congiunge; 
    la distanza $L_1$ (Manhattan) è ottenuta come somma di spostamenti lungo gli assi coordinati; 
    la distanza $L_\infty$ (Chebyshev) dipende esclusivamente dalla massima differenza lungo una delle coordinate.}
    \label{fig:metriche}
\end{figure}

\subsection{Distanza del coseno}
Un'altra metrica molto utilizzata, soprattutto in ambito di elaborazione del linguaggio naturale e recupero delle informazioni, è la \textbf{distanza del coseno}. Questa metrica misura la dissimilarità tra due vettori in termini dell'angolo tra di essi, piuttosto che della loro distanza euclidea. Questo viene fatto perché nella distanza euclidea si utilizzano i valori, nella distanza del coseno invece si considerano le loro coordinate

La similarità del coseno tra due vettori $x$ e $y$ è definita come:
\[
S_c(x, y) = \frac{x \cdot y}{\|x\| \|y\|},
\]
dove $x \cdot y$ è il prodotto scalare tra i vettori $x$ e $y$, e $\|x\|$ e $\|y\|$ sono le loro norme Euclidee. La distanza del coseno varia tra 0 (quando i vettori sono identici) e 2 (quando i vettori sono opposti).

La distanza del coseno misura l'angolo $\theta$ tra i due vettori e restituisce un valore compreso tra -1 e 1 dove:
\begin{itemize}
    \item 1 indica che i vettori sono identici (angolo di 0 gradi),
    \item 0 indica che i vettori sono ortogonali (angolo di 90 gradi),
    \item -1 indica che i vettori sono opposti (angolo di 180 gradi).
\end{itemize}

Dalla similarità del coseno, si può derivare la distanza del coseno come:
\[
D_c(x, y) = 1 - S_c(x, y) = 1 - \frac{x \cdot y}{\|x\| \|y\|}.
\]

Questa misura è utile quando la scala non è importante, ma lo è la direzione. Ad esempio, in applicazioni di elaborazione del linguaggio naturale, due documenti possono essere considerati simili se trattano argomenti simili, indipendentemente dalla loro lunghezza o dal numero di parole utilizzate.

\section{Feature e funzioni di rappresentazione}
Grazie alla rappresentazione dei dati in spazi vettoriali, è possibile applicare una vasta gamma di tecniche matematiche e computazionali per l'analisi dei dati per estrarre informazioni utili, identificare pattern nascosti, applicare modelli di Machine Learning a spazi non lineari e molto altro ancora.

\begin{figure}
    \centering
    \includegraphics[width=1\textwidth]{images/polar_coordinates_example.png}
    Esempio di come una diversa rappresentazione dei dati possa rendere più semplice un problema di separazione. 
    A sinistra, i dati sono rappresentati in coordinate cartesiane $(x,y)$: le due classi (\emph{ball} e \emph{ring}) non risultano linearmente separabili. 
    A destra, la stessa informazione è espressa in coordinate polari $(\theta, r)$: utilizzando il raggio come feature principale, le due classi diventano facilmente separabili. 
    Questo esempio mostra come una funzione di rappresentazione appropriata possa semplificare significativamente la struttura geometrica dei dati.
    \label{fig:polar_coordinates_example}
\end{figure}

\subsection{Feature extraction ''Black Box''}
In molti casi, la funzione di rappresentazione $f$ non è esplicitamente definita, come nell'esempio in figura \ref{fig:polar_coordinates_example}, ma viene appresa automaticamente dai dati stessi attraverso tecniche di \emph{feature learning} o \emph{representation learning}. Questo è il moderno \emph{machine learning}, chiamato \textbf{Deep Learning}, che utilizza reti neurali profonde per apprendere rappresentazioni complesse e gerarchiche dei dati, spesso superando le prestazioni delle tecniche di feature extraction manuali.

Alcuni dei (giganteschi) modelli di Deep Learning possono fare feature extraction su:
\begin{description}
    \item[Testo:] I modelli di linguaggio, o LLM (Large Language Models), come GPT-4, BERT e altri, sono in grado di apprendere rappresentazioni semantiche profonde del testo, catturando il significato, il contesto e le relazioni tra le parole.
    \item[Immagini:] Le reti neurali convoluzionali (CNN) sono ampiamente utilizzate per l'analisi delle immagini, apprendendo feature gerarchiche che vanno dai bordi e texture di basso livello fino a oggetti e scene complesse. Alcuni esempi sono le ResNet o le Vision Transformer (ViT).
    \item[Audio:] Le reti neurali ricorrenti (RNN) e le architetture basate su Transformer sono utilizzate per l'elaborazione del segnale audio, catturando pattern temporali e caratteristiche spettrali utili per il riconoscimento vocale, la classificazione musicale e altre applicazioni.
\end{description}

In tutti i casi i principi sono gli stessi elencati sopra: i dati vengono mappati in uno spazio di feature tramite una funzione di rappresentazione, e le proprietà di questo spazio (come le metriche) vengono utilizzate per analizzare e interpretare i dati.

\subsection{DPI: Data Processing Inequality}
La \textbf{Data Processing Inequality} (DPI) è un principio fondamentale nella teoria dell'informazione che afferma che l'elaborazione dei dati non può aumentare la quantità di informazione che essi contengono riguardo a una variabile di interesse. In altre parole, qualsiasi trasformazione o manipolazione dei dati non può migliorare la loro capacità di fornire informazioni utili su una variabile target.

Matematicamente, se abbiamo una catena di Markov $X \rightarrow Y \rightarrow Z$, dove $Z$ è condizionalmente indipendente da $X$ dato $Y$, allora l'informazione mutua\footnote{Per inforrmazione mutua si intende una misura della quantità di informazione condivisa tra due variabili casuali.} soddisfa la seguente disuguaglianza:
\[
I(X; Y) \geq I(X; Z)
\]
dove $I(X; Y)$ è l'informazione mutua tra le variabili $X$ e $Y$. Questa disuguaglianza implica che l'informazione che $Y$ fornisce su $X$ è sempre maggiore o uguale a quella che $Z$ fornisce su $X$.

Questo principio ha importanti implicazioni per la rappresentazione dei dati e la feature engineering:
\begin{itemize}
    \item \textbf{Perdita di informazione:} Ogni trasformazione dei dati, come la riduzione della dimensionalità o la selezione delle feature, può comportare una perdita di informazione. È essenziale valutare attentamente le trasformazioni per garantire che non compromettano la capacità dei dati di rappresentare la variabile di interesse.
    \item \textbf{Bottleneck informativo:} La DPI suggerisce che esiste un limite superiore alla quantità di informazione che può essere estratta dai dati attraverso qualsiasi processo di elaborazione. Questo concetto è cruciale nella progettazione di modelli di machine learning, poiché indica che non è possibile ottenere prestazioni migliori semplicemente aumentando la complessità del modello senza considerare la qualità e la quantità delle informazioni nei dati.
    \item \textbf{I vantaggi delle feature apprese:} Le tecniche di apprendimento delle rappresentazioni, come il deep learning, mirano a trovare trasformazioni dei dati che preservano quanta più informazione possibile riguardo alla variabile target, sfruttando la DPI per guidare la progettazione delle architetture dei modelli.
\end{itemize}
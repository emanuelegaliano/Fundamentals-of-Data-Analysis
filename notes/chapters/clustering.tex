\chapter{Clustering}
Durante l'osservazione di dataset complessi potrebbe essere utile capire le strutture \textbf{intrinseche} presenti nei dati. Un approccio per determinare se i dati possono essere suddivisi in gruppi distinti è il \emph{clustering}, una tecnica di unsupervised learning che mira a raggruppare i dati in cluster basati su somiglianze o distanze tra i punti dati.

\section{Definizione del problema}
Il clustering cerca di suddividere un insieme di dati in sottogruppi (cluster) in modo tale che i punti all'interno dello stesso cluster siano più simili tra loro rispetto a quelli appartenenti a cluster diversi. La somiglianza può essere misurata utilizzando varie metriche, come la distanza euclidea, la distanza di Manhattan o altre misure specifiche del dominio. 

\noindent
Sia un set di osservazioni:
\[
\mathbf{X} = \{x^{(i)}\}^N_{i=1}, \quad \mathbf{x}^{(i)} \in \mathbb{R}^n
\]

\noindent
Il compito del clustering è quello di dividere $\mathbf{X}$ in $K$ clusters (gruppi):
\[
S = \{S_1, S_2, \ldots, S_K\}
\]
in modo tale che:
\[
\forall \mathbf{x}^{(i)} \in \mathbf{X}, \exists ! S_j \in S : x^{(i)} \in S_j
\]
ovvero ogni punto dati appartiene esattamente a un cluster. Il numero $K$ di cluster è spesso un \textbf{iperparametro} che deve essere scelto prima dell'analisi.

\section{K-means Clustering}
Un algoritmo molto popolare di clustering è il \emph{K-means}. L'algoritmo K-means funziona iterativamente per assegnare ogni punto dati al cluster più vicino e aggiornare i centroidi dei cluster fino a quando le assegnazioni non cambiano più.

Per prima cosa si definiscono $K$ differenti vettori $\mu_k \in \mathbb{R}^n$ che rappresentano i centroidi dei cluster. Dopo si definiscono $N \times K$ variabili binarie $r_{ij}$ che ci permettono di mappare ogni punto dati al cluster più vicino:
\[
r_{ij} = \begin{cases}
1 & \text{se } x^{(i)} \text{ è assegnato al cluster } j \\
0 & \text{altrimenti}
\end{cases}
\]

Per rendere i cluster il più compatti possibile, l'algoritmo K-means definisce come \textbf{funzione di costo}, chiamata anche \textbf{distortion function}, la seguente espressione:
\[
J = \sum_{j=1}^K \sum_{i=1}^N r_{ij} \| x^{(i)} - \mu_j \|^2
\]
La funzione $J$ è anche definita \textbf{WCSS}: \emph{(Within-Cluster Sum of Squares)}.
Per un singolo cluster $S_j$, il termine interno della somma, \(\sum_{i=1}^N r_{ij} \| x^{(i)} - \mu_j \|^2\), rappresenta la somma delle distanze quadratiche tra ogni punto dati assegnato al cluster \(j\) e il centroide del cluster \(\mu_j\). Questo termine misura quanto i punti all'interno del cluster sono vicini al loro centroide, una misura dell'\emph{inerzia}\footnote{Per inerzia, in questo contesto, si intende la funzione di costo.} del cluster, contribuendo così alla compattezza del cluster.

L'obiettivo dell'algoritmo K-means è quello di trovare un set di centroidi $ \{ \mu_j \}$ e di assegnamenti $ \{ r_{ij} \} $ che minimizzano l'inerzia $J$:
\[
\hat{S} = \{\hat{r}_{ij}\}_{ij}, \{\hat{\mu}_j\}_j = \arg \min J
\]

\subsection{Ottimizzazione}
L'algoritmo K-means utilizza un approccio di ottimizzazione iterativa basato su due passaggi principali: l'assegnazione dei cluster e l'aggiornamento dei centroidi.

\begin{description}
    \item[Assegnazione.] - Per ogni punto dati \(x^{(i)}\), si assegna il punto al cluster il cui centroide è più vicino, calcolando la distanza tra il punto e ciascun centroide e scegliendo quello con la distanza minima:
    \[
    r_{ij} = \begin{cases}
    1 & \text{se } j = \arg \min_k \| x^{(i)} - \mu_k \|^2 \\
    0 & \text{altrimenti}
    \end{cases}
    \]
    Questo genererà l'insieme di cluster $S_i$:
    \[
    S_j = \{ \mathbf{x} \in \mathbf{X}: \|\mathbf{x} - \mu_j \|^2 \leq \|\mathbf{x} - \mu_k \|^2, \forall k \neq j \}
    \]
    \item[Aggiornamento.] - Una volta che tutti i punti dati sono stati assegnati ai cluster, si aggiornano i centroidi calcolando la media di tutti i punti assegnati a ciascun cluster:
    \[
    \mu_j = \frac{1}{|S_j|} \sum_{x \in S_j} \mathbf{x} = \frac{\sum_{i=1}^N r_{ij} x^{(i)}}{\sum_{i=1}^N r_{ij}}
    \]
    Questo processo di assegnazione e aggiornamento viene ripetuto fino a quando le assegnazioni dei cluster non cambiano più o fino a quando la riduzione della funzione di costo \(J\) diventa trascurabile.
\end{description}

\subsection{Scegliere il giusto K}
Nell'algoritmo K-means, il valore di $K$ (il numero di cluster) è un iperparametro che deve essere scelto prima dell'esecuzione dell'algoritmo. Esistono due tecniche utilizzate comunemente per determinare il valore ottimale di $K$:
\begin{description}
    \item[Metodo del gomito (Elbow Method).] - Questo metodo prevede di eseguire l'algoritmo K-means per una gamma di valori di $K$ e calcolare la somma delle distanze quadratiche all'interno dei cluster (WCSS) per ciascun valore di $K$. Si traccia quindi un grafico di WCSS in funzione di $K$. Il punto in cui la riduzione di WCSS inizia a diminuire significativamente (formando un "gomito" nel grafico) è considerato il valore ottimale di $K$.
    \begin{figure}[htbp]
        \centering
        \includegraphics[width=1\textwidth]{images/elbow_method.png}
        \caption{Esempio del metodo del gomito per scegliere il numero ottimale di cluster \(K\).}
        \label{fig:elbow_method}
    \end{figure}
    \item[Metodo Silhouette.] - Questo metodo valuta la qualità del clustering calcolando il coefficiente di silhouette per ogni punto dati. Il coefficiente di silhouette misura quanto un punto è ben assegnato al suo cluster rispetto ai cluster vicini. Si calcola la silhouette media per diversi valori di $K$ e si sceglie il valore di $K$ che massimizza questa media.
    
    Per fare questo, per un certo punto dati \(i\) assegnato al cluster $C_I$, viene calcolato lo score:
    \[
    a(i) = \frac{1}{|C_I| - 1} \sum_{j \in C_I, j \neq i} d(i, j)
    \]
    dove \(d(i, j)\) è la distanza tra i punti \(i\) e \(j\). Questo rappresenta la distanza media tra il punto \(i\) e tutti gli altri punti nel suo cluster, che deve rimanere piccolo se il cluster ha una poca varianza (cluster compatto).
    
    Successivamente, comparando il risultato $a(i)$ con un cluster diverso $C_J$, si calcola:
    \[
    b(i) = \min_{J \neq I} \frac{1}{|C_J|} \sum_{j \in C_J} d(i, j)
    \]
    che rappresenta la distanza media tra il punto \(i\) e tutti i punti nel cluster più vicino. Infine, il coefficiente di silhouette per il punto \(i\) è calcolato come:
    \[
    s(i) = 
    \begin{cases}
    \frac{b(i) - a(i)}{\max(a(i), b(i))} & \text{se } |C_I| > 1 \\
    0 & \text{altrimenti}
    \end{cases}
    \]
    Il risultato ottenuto, $-1 \leq s(i) \leq 1$, indica quanto bene il punto è stato assegnato al suo cluster: valori vicini a 1 indicano una buona assegnazione, valori vicini a 0 indicano che il punto è sul confine tra due cluster, e valori negativi indicano che il punto potrebbe essere stato assegnato al cluster sbagliato.
\end{description}
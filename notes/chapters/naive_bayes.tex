\chapter{Classificatori generativi}
I classificatori generativi sono modelli probabilistici che cercano di modellare la distribuzione congiunta delle caratteristiche e delle etichette di classe, ovvero \( P(X, Y) \). Questi modelli assumono che i dati siano generati da un processo probabilistico e cercano di stimare questa distribuzione per effettuare previsioni sulle etichette di classe.

A differenza dei modelli discriminativi, che si concentrano direttamente sulla modellazione della probabilità condizionata \( P(Y | X) \), i classificatori generativi cercano di comprendere come i dati vengono generati. Una volta che la distribuzione congiunta è stata stimata, è possibile utilizzare il teorema di Bayes per calcolare la probabilità condizionata delle etichette di classe dato le caratteristiche.

\section{MAP: Maximum A Posteriori}
Il criterio principale utilizzato nei classificatori generativi è il criterio MAP (Maximum A Posteriori). Questo criterio mira a massimizzare la probabilità a posteriori delle etichette di classe dato le caratteristiche osservate. Dobbiamo trovare la classe $k$ che è più probabile dopo aver osservato i dati $\mathbf{x}$:
\[
h(\mathbf{x}) = \arg \max_{k} P(Y = k | X = \mathbf{x})
\]
Utilizzando il teorema di Bayes, possiamo riscrivere questa espressione come:
\[
h(\mathbf{x}) = \arg \max_{k} \frac{P(X = \mathbf{x} | Y = k) P(Y = k)}{P(X = \mathbf{x})}
\]
Visto che siamo interessati a massimizzare rispetto a $k$, possiamo ignorare il denominatore \( P(X = \mathbf{x}) \) che è costante per tutte le classi. Quindi, il criterio MAP diventa:
\[
h(\mathbf{x}) \propto \arg \max_{k} P(X = \mathbf{x} | Y = k) P(Y = k)
\]
da cui abbiamo:
\[
h\left( \mathbf{x} \right) = \arg \max_{k} \underbrace{P( X | Y=k )}_{\text{Likelihood}} \underbrace{P(Y=k)}_{\text{Prior}}
\]
ovvero scegliamo la classe che massimizza il prodotto tra la probabilità di osservare i dati dato la classe (likelihood) e la probabilità a priori della classe (prior).

\subsection{Probabilità a priori}
Ricordando, dal \emph{teorema di Bayes}, che la probabilità a priori \( P(Y = k) \) rappresenta la nostra conoscenza iniziale sulla distribuzione delle classi prima di osservare i dati. Questa probabilità può essere stimata dalla frequenza relativa delle classi nel set di addestramento:
\[
P(Y = k) = \frac{\text{Numero di istanze della classe } k}{\text{Numero totale di istanze}}
\]

\subsection{Likelihood}
Sempre dal \emph{teorema di Bayes}, la likelihood \( P(X = \mathbf{x} | Y = k) \) rappresenta la probabilità di osservare i dati \( \mathbf{x} \) dato che sappiamo che appartengono alla classe \( k \). Se ripetiamo questo processo per tutte le classi \( k \), possiamo confrontare queste probabilità per determinare quale classe è più probabile dato i dati osservati abbiamo stimato la probabilità $P(X \mid Y=k)$ come:
\[
P(X = x \mid Y = k) = P(X_k)
\]

Per stimare questa probabilità $P(X_k)$ si assumono diverse assunzioni sulla distribuzione dei dati all'interno di ciascuna classe. In base a queste assunzioni, creiamo dei modelli differenti per rappresentare la distribuzione dei dati.

\section{Il problema della likelihood}
Un problema comune nei classificatori generativi è la stima accurata della likelihood \( P(X = \mathbf{x} | Y = k) \), specialmente quando le caratteristiche sono numerose o quando i dati sono scarsi. Infatti anche questa stima soffre della \emph{curse of dimensionality} (maledizione della dimensionalità), che rende difficile ottenere stime affidabili delle distribuzioni di probabilità in spazi ad alta dimensione.

Pensiamo all'esempio dello \emph{spam}:  ipotizziamo di avere un dizionario composto da 10.000 parole e di voler stimare la probabilità che una email sia spam o meno. Per fare questo, dovremmo stimare la probabilità di tutte le possibili combinazioni di parole, ovvero \( 2^{10000} \) combinazioni, che diventa computazionalmente costoso. Inoltre, con un numero limitato di esempi di addestramento, molte di queste combinazioni potrebbero non essere mai state osservate, portando a stime di probabilità nulle o inaffidabili.

\subsection{Il modello ideale nei dati discreti}
Se le feature da analizzare sono discrete e assumono un numero limitato di valori, possiamo provare a costruire una tabella di contingenza per ogni classe \( k \). Questa tabella conterrebbe la frequenza di ogni combinazione possibile di valori delle caratteristiche all'interno della classe \( k \). La likelihood \( P(X = \mathbf{x} | Y = k) \) può essere stimata come la frequenza relativa di ciascuna combinazione di caratteristiche nella tabella di contingenza.

Il problema principale di questo approccio è che il numero di combinazioni possibili cresce esponenzialmente con il numero di caratteristiche. Ad esempio, se abbiamo \( d \) caratteristiche binarie, ci sono \( 2^d \) combinazioni possibili. Questo rende impraticabile la costruzione di tabelle di contingenza per dataset con molte caratteristiche, poiché richiederebbe una quantità enorme di dati per ottenere stime affidabili delle probabilità.

\subsection{Il modello ideale nei dati continui}
Nel caso di feature continue, non si può costruire una tabella di contingenza. L'approccio "ideale" sarebbe stimare la probabilità \( P(X = \mathbf{x} | Y = k) \) utilizzando una \textbf{distribuzione gaussiana multivariata} $N$.

\paragraph{QDA: Quadratic Discriminant Analysis.}
La distribuzione gaussiana multivariata più flessibile è quella che permette a ogni classe di avere la propria matrice di covarianza. Questo modello è noto come \emph{Quadratic Discriminant Analysis} (QDA). Come assunzioni ogni classe $k$ ha il suo vettore di medie $\mu_k$ e la sua matrice di covarianza $\Sigma_k$. Il problema però, rimane la stima della matrice di covarianza, che richiede un numero elevato di dati per essere stimata accuratamente, specialmente in spazi ad alta dimensione. In particolare, per $d$ caratteristiche, la matrice di covarianza ha \( \frac{d(d+1)}{2} \) parametri da stimare per ogni classe e ne dobbiamo stimare una per ogni classe $k$. Il risultato, tuttavia, è un classificatore potente che può catturare relazioni complesse tra le caratteristiche.

\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.7\textwidth]{images/qda.png}
    \caption{Esempio di classificazione tramite \emph{Quadratic Discriminant Analysis} (QDA) applicato alle caratteristiche dei petali del dataset Iris. Le diverse regioni colorate rappresentano le aree dello spazio in cui il classificatore assegna la classe più probabile sulla base delle distribuzioni gaussiane stimate per ciascuna classe. Le ellissi mostrano le isodensity delle gaussiane apprese, che evidenziano le differenti varianze e correlazioni tra le feature: a differenza della LDA, la QDA permette covariance matrices differenti per ciascuna classe, producendo confini decisionali curvi e non lineari.}
    \label{fig:qda}
\end{figure}

\paragraph{LDA: Linear Discriminant Analysis.}
Per semplificare il modello dobbiamo cercare di ridurre il numero di parametri da stimare. Una delle assunzioni più comuni è quella di considerare che tutte le classi condividano la stessa matrice di covarianza, ovvero $\Sigma_k = \Sigma$ per ogni classe $k$. Questo modello è noto come \emph{Linear Discriminant Analysis} (LDA).

Questo modello è un modello molto più semplice e stabile rispetto al QDA, poiché riduce il numero di parametri da stimare. Comunque computazionalmente rimane costoso, in quanto richiede di stimare una matrice di covarianza di dimensione \( d \times d \), che può essere problematico in spazi ad alta dimensione (es. immagini).

\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.7\textwidth]{images/lda.png}
    \caption{Esempio di classificazione ottenuta tramite LDA (Linear Discriminant Analysis) sul dataset Iris utilizzando due caratteristiche dei petali. Le regioni colorate rappresentano le aree dello spazio delle caratteristiche assegnate a ciascuna classe dal modello lineare. Le curve ellittiche mostrano le distribuzioni gaussiane con covarianza condivisa stimate per ciascuna classe, mentre le linee di separazione evidenziano le \emph{decision boundary} lineari tipiche dell'LDA.}
    \label{fig:lda}
\end{figure}

\section{Naive Bayes}
Il modello di \emph{Naive Bayes} rappresenta un'ulteriore semplificazione rispetto all'LDA. LDA funzionava bene semplificando il modello, ma comunque richiedeva la stima di una matrice di covarianza \( d \times d \). Naive Bayes fa un'assunzione: le caratteristiche sono condizionatamente indipendenti dato la classe. Questa assunzione è spesso irrealistica, ma semplifica enormemente il modello e riduce il numero di parametri da stimare.

\subsection{Assunzione di indipendenza condizionata}
Ricordando che il problema è stimare la probabilità \( P(X = \mathbf{x} | Y = k) \). Con l'assunzione di indipendenza condizionata, possiamo riscrivere questa probabilità come il prodotto delle probabilità marginali delle singole caratteristiche:
\begin{align*}
  &P(X = \mathbf{x} \mid Y = k) \\
= &P(X_1, X_2, \dots, X_n \mid Y = k) \\
= &P(X_1, \ldots, X_n, Y = k) \cdot P(X_2 \mid X_1, Y = k) \ldots P(X_n \mid X_1, X_2, \dots, X_{n-1}, Y = k) 
\end{align*}
Dove \( X_j \) rappresenta la \( j \)-esima caratteristica del vettore \( \mathbf{x} \). Tuttavia, questa fattorizzazione non è sempre utile perché la probabilità $P(X_i \mid X_1, \ldots, X_{i-1}, C)$ è condizionata da tutte le altre feature.

Assumiamo quindi \textbf{l'indipendenza condizionata} delle features data la classe $Y = k$, ovvero:
\[
X_i \perp X_j \mid Y = k \quad \forall i \neq j
\]

Sapendo questo, possiamo scrivere:
\[
X \perp Y \mid Z \Leftrightarrow P(X, Y \mid Z) = P(X \mid Z) P(Y \mid Z)
\]
quindi segue che, sotto l'assunzione di indipendenza condizionata:
\[
P(X_1, \ldots, X_n \mid Y=k) = P(X_1 \mid Y) \cdot P(X_2 \mid Y) \ldots P(X_n \mid Y)
= \prod_{j=1}^{n} P(X_j \mid Y=k)
\]

\noindent
Quindi, possiamo riscrivere la classificazione MAP come:
\[
f(\mathbf{x}) = \arg \max_{k} P(\mathbf{x_1} \mid Y=k) \cdot P(\mathbf{x_2} \mid Y=k) \ldots P(\mathbf{x_n} \mid Y=k) \cdot P(Y=k)
\]

\paragraph{Varianti del modello Naive Bayes.}
Il classificatore Naïve Bayes si basa sulla modellazione delle probabilità condizionate dei singoli attributi $P(X_i \mid Y=k)$, che risultano semplici da trattare grazie all'assunzione di indipendenza tra le variabili. A seconda della natura dei dati, queste distribuzioni possono essere modellate in modi differenti. Se si assume che ogni $X_i$ segua una distribuzione Gaussiana all'interno di ciascuna classe, il modello prende il nome di \emph{Gaussian Naïve Bayes}. Al contrario, quando le variabili rappresentano conteggi o categorie discreti, è più appropriato adottare una distribuzione multinomiale, ottenendo il modello \emph{Multinomial Naïve Bayes}. La scelta della distribuzione dipende quindi dal tipo di dati e dal contesto applicativo.

\section{Naive Bayes Gaussiano}
Nel caso di feature continue, una scelta comune è quella di assumere che ogni caratteristica \( X_j \) segua una distribuzione gaussiana all'interno di ciascuna classe \( k \). In questo caso, la probabilità condizionata \( P(X_j | Y = k) \) può essere modellata come:
\[
P(X_j = x_j | Y = k) = \frac{1}{\sqrt{2 \pi \sigma_{jk}^2}} \exp \left( -\frac{(x_j - \mu_{jk})^2}{2 \sigma_{jk}^2} \right)
\]
dove \( \mu_{jk} \) e \( \sigma_{jk}^2 \) sono la media e la varianza della caratteristica \( X_j \) nella classe \( k \).

Ipotizziamo l'esempio di un dover classificare in base a $X = [H, W]$ (altezza e peso) se una persona è maschio o femmina. Se assumiamo che i dati seguono una distribuzione gaussiana, le porbabilità $P(H \mid Y = k)$ e $P(W \mid Y = k)$ (ovvero le likelihoods) possono essere modellate con gaussiane separateper ogni classe. Da qui otteniamo:
\begin{itemize}
    \item $H_1$: le altezze di un soggetto quando $k=1$ (maschio).
    \item $H_0$: le altezze di un soggetto quando $k=0$ (femmina).
    \item $W_1$: i pesi di un soggetto quando $k=1$ (maschio).
    \item $W_0$: i pesi di un soggetto quando $k=0$ (femmina).
\end{itemize}

\noindent
Ulteriormente, possiamo definire le gaussiane come:
\begin{itemize}
    \item $P(H = h \mid Y = 1) = N(h; \mu_{H1}, \sigma_{H1}^2)$
    \item $P(H = h \mid Y = 0) = N(h; \mu_{H0}, \sigma_{H0}^2)$
    \item $P(W = w \mid Y = 1) = N(w; \mu_{W1}, \sigma_{W1}^2)$
    \item $P(W = w \mid Y = 0) = N(w; \mu_{W0}, \sigma_{W0}^2)$
\end{itemize}

Ovvero le varia distribuzioni gaussiane per ogni caratteristica e classe. Applicando dopo la regola di classificazione, possiamo dire che un nuovo soggetto con altezza $h$ e peso $w$ viene classificato come maschio se:
\[
P(H = h \mid Y = 1)  P(W = w \mid Y = 1)  P(Y = 1) > P(H = h \mid Y = 0) P(W = w \mid Y = 0) P(Y = 0)
\]
altrimenti viene classificato come femmina.

Questa rappresentazione può essere visualizzata nella figura \ref{fig:naive_bayes_gaussian}.

\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.8\textwidth]{images/naive_bayes_gaussian.png}
    \caption{Distribuzioni marginali delle caratteristiche utilizzate in un classificatore Naïve Bayes. La curva rossa rappresenta la distribuzione della variabile \textit{peso} condizionata alla classe $C$ (\(P(W \mid C)\)), mentre la curva verde rappresenta la distribuzione della variabile \textit{altezza} condizionata alla stessa classe (\(P(H \mid C)\)). Il grafico mostra come le due variabili vengano trattate come indipendenti nel modello, permettendo di modellare separatamente le loro densità anche in presenza di una forte correlazione apparente nei dati osservati.}
    \label{fig:naive_bayes_gaussian}
\end{figure}

\subsection{Implicazioni delle assunzioni di Naive Bayes gaussiano}
Il modello di Naive Bayes gaussiano, pur essendo semplice ed efficiente, si basa su alcune assunzioni che possono non riflettere accuratamente la realtà dei dati. In particolare, l'assunzione di indipendenza condizionata tra le caratteristiche dato la classe può essere irrealistica in molti contesti, specialmente quando le caratteristiche sono fortemente correlate. Ad esempio, nel caso di dati biometrici come altezza e peso, queste due variabili tendono a essere correlate, e l'assunzione di indipendenza potrebbe portare a stime di probabilità imprecise.

Generalmente questo porta ad avere una matrice di covarianza \emph{diagonale}, dove le covarianze sono 0 e le varianze sono sugli elementi diagonali. 

\[
\Sigma_k = 
\begin{bmatrix}
\sigma_{1k}^2 & 0 & \cdots & 0 \\
0 & \sigma_{2k}^2 & \cdots & 0 \\
\vdots & \vdots & \ddots & \vdots \\
0 & 0 & \cdots & \sigma_{dk}^2
\end{bmatrix}
\]

Questo implica che il modello non cattura le relazioni lineari tra le caratteristiche, limitando la sua capacità di modellare dati complessi. Come risultato, abbiamo che le \emph{decision boundary} risultano essere \textbf{lineari}.

Questo significa che, pur mantenendo l'assunzione di indipendenza (e dunque una matrice di covarianza priva di termini fuori diagonale), il modello consente comunque a ciascuna classe di avere varianze diverse per ogni caratteristica. In altre parole, nonostante la struttura diagonale sia la stessa per tutte le classi, i valori sulle diagonali possono cambiare da classe a classe. Questa flessibilità fa sì che le distribuzioni gaussiane associate alle diverse classi possano avere “larghezze” differenti lungo ciascun asse, producendo così confini decisionali che non sono più puramente lineari. Le decision boundary risultanti possono quindi curvarsi in base alle differenze di varianza tra le classi, introducendo una forma di non-linearità, pur rimanendo vincolate dall'impossibilità di modellare covarianze tra le variabili.

\subsection{Confronto dei decision boundaries}
Nella figura \ref{fig:decision_boundaries} sono mostrati i confini decisionali (decision boundaries) di tre diversi classificatori generativi: QDA, LDA e Naive Bayes gaussiano.

Si può osservare come le diverse assunzioni sui modelli di covarianza influenzino la forma dei confini decisionali. QDA, con la sua matrice di covarianza completa e differente per ogni classe, produce confini decisionali non lineari e flessibili. LDA, con la sua matrice di covarianza condivisa, genera confini lineari più semplici. Infine, il Naive Bayes gaussiano, con la sua matrice di covarianza diagonale, mantiene confini lineari ma con una struttura che riflette le varianze specifiche di ogni classe.

\begin{figure}[htbp]
    \centering
    \includegraphics[width=\textwidth]{images/decision_boundaries.png}
    \caption{Confronto tra tre modelli generativi per la classificazione multiclasse sul dataset \textit{Iris}. 
    A sinistra è mostrato il modello QDA, che utilizza matrici di covarianza \emph{complete} e differenti per ciascuna classe: ciò permette di modellare forme ellittiche con orientamenti e ampiezze diverse, producendo confini decisionali nettamente \textbf{non lineari}. 
    Al centro è riportato il modello LDA, che assume una matrice di covarianza \emph{condivisa} tra tutte le classi: le distribuzioni risultano quindi ellissi con la stessa forma e orientamento, e i confini decisionali diventano necessariamente \textbf{lineari}. 
    A destra è illustrato il Gaussian Naive Bayes, che assume una matrice di covarianza \emph{diagonale} e diversa per ciascuna classe: ogni classe presenta ellissi allineate con gli assi, e i confini decisionali rimangono \textbf{lineari}, ma organizzati secondo regioni che riflettono le varianze specifiche di ogni classe. 
    Questo confronto evidenzia come le diverse assunzioni sulla struttura della covarianza influenzino profondamente la flessibilità del modello e la forma dei confini decisionali.}
    \label{fig:decision_boundaries}
\end{figure}

\paragraph{Perché il decision boundary di Naive Bayes è lineare?}
Nonostante nell'immagine si veda una certa curvatura nei confini decisionali del Naive Bayes gaussiano, è importante chiarire che, in realtà, i confini decisionali di questo modello sono \textbf{linearmente separabili} nello spazio delle caratteristiche originali. Questo significa che, sebbene possano apparire curvi a causa della rappresentazione grafica o della scala degli assi, matematicamente possono essere descritti come iperpiani lineari. Questo avviene perché il modello assume che le caratteristiche siano indipendenti dato la classe, il che porta a una forma di decision boundary che può essere espressa come una combinazione lineare delle caratteristiche.

Inoltre, l'unico modo di rappresentarli linearmente sarebbe quello di imporre che tutte le varianze siano uguali:
\[
\sigma_{jk}^2 = \sigma_{j} \quad \forall k
\]
e questo porterebbe a confini decisionali effettivamente lineari e paralleli tra loro, come in LDA (infatti in questo caso la matrice di covarianza sarebbe la stessa per tutte le classi e diagonale).

\section{Naive Bayes Multinomiale}
Nel caso di feature discrete, si utilizza (comunemente) il modello di \emph{Naive Bayes Multinomiale}. Ricordando che la distribuzione multinomiale è una generalizzazione della distribuzione binomiale per più di due categorie, e che modella la probabilità di ottenere esattamente $(x_1, \ldots x_d)$ successi per ogni dei $d$ possibili risultati in una sequenza di $n$ prove indipendenti, dove ogni prova ha una delle $d$ categorie ognuna delle quali con probabilità $(p_{k1}, p_{k2}, \ldots, p_{kd})$. Da qui:
\begin{itemize}
    \item \( n = \sum_{i=1}^{k} x_i \): Il numero totale di prove (o conteggi).
    \item $d$: il numero di categorie possibili. 
    \item $p_{ki}$ la probabilità di successo per la categoria $i$ nella classe $k$.
\end{itemize}

Usando la forma analitica della distribuzione multinomiale, si può scrivere:
\[
P(\mathbf{x} \mid Y = k) = \frac{n!}{x_1! x_2! \ldots x_d!} p_{k1}^{x_1} p_{k2}^{x_2} \ldots p_{kd}^{x_d}
\]
e quindi la regola di classificazione MAP diventa:
\[
h(\mathbf{x}) = \arg \max_{k} P(Y = k) \cdot \frac{(\sum_{i=1}^d x_i)!}{x_1! x_2! \ldots x_d!} \prod_{j=1}^{d} p_{kj}^{x_j}
\]
Ma poiché il fattore \(\frac{(\sum_{i=1}^d x_i)!}{x_1! x_2! \ldots x_d!}\) è costante rispetto a \( k \), possiamo ignorarlo nella massimizzazione, ottenendo:
\[
h(\mathbf{x}) = \arg \max_{k} P(Y = k) \cdot \prod_{j=1}^{d} p_{kj}^{x_j}
\]

\subsection{Stima dei parametri}
Dobbiamo stimare, per utilizzare il modello, la probabiità a priori \( P(Y = k) \) e le probabilità condizionate \( p_{kj} \).

\paragraph{Stimare la probabilità a priori.}
Stimiamo la probabilità a priori \( P(Y = k) \) come la frequenza relativa della classe \( k \) nel set di training:
\[
P(Y = k) = \frac{\text{Numero di istanze della classe } k}{\text{Numero totale di istanze}} = \frac{\sum_{j} [y_j = k]}{N}
\]
dove \( N \) è il numero totale di istanze nel set di training e \( [y_j = k] \) sono le parentesi di Iverson, funzione indicatrice che vale 1 se l'istanza \( j \) appartiene alla classe \( k \), altrimenti vale 0.

\paragraph{Stimare le probabilità condizionate.}
Per stimare le probabilità condizionate \( p_{kj} \), possiamo utilizzare la frequenza relativa delle caratteristiche nella classe \( k \):
\begin{align*}
p_{kj}
    &= P(X_j = 1 \mid Y = k) \\
    &= \frac{\text{Numero di volte che la caratteristica } j \text{ appare nella classe } k}
            {\text{Numero totale di caratteristiche nella classe } k} \\
    &= \frac{\sum_{i} x_{ij} [y_i = k]}
            {\sum_{i} \sum_{j} x_{ij} [y_i = k]}
\end{align*}
Dove \( x_{ij} \) rappresenta il conteggio della caratteristica \( j \) nell'istanza \( i \).

\subsection{Problemi di stima e smoothing}
Queste due stime, però, possono causare dei problemi in certe situazioni.

\paragraph{Problema delle probabilità nulle.}
Un problema comune è quello delle probabilità nulle. Se una caratteristica \( j \) non appare mai nella classe \( k \) nel set di training, allora la stima di \( p_{kj} \) sarà zero. Questo porta a un problema quando si calcola la probabilità condizionata \( P(X = \mathbf{x} | Y = k) \), poiché il prodotto delle probabilità condizionate includerà un termine zero, rendendo l'intera probabilità nulla, indipendentemente dagli altri termini. Per risolvere questo problema, si può utilizzare una tecnica chiamata \emph{smoothing}, come il \emph{Laplace smoothing}. Questa tecnica consiste nell'aggiungere un piccolo valore costante (solitamente 1) a ciascun conteggio delle caratteristiche, in modo da evitare probabilità nulle. La stima di \( p_{kj} \) con Laplace smoothing diventa:
\[
p_{kj} = \frac{\sum_{i} x_{ij} [y_i = k] + 1}
            {\sum_{i} \sum_{j} x_{ij} [y_i = k] + d}
\]
dove \( d \) è il numero totale di caratteristiche.

\paragraph{Problema dell'underflow numerico.}
Un altro problema che può sorgere durante il calcolo della probabilità condizionata \( P(X = \mathbf{x} | Y = k) \) è l'underflow numerico. Poiché questa probabilità è calcolata come il prodotto di molte probabilità condizionate, il risultato può diventare estremamente piccolo, portando a problemi di precisione numerica. Per evitare questo problema, è comune lavorare con i logaritmi delle probabilità invece delle probabilità stesse. Poiché il logaritmo di un prodotto è la somma dei logaritmi, possiamo riscrivere la regola di classificazione MAP come:
\[
h(\mathbf{x}) = \arg \max_{k} \log P(Y = k
) + \sum_{j=1}^{d} x_j \log p_{kj}
\] 
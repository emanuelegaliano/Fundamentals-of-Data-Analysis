\relax 
\providecommand\hyper@newdestlabel[2]{}
\@writefile{toc}{\contentsline {chapter}{\numberline {10}Classificazione}{113}{chapter.10}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {10.1}Definizione formale}{113}{section.10.1}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {10.2}Misure di valutazione}{114}{section.10.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {10.2.1}Accuratezza}{114}{subsection.10.2.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {10.2.2}Tipi di errori}{114}{subsection.10.2.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {10.2.3}Matrice di confusione}{115}{subsection.10.2.3}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {10.1}{\ignorespaces Matrice di confusione per un problema di classificazione binaria\relax }}{115}{table.caption.117}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Esempio.}{115}{section*.118}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {10.2}{\ignorespaces Matrice di confusione per il modello naive\relax }}{115}{table.caption.119}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {10.2.4}Precision e recall}{116}{subsection.10.2.4}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{High precision vs high recall.}{116}{section*.120}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {10.2.5}$F_1$-score}{116}{subsection.10.2.5}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {10.1}{\ignorespaces Curve di livello dell'F\textsubscript  {1}-score (a sinistra) e della media aritmetica tra precision e recall, (precision+recall)/2 (a destra), al variare di precision e recall.\relax }}{117}{figure.caption.121}\protected@file@percent }
\newlabel{fig:f1_score}{{10.1}{117}{Curve di livello dell'F\textsubscript {1}-score (a sinistra) e della media aritmetica tra precision e recall, (precision+recall)/2 (a destra), al variare di precision e recall.\relax }{figure.caption.121}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {10.2.6}Matrice di confusione multiclasse}{117}{subsection.10.2.6}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {10.2.7}ROC e AUC}{117}{subsection.10.2.7}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {10.2}{\ignorespaces Matrice di confusione per un problema di classificazione a quattro classi: sulla diagonale sono visibili le predizioni corrette, mentre i valori fuori diagonale rappresentano gli errori di classificazione tra le diverse classi.\relax }}{118}{figure.caption.122}\protected@file@percent }
\newlabel{fig:confusion_matrix_multiclass}{{10.2}{118}{Matrice di confusione per un problema di classificazione a quattro classi: sulla diagonale sono visibili le predizioni corrette, mentre i valori fuori diagonale rappresentano gli errori di classificazione tra le diverse classi.\relax }{figure.caption.122}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {10.3}{\ignorespaces Curva ROC ottenuta sul dataset Breast Cancer: la curva mostra il compromesso tra TPR e FPR al variare della soglia di decisione, mentre l'AUC pari a 0.78 indica una buona capacità discriminante rispetto alla classificazione casuale (linea tratteggiata).\relax }}{119}{figure.caption.123}\protected@file@percent }
\newlabel{fig:roc_curve}{{10.3}{119}{Curva ROC ottenuta sul dataset Breast Cancer: la curva mostra il compromesso tra TPR e FPR al variare della soglia di decisione, mentre l'AUC pari a 0.78 indica una buona capacità discriminante rispetto alla classificazione casuale (linea tratteggiata).\relax }{figure.caption.123}{}}
\@writefile{toc}{\contentsline {paragraph}{Statistica J di Youden.}{119}{section*.124}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Esempio: classificatore a soglia.}{119}{section*.125}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {10.4}{\ignorespaces Distribuzione dell'altezza per uomini e donne.\relax }}{120}{figure.caption.126}\protected@file@percent }
\newlabel{fig:height_distribution}{{10.4}{120}{Distribuzione dell'altezza per uomini e donne.\relax }{figure.caption.126}{}}
\@writefile{lot}{\contentsline {table}{\numberline {10.3}{\ignorespaces Matrice di confusione per il classificatore a soglia $\theta = 170$ cm\relax }}{120}{table.caption.127}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {10.4}{\ignorespaces Metriche di classificazione per le classi \texttt  {False} e \texttt  {True}.\relax }}{120}{table.caption.128}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {10.5}{\ignorespaces Statistica J di Youden: la curva mostra il compromesso tra sensibilità e specificità al variare della soglia di decisione, mentre il punto ottimale (massimo della curva) indica la soglia che bilancia meglio i due aspetti.\relax }}{121}{figure.caption.129}\protected@file@percent }
\newlabel{fig:youden_statistic}{{10.5}{121}{Statistica J di Youden: la curva mostra il compromesso tra sensibilità e specificità al variare della soglia di decisione, mentre il punto ottimale (massimo della curva) indica la soglia che bilancia meglio i due aspetti.\relax }{figure.caption.129}{}}
\@writefile{toc}{\contentsline {section}{\numberline {10.3}K-Nearest Neighbors (KNN)}{121}{section.10.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {10.3.1}1-NN}{121}{subsection.10.3.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {10.3.2}K-NN}{122}{subsection.10.3.2}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {10.4}Curse of dimensionality}{122}{section.10.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {10.4.1}Spazio vuoto}{123}{subsection.10.4.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {10.6}{\ignorespaces Distribuzione di 50 punti in spazi di diversa dimensionalità: a sinistra uno spazio 1D, al centro uno spazio 2D e a destra uno spazio 3D. Al crescere della dimensionalità, i punti si disperdono in regioni sempre più ampie dello spazio delle caratteristiche.\relax }}{123}{figure.caption.130}\protected@file@percent }
\newlabel{fig:empty_space_multidimensional}{{10.6}{123}{Distribuzione di 50 punti in spazi di diversa dimensionalità: a sinistra uno spazio 1D, al centro uno spazio 2D e a destra uno spazio 3D. Al crescere della dimensionalità, i punti si disperdono in regioni sempre più ampie dello spazio delle caratteristiche.\relax }{figure.caption.130}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {10.4.2}Impatto su K-NN}{123}{subsection.10.4.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {10.7}{\ignorespaces Illustrazione del \emph  {curse of dimensionality}: a sinistra sono riportate la distanza minima e massima tra 100 punti generati casualmente al crescere della dimensionalità dello spazio; a destra è mostrata la riduzione relativa della variabilità delle distanze, evidenziando come, in spazi ad alta dimensionalità, le distanze tra punti tendano a diventare sempre più simili, causando il collasso del concetto di “vicinato”.\relax }}{123}{figure.caption.131}\protected@file@percent }
\newlabel{fig:curse_of_dimensionality}{{10.7}{123}{Illustrazione del \emph {curse of dimensionality}: a sinistra sono riportate la distanza minima e massima tra 100 punti generati casualmente al crescere della dimensionalità dello spazio; a destra è mostrata la riduzione relativa della variabilità delle distanze, evidenziando come, in spazi ad alta dimensionalità, le distanze tra punti tendano a diventare sempre più simili, causando il collasso del concetto di “vicinato”.\relax }{figure.caption.131}{}}
\@writefile{toc}{\contentsline {section}{\numberline {10.5}Classificatori discriminativi e generativi}{124}{section.10.5}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {10.5.1}Classificatori discriminativi}{124}{subsection.10.5.1}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Esempio.}{124}{section*.132}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {10.5.2}Classificatori generativi}{124}{subsection.10.5.2}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Esempio.}{124}{section*.133}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {10.5.3}Macro, Micro e Weighted Averaging}{125}{subsection.10.5.3}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Macro averaging.}{125}{section*.134}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Weighted averaging.}{125}{section*.135}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Micro averaging.}{125}{section*.136}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {10.5.4}Decision Boundary}{125}{subsection.10.5.4}\protected@file@percent }
\newlabel{fig:knn_k1}{{10.8a}{126}{$K = 1$\relax }{figure.caption.137}{}}
\newlabel{sub@fig:knn_k1}{{a}{126}{$K = 1$\relax }{figure.caption.137}{}}
\newlabel{fig:knn_k5}{{10.8b}{126}{$K = 5$\relax }{figure.caption.137}{}}
\newlabel{sub@fig:knn_k5}{{b}{126}{$K = 5$\relax }{figure.caption.137}{}}
\newlabel{fig:knn_k10}{{10.8c}{126}{$K = 10$\relax }{figure.caption.137}{}}
\newlabel{sub@fig:knn_k10}{{c}{126}{$K = 10$\relax }{figure.caption.137}{}}
\newlabel{fig:knn_k20}{{10.8d}{126}{$K = 20$\relax }{figure.caption.137}{}}
\newlabel{sub@fig:knn_k20}{{d}{126}{$K = 20$\relax }{figure.caption.137}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {10.8}{\ignorespaces Confronto dei confini decisionali del classificatore K-NN sul dataset Iris al variare del numero di vicini $K$. Per $K=1$ (in alto a sinistra) la frontiera segue in modo molto dettagliato i singoli punti di training, risultando irregolare e potenzialmente soggetta a overfitting. Aumentando $K$ a 5 e 10 (in alto a destra e in basso a sinistra) le regioni decisionali diventano via via più lisce, riducendo la sensibilità al rumore ma smussando alcune strutture locali, in particolare nella zona di separazione tra \textit  {versicolor} e \textit  {virginica}. Per $K=20$ (in basso a destra) la frontiera è molto regolare: il modello è più stabile, ma rischia di sottostimare le differenze tra le due classi non linearmente separabili, evidenziando il classico compromesso tra complessità del modello e capacità di generalizzazione.\relax }}{126}{figure.caption.137}\protected@file@percent }
\newlabel{fig:knn_decision_boundaries_grid}{{10.8}{126}{Confronto dei confini decisionali del classificatore K-NN sul dataset Iris al variare del numero di vicini $K$. Per $K=1$ (in alto a sinistra) la frontiera segue in modo molto dettagliato i singoli punti di training, risultando irregolare e potenzialmente soggetta a overfitting. Aumentando $K$ a 5 e 10 (in alto a destra e in basso a sinistra) le regioni decisionali diventano via via più lisce, riducendo la sensibilità al rumore ma smussando alcune strutture locali, in particolare nella zona di separazione tra \textit {versicolor} e \textit {virginica}. Per $K=20$ (in basso a destra) la frontiera è molto regolare: il modello è più stabile, ma rischia di sottostimare le differenze tra le due classi non linearmente separabili, evidenziando il classico compromesso tra complessità del modello e capacità di generalizzazione.\relax }{figure.caption.137}{}}
\@writefile{toc}{\contentsline {section}{\numberline {10.6}K-NN per regressione}{127}{section.10.6}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {10.6.1}Bias-Varianza trade-off}{127}{subsection.10.6.1}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Trovare il miglior valore di $k$.}{127}{section*.138}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {10.9}{\ignorespaces Curva di validazione per il modello K-NN in regressione, ottenuta tramite cross-validation. L'asse delle ascisse rappresenta il numero di vicini $K$, mentre l'asse delle ordinate mostra l'errore medio (RMSE) sui diversi fold. Si osserva che l'errore diminuisce rapidamente per valori piccoli di $K$, per poi stabilizzarsi. Il valore ottimale individuato è $K = 25$, che corrisponde al minimo RMSE medio, rappresentato dalla linea tratteggiata rossa.\relax }}{128}{figure.caption.139}\protected@file@percent }
\newlabel{fig:knn_regression_bias_variance}{{10.9}{128}{Curva di validazione per il modello K-NN in regressione, ottenuta tramite cross-validation. L'asse delle ascisse rappresenta il numero di vicini $K$, mentre l'asse delle ordinate mostra l'errore medio (RMSE) sui diversi fold. Si osserva che l'errore diminuisce rapidamente per valori piccoli di $K$, per poi stabilizzarsi. Il valore ottimale individuato è $K = 25$, che corrisponde al minimo RMSE medio, rappresentato dalla linea tratteggiata rossa.\relax }{figure.caption.139}{}}
\@setckpt{chapters/classification}{
\setcounter{page}{129}
\setcounter{equation}{0}
\setcounter{enumi}{5}
\setcounter{enumii}{0}
\setcounter{enumiii}{0}
\setcounter{enumiv}{0}
\setcounter{footnote}{2}
\setcounter{mpfootnote}{0}
\setcounter{part}{0}
\setcounter{chapter}{10}
\setcounter{section}{6}
\setcounter{subsection}{1}
\setcounter{subsubsection}{0}
\setcounter{paragraph}{0}
\setcounter{subparagraph}{0}
\setcounter{figure}{9}
\setcounter{table}{4}
\setcounter{lstnumber}{1}
\setcounter{parentequation}{0}
\setcounter{section@level}{4}
\setcounter{Item}{13}
\setcounter{Hfootnote}{23}
\setcounter{bookmark@seq@number}{229}
\setcounter{caption@flags}{2}
\setcounter{continuedfloat}{0}
\setcounter{subfigure}{0}
\setcounter{subtable}{0}
\setcounter{float@type}{16}
\setcounter{tcbbreakpart}{1}
\setcounter{tcblayer}{0}
\setcounter{tcolorbox@number}{52}
\setcounter{tcbrastercolumn}{1}
\setcounter{tcbrasterrow}{1}
\setcounter{tcbrasternum}{1}
\setcounter{tcbraster}{0}
\setcounter{tcblisting}{0}
\setcounter{nicequotecnt}{1}
\setcounter{lstlisting}{0}
}

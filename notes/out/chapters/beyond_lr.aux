\relax 
\providecommand\hyper@newdestlabel[2]{}
\@writefile{toc}{\contentsline {chapter}{\numberline {9}Oltre la regressione lineare}{103}{chapter.9}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {9.1}Interazione tra variabili}{103}{section.9.1}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Additive Model (R-squared)}{103}{section*.103}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Interaction Model (R-squared)}{103}{section*.104}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {9.1}{\ignorespaces Coefficients for the Interaction Regression Model\relax }}{103}{table.caption.105}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {9.1.1}Interpretazione dei coefficienti}{103}{subsection.9.1.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {9.1}{\ignorespaces Confronto tra il modello additivo $mpg = \beta _0 + \beta _1 \text  {horsepower} + \beta _2 \text  {weight}$ (prima riga) e il modello con interazione $mpg = \beta _0 + \beta _1 \text  {horsepower} + \beta _2 \text  {weight} + \beta _3 \text  {horsepower}\times \text  {weight}$ (seconda riga). A sinistra sono riportati i grafici dei residui, a destra i Q-Q plot dei residui rispetto alla distribuzione normale teorica.\relax }}{104}{figure.caption.106}\protected@file@percent }
\newlabel{fig:interaction_plot}{{9.1}{104}{Confronto tra il modello additivo $mpg = \beta _0 + \beta _1 \text {horsepower} + \beta _2 \text {weight}$ (prima riga) e il modello con interazione $mpg = \beta _0 + \beta _1 \text {horsepower} + \beta _2 \text {weight} + \beta _3 \text {horsepower}\times \text {weight}$ (seconda riga). A sinistra sono riportati i grafici dei residui, a destra i Q-Q plot dei residui rispetto alla distribuzione normale teorica.\relax }{figure.caption.106}{}}
\@writefile{toc}{\contentsline {section}{\numberline {9.2}Regressione polinomiale}{104}{section.9.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {9.2.1}Regressione quadratica}{104}{subsection.9.2.1}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {9.2}{\ignorespaces Risultati della regressione con termine quadratico\relax }}{105}{table.caption.107}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {9.2}{\ignorespaces Curva del modello di regressione polinomiale di secondo grado per il dataset delle auto. Si nota come la curva si adatti meglio ai dati rispetto a una retta.\relax }}{105}{figure.caption.108}\protected@file@percent }
\newlabel{fig:polynomial_regression}{{9.2}{105}{Curva del modello di regressione polinomiale di secondo grado per il dataset delle auto. Si nota come la curva si adatti meglio ai dati rispetto a una retta.\relax }{figure.caption.108}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {9.2.2}Polinomi di grado superiore}{105}{subsection.9.2.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {9.2.3}Problema dell'interpretabilità}{105}{subsection.9.2.3}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {9.3}{\ignorespaces Curva del modello di regressione polinomiale di quarto grado per il dataset delle auto. Si nota come la curva si adatti molto bene ai dati.\relax }}{106}{figure.caption.109}\protected@file@percent }
\newlabel{fig:polynomial_regression_degree4}{{9.3}{106}{Curva del modello di regressione polinomiale di quarto grado per il dataset delle auto. Si nota come la curva si adatti molto bene ai dati.\relax }{figure.caption.109}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {9.2.4}Metriche di predizione}{106}{subsection.9.2.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {9.2.5}MSE: Mean Squared Error}{106}{subsection.9.2.5}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {9.2.6}RMSE: Root Mean Squared Error}{107}{subsection.9.2.6}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {9.2.7}MAE: Mean Absolute Error}{107}{subsection.9.2.7}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {9.4}{\ignorespaces Confronto visivo tra tre modelli di regressione con diverso livello di adattamento ai dati. Da sinistra a destra: \textit  {High Fit}, \textit  {Medium Fit} e \textit  {Low Fit}. Per ciascun modello sono riportati i punti osservati, la retta di regressione stimata e le principali metriche di performance ($R^2$, MSE, RMSE e MAE). Si osserva come la dispersione crescente dei dati attorno alla retta comporti un peggioramento sistematico di tutte le metriche di errore.\relax }}{107}{figure.caption.110}\protected@file@percent }
\newlabel{fig:error_metrics_comparison}{{9.4}{107}{Confronto visivo tra tre modelli di regressione con diverso livello di adattamento ai dati. Da sinistra a destra: \textit {High Fit}, \textit {Medium Fit} e \textit {Low Fit}. Per ciascun modello sono riportati i punti osservati, la retta di regressione stimata e le principali metriche di performance ($R^2$, MSE, RMSE e MAE). Si osserva come la dispersione crescente dei dati attorno alla retta comporti un peggioramento sistematico di tutte le metriche di errore.\relax }{figure.caption.110}{}}
\@writefile{toc}{\contentsline {section}{\numberline {9.3}Overfitting}{108}{section.9.3}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {9.5}{\ignorespaces Esempio di underfitting, good fit e overfitting nel rapporto tra \textit  {horsepower} e \textit  {MPG}. La curva di grado 1 (verde) sottostima la complessità della relazione (\textit  {underfit}), quella di grado 2 (rossa) fornisce un buon compromesso tra bias e varianza, mentre il modello di grado 15 (linea tratteggiata nera) si adatta eccessivamente al rumore dei dati (\textit  {overfit}), mostrando forti oscillazioni prive di significato inferenziale.\relax }}{108}{figure.caption.111}\protected@file@percent }
\newlabel{fig:overfitting_underfitting}{{9.5}{108}{Esempio di underfitting, good fit e overfitting nel rapporto tra \textit {horsepower} e \textit {MPG}. La curva di grado 1 (verde) sottostima la complessità della relazione (\textit {underfit}), quella di grado 2 (rossa) fornisce un buon compromesso tra bias e varianza, mentre il modello di grado 15 (linea tratteggiata nera) si adatta eccessivamente al rumore dei dati (\textit {overfit}), mostrando forti oscillazioni prive di significato inferenziale.\relax }{figure.caption.111}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {9.3.1}Regolarizzazione}{108}{subsection.9.3.1}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Ridge Regression (L2 Regularization)}{108}{section*.112}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {9.6}{\ignorespaces Percorsi dei coefficienti del modello Ridge al variare del parametro di penalizzazione $\lambda $. All'aumentare di $\lambda $, i coefficienti vengono spinti verso valori più piccoli in modulo, riducendo la varianza del modello. Si osserva come le variabili con minore rilevanza predittiva vengano contratte più rapidamente, mentre i predittori più informativi rimangono relativamente stabili per valori più elevati di $\lambda $.\relax }}{109}{figure.caption.113}\protected@file@percent }
\newlabel{fig:ridge_regression_effect}{{9.6}{109}{Percorsi dei coefficienti del modello Ridge al variare del parametro di penalizzazione $\lambda $. All'aumentare di $\lambda $, i coefficienti vengono spinti verso valori più piccoli in modulo, riducendo la varianza del modello. Si osserva come le variabili con minore rilevanza predittiva vengano contratte più rapidamente, mentre i predittori più informativi rimangono relativamente stabili per valori più elevati di $\lambda $.\relax }{figure.caption.113}{}}
\@writefile{toc}{\contentsline {paragraph}{Lasso Regression (L1 Regularization)}{109}{section*.114}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {9.7}{\ignorespaces Percorsi dei coefficienti del modello Lasso al variare del parametro di penalizzazione $\lambda $. A differenza della regressione Ridge, la penalizzazione L1 forza progressivamente alcuni coefficienti esattamente a zero, effettuando una vera e propria selezione delle variabili. Si osserva che, per valori crescenti di $\lambda $, solo pochi predittori mantengono un coefficiente diverso da zero, mentre gli altri vengono eliminati dal modello.\relax }}{110}{figure.caption.115}\protected@file@percent }
\newlabel{fig:lasso_regression_effect}{{9.7}{110}{Percorsi dei coefficienti del modello Lasso al variare del parametro di penalizzazione $\lambda $. A differenza della regressione Ridge, la penalizzazione L1 forza progressivamente alcuni coefficienti esattamente a zero, effettuando una vera e propria selezione delle variabili. Si osserva che, per valori crescenti di $\lambda $, solo pochi predittori mantengono un coefficiente diverso da zero, mentre gli altri vengono eliminati dal modello.\relax }{figure.caption.115}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {9.3.2}Bias-Varianza trade-off con la regolarizzazione}{110}{subsection.9.3.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {9.8}{\ignorespaces Curva di validazione per la regressione Ridge con modello polinomiale di grado 15. La penalizzazione $\lambda $ controlla il compromesso tra bias e varianza: per valori molto piccoli di $\lambda $ il modello soffre di alta varianza (overfitting), mentre per valori molto grandi mostra alto bias (underfitting). L'errore di validazione (curva rossa) raggiunge il minimo attorno a $\lambda \approx 2.85$, indicato dalla linea tratteggiata, che rappresenta il valore ottimale di regolarizzazione.\relax }}{111}{figure.caption.116}\protected@file@percent }
\newlabel{fig:bias_variance_tradeoff_reg}{{9.8}{111}{Curva di validazione per la regressione Ridge con modello polinomiale di grado 15. La penalizzazione $\lambda $ controlla il compromesso tra bias e varianza: per valori molto piccoli di $\lambda $ il modello soffre di alta varianza (overfitting), mentre per valori molto grandi mostra alto bias (underfitting). L'errore di validazione (curva rossa) raggiunge il minimo attorno a $\lambda \approx 2.85$, indicato dalla linea tratteggiata, che rappresenta il valore ottimale di regolarizzazione.\relax }{figure.caption.116}{}}
\@setckpt{chapters/beyond_lr}{
\setcounter{page}{112}
\setcounter{equation}{0}
\setcounter{enumi}{5}
\setcounter{enumii}{0}
\setcounter{enumiii}{0}
\setcounter{enumiv}{0}
\setcounter{footnote}{0}
\setcounter{mpfootnote}{0}
\setcounter{part}{0}
\setcounter{chapter}{9}
\setcounter{section}{3}
\setcounter{subsection}{2}
\setcounter{subsubsection}{0}
\setcounter{paragraph}{0}
\setcounter{subparagraph}{0}
\setcounter{figure}{8}
\setcounter{table}{2}
\setcounter{lstnumber}{1}
\setcounter{parentequation}{0}
\setcounter{section@level}{2}
\setcounter{Item}{13}
\setcounter{Hfootnote}{21}
\setcounter{bookmark@seq@number}{206}
\setcounter{caption@flags}{2}
\setcounter{continuedfloat}{0}
\setcounter{subfigure}{0}
\setcounter{subtable}{0}
\setcounter{float@type}{16}
\setcounter{tcbbreakpart}{1}
\setcounter{tcblayer}{0}
\setcounter{tcolorbox@number}{51}
\setcounter{tcbrastercolumn}{1}
\setcounter{tcbrasterrow}{1}
\setcounter{tcbrasternum}{1}
\setcounter{tcbraster}{0}
\setcounter{tcblisting}{0}
\setcounter{nicequotecnt}{0}
\setcounter{lstlisting}{0}
}

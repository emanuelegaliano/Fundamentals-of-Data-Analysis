\relax 
\providecommand\hyper@newdestlabel[2]{}
\@writefile{toc}{\contentsline {chapter}{\numberline {7}Analisi predittiva}{69}{chapter.7}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {7.1}Modello}{69}{section.7.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {7.1.1}Modelli predittivi}{69}{subsection.7.1.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {7.1}{\ignorespaces Distribuzione del BMI in una popolazione. Istogramma reale composto da $n=768$ individui (barre grigie) a confronto con un modello gaussiano (linea nera). La Gaussiana riassume con media e deviazione una parte della popolazione in modo corretto, ma assegna probabilità anche a BMI negativi.\relax }}{70}{figure.caption.63}\protected@file@percent }
\newlabel{fig:bmi_model}{{7.1}{70}{Distribuzione del BMI in una popolazione. Istogramma reale composto da $n=768$ individui (barre grigie) a confronto con un modello gaussiano (linea nera). La Gaussiana riassume con media e deviazione una parte della popolazione in modo corretto, ma assegna probabilità anche a BMI negativi.\relax }{figure.caption.63}{}}
\@writefile{toc}{\contentsline {section}{\numberline {7.2}Predizione vs Spiegazione}{70}{section.7.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {7.2.1}Predizione}{70}{subsection.7.2.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {7.2.2}Spiegazione}{71}{subsection.7.2.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {7.2.3}Compromesso tra Predizione e Spiegazione}{71}{subsection.7.2.3}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {7.3}Statistica vs Machine Learning}{71}{section.7.3}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {7.2}{\ignorespaces Confronto tra \emph  {glass box} statistica e \emph  {black box} di ML: la statistica privilegia interpretabilità e spiegazione del \textit  {perché}, mentre il machine learning privilegia accuratezza predittiva sul \textit  {che cosa} a partire dai dati, spesso con modelli opachi.\relax }}{71}{figure.caption.64}\protected@file@percent }
\newlabel{fig:stats_vs_ml}{{7.2}{71}{Confronto tra \emph {glass box} statistica e \emph {black box} di ML: la statistica privilegia interpretabilità e spiegazione del \textit {perché}, mentre il machine learning privilegia accuratezza predittiva sul \textit {che cosa} a partire dai dati, spesso con modelli opachi.\relax }{figure.caption.64}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.3.1}Approccio statistico}{72}{subsection.7.3.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {7.3.2}Approccio di Machine Learning}{72}{subsection.7.3.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {7.3.3}Trade-Off di Complessità-Interpretabilità}{72}{subsection.7.3.3}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {7.4}Tipologie di problema}{72}{section.7.4}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {7.3}{\ignorespaces Accuratezza vs complessità del modello: dalla Regressione Lineare a Decision Tree e Random Forest fino alle Reti Neurali, l’errore tende a diminuire man mano che cresce la complessità (ma aumenta il costo/opaquezza del modello).\relax }}{73}{figure.caption.65}\protected@file@percent }
\newlabel{fig:complexity_interpretability_tradeoff}{{7.3}{73}{Accuratezza vs complessità del modello: dalla Regressione Lineare a Decision Tree e Random Forest fino alle Reti Neurali, l’errore tende a diminuire man mano che cresce la complessità (ma aumenta il costo/opaquezza del modello).\relax }{figure.caption.65}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.4.1}Regressione}{73}{subsection.7.4.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {7.4.2}Classificazione}{73}{subsection.7.4.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {7.4}{\ignorespaces Esempio di problema di regressione: prevedere il prezzo di una casa in base ai metri quadri.\relax }}{74}{figure.caption.66}\protected@file@percent }
\newlabel{fig:regression_example}{{7.4}{74}{Esempio di problema di regressione: prevedere il prezzo di una casa in base ai metri quadri.\relax }{figure.caption.66}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {7.5}{\ignorespaces Esempio di problema di classificazione: capire se una malattia è presente oppure no in base all'età e a un'analisi del sangue.\relax }}{74}{figure.caption.67}\protected@file@percent }
\newlabel{fig:classification_example}{{7.5}{74}{Esempio di problema di classificazione: capire se una malattia è presente oppure no in base all'età e a un'analisi del sangue.\relax }{figure.caption.67}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.4.3}Clustering}{75}{subsection.7.4.3}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {7.6}{\ignorespaces Esempio di problema di clustering: raggruppare i clienti in segmenti basandosi sui loro comportamenti di acquisto.\relax }}{75}{figure.caption.68}\protected@file@percent }
\newlabel{fig:clustering_example}{{7.6}{75}{Esempio di problema di clustering: raggruppare i clienti in segmenti basandosi sui loro comportamenti di acquisto.\relax }{figure.caption.68}{}}
\@writefile{toc}{\contentsline {section}{\numberline {7.5}Modelli parametrici vs Modelli non parametrici}{75}{section.7.5}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {7.5.1}Modelli parametrici}{75}{subsection.7.5.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {7.5.2}Modelli non parametrici}{76}{subsection.7.5.2}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {7.6}Learning}{76}{section.7.6}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {7.6.1}Definizione formale}{76}{subsection.7.6.1}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Esempio 1: non parametrico.}{77}{section*.69}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Esempio 2: parametrico.}{77}{section*.70}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {7.6.2}Il processo di Learning}{77}{subsection.7.6.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {7.6.3}ERM: Empirical Risk Minimization}{77}{subsection.7.6.3}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {7.7}Capacità del modello}{78}{section.7.7}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {7.7.1}Misurare la capacità del modello}{78}{subsection.7.7.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {7.7.2}Bias e Varianza}{79}{subsection.7.7.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {7.7}{\ignorespaces Underfitting---capacità adeguata---overfitting in regressione polinomiale: punti blu \(=\) dati di training (\(\mathrm  {TR}\)), punti rossi \(=\) fuori da \(\mathrm  {TR}\); la linea verde mostra il fit del modello: lineare (sinistra), polinomio di grado \(2\) (centro) e polinomio di grado \(5\) (destra).\relax }}{79}{figure.caption.71}\protected@file@percent }
\newlabel{fig:ovfit_underfit_rightfit}{{7.7}{79}{Underfitting---capacità adeguata---overfitting in regressione polinomiale: punti blu \(=\) dati di training (\(\mathrm {TR}\)), punti rossi \(=\) fuori da \(\mathrm {TR}\); la linea verde mostra il fit del modello: lineare (sinistra), polinomio di grado \(2\) (centro) e polinomio di grado \(5\) (destra).\relax }{figure.caption.71}{}}
\@writefile{toc}{\contentsline {paragraph}{Bias.}{79}{section*.72}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Varianza.}{79}{section*.73}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {7.8}{\ignorespaces Trade-off tra bias e varianza in funzione della complessità del modello.\relax }}{80}{figure.caption.74}\protected@file@percent }
\newlabel{fig:bias_variance_tradeoff_curves}{{7.8}{80}{Trade-off tra bias e varianza in funzione della complessità del modello.\relax }{figure.caption.74}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.7.3}Parametri vs Iperparametri}{80}{subsection.7.7.3}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {7.8}Selezione del modello}{80}{section.7.8}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {7.8.1}Approccio 1: selezione statistica}{80}{subsection.7.8.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {7.8.2}Approccio 2: selezione predittiva}{81}{subsection.7.8.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {7.8.3}Validazione Holdout}{81}{subsection.7.8.3}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {7.9}{\ignorespaces Validazione Holdout: il dataset viene prima mescolato, poi diviso in training set (70\%) e test set (30\%). Il modello viene addestrato sul training set e valutato sul test set per stimare la capacità di generalizzazione.\relax }}{82}{figure.caption.75}\protected@file@percent }
\newlabel{fig:holdout_validation}{{7.9}{82}{Validazione Holdout: il dataset viene prima mescolato, poi diviso in training set (70\%) e test set (30\%). Il modello viene addestrato sul training set e valutato sul test set per stimare la capacità di generalizzazione.\relax }{figure.caption.75}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.8.4}K-Fold Cross-Validation}{82}{subsection.7.8.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {7.8.5}Leave-One-Out Cross-Validation (LOOCV)}{82}{subsection.7.8.5}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {7.8.6}Ottimizzazione degli iperparametri}{82}{subsection.7.8.6}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {7.10}{\ignorespaces K-Fold Cross-Validation: il dataset viene diviso in \(K\) folds. In ogni iterazione, un fold viene utilizzato come test set e gli altri \(K-1\) folds come training set. Questo processo viene ripetuto \(K\) volte, e le prestazioni del modello vengono mediate su tutte le iterazioni per ottenere una stima più robusta della capacità di generalizzazione.\relax }}{83}{figure.caption.76}\protected@file@percent }
\newlabel{fig:k_fold_cross_validation}{{7.10}{83}{K-Fold Cross-Validation: il dataset viene diviso in \(K\) folds. In ogni iterazione, un fold viene utilizzato come test set e gli altri \(K-1\) folds come training set. Questo processo viene ripetuto \(K\) volte, e le prestazioni del modello vengono mediate su tutte le iterazioni per ottenere una stima più robusta della capacità di generalizzazione.\relax }{figure.caption.76}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {7.11}{\ignorespaces Leave-One-Out Cross-Validation (LOOCV): in ogni iterazione, un singolo esempio viene utilizzato come test set, mentre tutti gli altri esempi vengono utilizzati come training set. Questo processo viene ripetuto per ogni esempio nel dataset.\relax }}{83}{figure.caption.77}\protected@file@percent }
\newlabel{fig:loocv}{{7.11}{83}{Leave-One-Out Cross-Validation (LOOCV): in ogni iterazione, un singolo esempio viene utilizzato come test set, mentre tutti gli altri esempi vengono utilizzati come training set. Questo processo viene ripetuto per ogni esempio nel dataset.\relax }{figure.caption.77}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {7.12}{\ignorespaces Utilizzo di un validation set per l'ottimizzazione degli iperparametri: il dataset viene diviso in training set (60\%), validation set (20\%) e test set (20\%). Il modello viene addestrato sul training set, gli iperparametri vengono ottimizzati sul validation set e infine le prestazioni del modello vengono valutate sul test set.\relax }}{84}{figure.caption.80}\protected@file@percent }
\newlabel{fig:validation_set}{{7.12}{84}{Utilizzo di un validation set per l'ottimizzazione degli iperparametri: il dataset viene diviso in training set (60\%), validation set (20\%) e test set (20\%). Il modello viene addestrato sul training set, gli iperparametri vengono ottimizzati sul validation set e infine le prestazioni del modello vengono valutate sul test set.\relax }{figure.caption.80}{}}
\@writefile{toc}{\contentsline {paragraph}{Grid Search.}{84}{section*.78}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Validation set.}{84}{section*.79}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {7.13}{\ignorespaces K-Fold Cross Validation con ottimizzazione degli iperparametri: il dataset viene diviso in training/validation set e test set. Il training/validation set viene sottoposto a K-Fold Cross Validation per ottimizzare gli iperparametri, mentre il test set viene utilizzato solo alla fine per valutare le prestazioni finali del modello.\relax }}{85}{figure.caption.81}\protected@file@percent }
\newlabel{fig:k_fold_with_hyperparameter_tuning}{{7.13}{85}{K-Fold Cross Validation con ottimizzazione degli iperparametri: il dataset viene diviso in training/validation set e test set. Il training/validation set viene sottoposto a K-Fold Cross Validation per ottimizzare gli iperparametri, mentre il test set viene utilizzato solo alla fine per valutare le prestazioni finali del modello.\relax }{figure.caption.81}{}}
\@setckpt{chapters/predictive_analysis}{
\setcounter{page}{86}
\setcounter{equation}{0}
\setcounter{enumi}{5}
\setcounter{enumii}{0}
\setcounter{enumiii}{0}
\setcounter{enumiv}{0}
\setcounter{footnote}{6}
\setcounter{mpfootnote}{0}
\setcounter{part}{0}
\setcounter{chapter}{7}
\setcounter{section}{8}
\setcounter{subsection}{6}
\setcounter{subsubsection}{0}
\setcounter{paragraph}{0}
\setcounter{subparagraph}{0}
\setcounter{figure}{13}
\setcounter{table}{0}
\setcounter{lstnumber}{1}
\setcounter{parentequation}{0}
\setcounter{section@level}{4}
\setcounter{Item}{13}
\setcounter{Hfootnote}{12}
\setcounter{bookmark@seq@number}{172}
\setcounter{caption@flags}{2}
\setcounter{continuedfloat}{0}
\setcounter{subfigure}{0}
\setcounter{subtable}{0}
\setcounter{float@type}{16}
\setcounter{tcbbreakpart}{1}
\setcounter{tcblayer}{0}
\setcounter{tcolorbox@number}{49}
\setcounter{tcbrastercolumn}{1}
\setcounter{tcbrasterrow}{1}
\setcounter{tcbrasternum}{1}
\setcounter{tcbraster}{0}
\setcounter{tcblisting}{0}
\setcounter{nicequotecnt}{4}
\setcounter{lstlisting}{0}
}

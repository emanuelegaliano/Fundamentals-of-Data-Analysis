\relax 
\providecommand\hyper@newdestlabel[2]{}
\@writefile{toc}{\contentsline {chapter}{\numberline {7}Analisi predittiva}{75}{chapter.7}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {7.1}Modello}{75}{section.7.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {7.1.1}Modelli predittivi}{75}{subsection.7.1.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {7.1}{\ignorespaces Distribuzione del BMI in una popolazione. Istogramma reale composto da $n=768$ individui (barre grigie) a confronto con un modello gaussiano (linea nera). La Gaussiana riassume con media e deviazione una parte della popolazione in modo corretto, ma assegna probabilità anche a BMI negativi.}}{76}{figure.7.1}\protected@file@percent }
\newlabel{fig:bmi_model}{{7.1}{76}{Distribuzione del BMI in una popolazione. Istogramma reale composto da $n=768$ individui (barre grigie) a confronto con un modello gaussiano (linea nera). La Gaussiana riassume con media e deviazione una parte della popolazione in modo corretto, ma assegna probabilità anche a BMI negativi}{figure.7.1}{}}
\@writefile{toc}{\contentsline {section}{\numberline {7.2}Predizione vs Spiegazione}{76}{section.7.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {7.2.1}Predizione}{76}{subsection.7.2.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {7.2.2}Spiegazione}{77}{subsection.7.2.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {7.2.3}Compromesso tra Predizione e Spiegazione}{77}{subsection.7.2.3}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {7.3}Statistica vs Machine Learning}{77}{section.7.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {7.3.1}Approccio statistico}{77}{subsection.7.3.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {7.2}{\ignorespaces Confronto tra \emph  {glass box} statistica e \emph  {black box} di ML: la statistica privilegia interpretabilità e spiegazione del \textit  {perché}, mentre il machine learning privilegia accuratezza predittiva sul \textit  {che cosa} a partire dai dati, spesso con modelli opachi.}}{78}{figure.7.2}\protected@file@percent }
\newlabel{fig:stats_vs_ml}{{7.2}{78}{Confronto tra \emph {glass box} statistica e \emph {black box} di ML: la statistica privilegia interpretabilità e spiegazione del \textit {perché}, mentre il machine learning privilegia accuratezza predittiva sul \textit {che cosa} a partire dai dati, spesso con modelli opachi}{figure.7.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.3.2}Approccio di Machine Learning}{78}{subsection.7.3.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {7.3.3}Trade-Off di Complessità-Interpretabilità}{78}{subsection.7.3.3}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {7.3}{\ignorespaces Accuratezza vs complessità del modello: dalla Regressione Lineare a Decision Tree e Random Forest fino alle Reti Neurali, l’errore tende a diminuire man mano che cresce la complessità (ma aumenta il costo/opaquezza del modello).}}{79}{figure.7.3}\protected@file@percent }
\newlabel{fig:complexity_interpretability_tradeoff}{{7.3}{79}{Accuratezza vs complessità del modello: dalla Regressione Lineare a Decision Tree e Random Forest fino alle Reti Neurali, l’errore tende a diminuire man mano che cresce la complessità (ma aumenta il costo/opaquezza del modello)}{figure.7.3}{}}
\@writefile{toc}{\contentsline {section}{\numberline {7.4}Tipologie di problema}{79}{section.7.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {7.4.1}Regressione}{79}{subsection.7.4.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {7.4}{\ignorespaces Esempio di problema di regressione: prevedere il prezzo di una casa in base ai metri quadri.}}{80}{figure.7.4}\protected@file@percent }
\newlabel{fig:regression_example}{{7.4}{80}{Esempio di problema di regressione: prevedere il prezzo di una casa in base ai metri quadri}{figure.7.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.4.2}Classificazione}{80}{subsection.7.4.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {7.5}{\ignorespaces Esempio di problema di classificazione: capire se una malattia è presente oppure no in base all'età e a un'analisi del sangue.}}{80}{figure.7.5}\protected@file@percent }
\newlabel{fig:classification_example}{{7.5}{80}{Esempio di problema di classificazione: capire se una malattia è presente oppure no in base all'età e a un'analisi del sangue}{figure.7.5}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.4.3}Clustering}{81}{subsection.7.4.3}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {7.6}{\ignorespaces Esempio di problema di clustering: raggruppare i clienti in segmenti basandosi sui loro comportamenti di acquisto.}}{81}{figure.7.6}\protected@file@percent }
\newlabel{fig:clustering_example}{{7.6}{81}{Esempio di problema di clustering: raggruppare i clienti in segmenti basandosi sui loro comportamenti di acquisto}{figure.7.6}{}}
\@writefile{toc}{\contentsline {section}{\numberline {7.5}Modelli parametrici vs Modelli non parametrici}{81}{section.7.5}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {7.5.1}Modelli parametrici}{82}{subsection.7.5.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {7.5.2}Modelli non parametrici}{82}{subsection.7.5.2}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {7.6}Learning}{82}{section.7.6}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {7.6.1}Definizione formale}{83}{subsection.7.6.1}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Esempio 1: non parametrico.}{83}{section*.30}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Esempio 2: parametrico.}{83}{section*.31}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {7.6.2}Il processo di Learning}{83}{subsection.7.6.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {7.6.3}ERM: Empirical Risk Minimization}{84}{subsection.7.6.3}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {7.7}Capacità del modello}{84}{section.7.7}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {7.7.1}Misurare la capacità del modello}{85}{subsection.7.7.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {7.7.2}Bias e Varianza}{85}{subsection.7.7.2}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Bias.}{85}{section*.32}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {7.7}{\ignorespaces Underfitting---capacità adeguata---overfitting in regressione polinomiale: punti blu \(=\) dati di training (\(\mathrm  {TR}\)), punti rossi \(=\) fuori da \(\mathrm  {TR}\); la linea verde mostra il fit del modello: lineare (sinistra), polinomio di grado \(2\) (centro) e polinomio di grado \(5\) (destra).}}{86}{figure.7.7}\protected@file@percent }
\newlabel{fig:ovfit_underfit_rightfit}{{7.7}{86}{Underfitting---capacità adeguata---overfitting in regressione polinomiale: punti blu \(=\) dati di training (\(\mathrm {TR}\)), punti rossi \(=\) fuori da \(\mathrm {TR}\); la linea verde mostra il fit del modello: lineare (sinistra), polinomio di grado \(2\) (centro) e polinomio di grado \(5\) (destra)}{figure.7.7}{}}
\@writefile{toc}{\contentsline {paragraph}{Varianza.}{86}{section*.33}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {7.8}{\ignorespaces Trade-off tra bias e varianza in funzione della complessità del modello.}}{86}{figure.7.8}\protected@file@percent }
\newlabel{fig:bias_variance_tradeoff_curves}{{7.8}{86}{Trade-off tra bias e varianza in funzione della complessità del modello}{figure.7.8}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.7.3}Parametri vs Iperparametri}{87}{subsection.7.7.3}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {7.8}Selezione del modello}{87}{section.7.8}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {7.8.1}Approccio 1: selezione statistica}{87}{subsection.7.8.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {7.8.2}Approccio 2: selezione predittiva}{87}{subsection.7.8.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {7.8.3}Validazione Holdout}{88}{subsection.7.8.3}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {7.9}{\ignorespaces Validazione Holdout: il dataset viene prima mescolato, poi diviso in training set (70\%) e test set (30\%). Il modello viene addestrato sul training set e valutato sul test set per stimare la capacità di generalizzazione.}}{88}{figure.7.9}\protected@file@percent }
\newlabel{fig:holdout_validation}{{7.9}{88}{Validazione Holdout: il dataset viene prima mescolato, poi diviso in training set (70\%) e test set (30\%). Il modello viene addestrato sul training set e valutato sul test set per stimare la capacità di generalizzazione}{figure.7.9}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.8.4}K-Fold Cross-Validation}{88}{subsection.7.8.4}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {7.10}{\ignorespaces K-Fold Cross-Validation: il dataset viene diviso in \(K\) folds. In ogni iterazione, un fold viene utilizzato come test set e gli altri \(K-1\) folds come training set. Questo processo viene ripetuto \(K\) volte, e le prestazioni del modello vengono mediate su tutte le iterazioni per ottenere una stima più robusta della capacità di generalizzazione.}}{89}{figure.7.10}\protected@file@percent }
\newlabel{fig:k_fold_cross_validation}{{7.10}{89}{K-Fold Cross-Validation: il dataset viene diviso in \(K\) folds. In ogni iterazione, un fold viene utilizzato come test set e gli altri \(K-1\) folds come training set. Questo processo viene ripetuto \(K\) volte, e le prestazioni del modello vengono mediate su tutte le iterazioni per ottenere una stima più robusta della capacità di generalizzazione}{figure.7.10}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.8.5}Leave-One-Out Cross-Validation (LOOCV)}{89}{subsection.7.8.5}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {7.11}{\ignorespaces Leave-One-Out Cross-Validation (LOOCV): in ogni iterazione, un singolo esempio viene utilizzato come test set, mentre tutti gli altri esempi vengono utilizzati come training set. Questo processo viene ripetuto per ogni esempio nel dataset.}}{90}{figure.7.11}\protected@file@percent }
\newlabel{fig:loocv}{{7.11}{90}{Leave-One-Out Cross-Validation (LOOCV): in ogni iterazione, un singolo esempio viene utilizzato come test set, mentre tutti gli altri esempi vengono utilizzati come training set. Questo processo viene ripetuto per ogni esempio nel dataset}{figure.7.11}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.8.6}Ottimizzazione degli iperparametri}{90}{subsection.7.8.6}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Grid Search.}{90}{section*.34}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Validation set.}{90}{section*.35}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {7.12}{\ignorespaces Utilizzo di un validation set per l'ottimizzazione degli iperparametri: il dataset viene diviso in training set (60\%), validation set (20\%) e test set (20\%). Il modello viene addestrato sul training set, gli iperparametri vengono ottimizzati sul validation set e infine le prestazioni del modello vengono valutate sul test set.}}{91}{figure.7.12}\protected@file@percent }
\newlabel{fig:validation_set}{{7.12}{91}{Utilizzo di un validation set per l'ottimizzazione degli iperparametri: il dataset viene diviso in training set (60\%), validation set (20\%) e test set (20\%). Il modello viene addestrato sul training set, gli iperparametri vengono ottimizzati sul validation set e infine le prestazioni del modello vengono valutate sul test set}{figure.7.12}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {7.13}{\ignorespaces K-Fold Cross Validation con ottimizzazione degli iperparametri: il dataset viene diviso in training/validation set e test set. Il training/validation set viene sottoposto a K-Fold Cross Validation per ottimizzare gli iperparametri, mentre il test set viene utilizzato solo alla fine per valutare le prestazioni finali del modello.}}{91}{figure.7.13}\protected@file@percent }
\newlabel{fig:k_fold_with_hyperparameter_tuning}{{7.13}{91}{K-Fold Cross Validation con ottimizzazione degli iperparametri: il dataset viene diviso in training/validation set e test set. Il training/validation set viene sottoposto a K-Fold Cross Validation per ottimizzare gli iperparametri, mentre il test set viene utilizzato solo alla fine per valutare le prestazioni finali del modello}{figure.7.13}{}}
\@setckpt{predictive_analysis}{
\setcounter{page}{92}
\setcounter{equation}{0}
\setcounter{enumi}{5}
\setcounter{enumii}{0}
\setcounter{enumiii}{0}
\setcounter{enumiv}{0}
\setcounter{footnote}{6}
\setcounter{mpfootnote}{0}
\setcounter{part}{0}
\setcounter{chapter}{7}
\setcounter{section}{8}
\setcounter{subsection}{6}
\setcounter{subsubsection}{0}
\setcounter{paragraph}{0}
\setcounter{subparagraph}{0}
\setcounter{figure}{13}
\setcounter{table}{0}
\setcounter{float@type}{16}
\setcounter{lstnumber}{1}
\setcounter{parentequation}{0}
\setcounter{section@level}{4}
\setcounter{Item}{13}
\setcounter{Hfootnote}{12}
\setcounter{bookmark@seq@number}{167}
\setcounter{tcbbreakpart}{1}
\setcounter{tcblayer}{0}
\setcounter{tcolorbox@number}{49}
\setcounter{tcbrastercolumn}{1}
\setcounter{tcbrasterrow}{1}
\setcounter{tcbrasternum}{1}
\setcounter{tcbraster}{0}
\setcounter{tcblisting}{0}
\setcounter{nicequotecnt}{4}
\setcounter{lstlisting}{0}
}

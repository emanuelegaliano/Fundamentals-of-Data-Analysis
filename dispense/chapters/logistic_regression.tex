\chapter{Regressione logistica}
La regressione logistica è un modello statistico utilizzato per prevedere la probabilità di un evento binario (ad esempio, successo/fallimento, sì/no) in base a una o più variabili indipendenti. A differenza di K-NN, che è \emph{non parametrizzato}, la regressione logistica è un modello \emph{parametrizzato}, il che significa che assume una forma specifica per la relazione tra le variabili indipendenti e la probabilità dell'evento.

La differenza principale tra la regressione logistica e la regressione lineare risiede nella natura della variabile dipendente. Mentre la regressione lineare è utilizzata per prevedere valori continui, la regressione logistica è progettata per gestire variabili dipendenti categoriali, in particolare binarie. Si può evincere bene questo limite quando andiamo a utilizzare un modello di regressione logistica su una variabile binaria come uomo o donna, come mostrato in figura \ref{fig:linear_reegression_binary_var}.

\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.7\textwidth]{images/linear_regression_binary_variable.png}
    \caption{Esempio di regressione lineare su variabile binaria.}
    \label{fig:linear_reegression_binary_var}
\end{figure}

\section{Modello di regressione logistica}
Il problema princpale è: come trovare quella funzione che permette di fare regressione su una varabile binaria? Si potrebbe utilizzare un classificatore a soglia, ma questo avrebbe dei problemi quando ci sono delle zone di \textbf{overlap} tra le classi. 

\subsection{Funzione logistica}
La \emph{funzione logistica} (o funzione sigmoide) è una soluzione al problema, in quanto crea una curva a forma di S tra due asintoti orizzontali (0 e 1), che rappresentano le probabilità delle due classi. La funzione logistica è definita come:
\[
P(Y=1|X) = \frac{1}{1 + e^{-x}}
\]

\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.7\textwidth]{images/logistic_function.png}
    \caption{Esempio di funzione logistica. Come si evince dalla figura, la funzione logistica è limitata tra 0 e 1, rendendola adatta per modellare probabilità.}
    \label{fig:logistic_function}
\end{figure}

\subsection{Modello di regressione logistica}
Si può definire il modello $f$ di regressione logistica come:
\[
P(Y=1|X) = f(\beta_0 + \beta_1 X) = \frac{1}{1 + e^{-(\beta_0 + \beta_1 X)}}
\]
o nel caso più generale della regressione logistica multipla:
\[
P(Y=1|X) = f(\beta_0 + \beta_1 X_1 + \beta_2 X_2 + ... + \beta_n X_n) = \frac{1}{1 + e^{-(\beta_0 + \beta_1 X_1 + \beta_2 X_2 + ... + \beta_n X_n)}}
\]
dove $\beta_0$ è l'intercetta, il vettore $\beta = (\beta_1, \beta_2, ..., \beta_n)$ rappresenta i coefficienti delle variabili indipendenti $X = (X_1, X_2, ..., X_n)$.

\subsection{Odds}
Un concetto importante nella regressione logistica è quello degli \emph{odds}, ovvero il rapporto tra la probabilità di successo e la probabilità di insuccesso. Gli odds sono definiti come:
\[
\text{Odds} = \frac{P(Y=1|X)}{P(Y=0|X)} = \frac{P(Y=1|X)}{1 - P(Y=1|X)}
\]

L'odd si può interpretare come la probabilità che un evento si verifichi rispetto alla probabilità che non si verifichi. Ad esempio, se la probabilità di successo è 0.8, allora la probabilità di insuccesso è 0.2, e gli odds sono:
\[
\text{Odds} = \frac{0.8}{0.2} = 4
\]
Ciò significa che l'evento di successo è quattro volte più probabile dell'evento di insuccesso.

La funzione logistica può essere riscritta in termini di odds:
\[
p = \frac{1}{1+e^{-x}} \Rightarrow p + pe^{-x} = 1 \Rightarrow pe^{-x} = 1 - p \Rightarrow \frac{p}{1-p} = e^{x}
\]
e da qui:
\[
e^{\beta_0 + \mathbf{\beta} \cdot \mathbf{x}} = \frac{P(Y=1|X)}{1 - P(Y=1|X)}
\]

Il termine a destra rappresenta gli odds, mentre il termine a sinistra è l'esponenziale della combinazione lineare delle variabili indipendenti. Questo mostra come la regressione logistica modella gli odds in funzione delle variabili indipendenti.

\subsection{Log-odds}
Prendendo il logaritmo naturale degli odds, otteniamo i \emph{log-odds} (o logit):
\[
\log\left(\frac{P(Y=1|X)}{1 - P(Y=1|X)}\right) = \beta_0 + \mathbf{\beta} \cdot \mathbf{x}
\]
I log-odds rappresentano una trasformazione lineare della probabilità, rendendo più semplice l'interpretazione dei coefficienti $\beta$. Ogni incremento unitario in una variabile indipendente $X_i$ comporta un cambiamento di $\beta_i$ nei log-odds.

Quindi, la regressione logistica può essere vista come un modello di regressione lineare applicato ai log-odds della probabilità dell'evento di interesse. Questo consente di stimare l'effetto delle variabili indipendenti sulla probabilità dell'evento, facilitando l'interpretazione e l'analisi dei risultati.

\section{Stima dei parametri}

Per stimare i parametri del modello di regressione logistica, dobbiamo definire una loss function adeguata, analoga a quella utilizzata per la regressione lineare. Tuttavia, poiché la regressione logistica ha una natura probabilistica e l'output desiderato è una variabile binaria, l'errore quadratico non è una scelta appropriata. Il modello produce infatti una probabilità tramite la funzione sigmoide, e ciò richiede una funzione di costo coerente con tale interpretazione.

\subsection{Interpretazione probabilistica del modello}

Ricordiamo che il modello di regressione logistica associa a ciascuna osservazione $\mathbf{x}$ la probabilità stimata che appartenga alla classe positiva:
\[
f(\mathbf{x}) = \sigma(\beta_0 + \beta_1 x_1 + \dots + \beta_n x_n),
\]
dove $\sigma$ è la funzione sigmoide. Possiamo quindi interpretare $f(\mathbf{x}_i)$ come la probabilità che $y_i = 1$. Di conseguenza, la probabilità di osservare un'etichetta $y_i \in \{0,1\}$ è:
\[
P(y_i \mid \mathbf{x}_i, \beta) = f(\mathbf{x}_i)^{y_i} (1 - f(\mathbf{x}_i))^{1 - y_i}.
\]

\subsection{Cross-entropy loss}
La funzione di costo più comune per la regressione logistica è la \emph{cross-entropy loss}, che misura la differenza tra le probabilità previste dal modello e le etichette osservate.

Possiamo stimare i parametri utilizzando la log-likehood, ovvero scegliamo i parametri che massimizzano la probabilità di osservare i dati secondo il modello definito dai parametri stessi:
\[
L(\beta) = P(Y \mid X; \beta)
\]
dove $Y$ è il vettore delle etichette osservate e $X$ è la matrice delle caratteristiche. Questo significa che scegliamo gli elementi $\beta_i \in \beta$ tali che il modello assegni probabilità alte ai valori realmente osservati.

Si può trasformare la log-likehood in una somma logaritmica per semplificare i calcoli:
\[
L(\beta) = \prod_{i=1}^N P(y^{(i)} \mid \mathbf{x}^{(i)}; \beta) = \prod_{i=1}^N f_\beta \left( \mathbf{x}^{(i)} \right)^{y^{(i)}} \left( f_\beta \left( \mathbf{x}^{(i)} \right) \right)^{1 - y^{(i)}}
\]

Massimizzare questa espressione è equivalente a minimizzare il logaritmo negativo di $L(\beta)$ (nll), che porta:
\[
\begin{aligned}
nll(\beta) 
&= -\log L(\beta) \\
&= -\sum_{i=1}^N 
    \log \left[
        f_\beta\!\left( \mathbf{x}^{(i)} \right)^{y^{(i)}}
        \left( 1 - f_\beta\!\left( \mathbf{x}^{(i)} \right) \right)^{1 - y^{(i)}}
    \right] \\
&= -\sum_{i=1}^N 
    \left[
        y^{(i)} \log f_\beta\!\left( \mathbf{x}^{(i)} \right)
        + 
        (1 - y^{(i)}) \log\!\left( 1 - f_\beta\!\left( \mathbf{x}^{(i)} \right) \right)
    \right]
\end{aligned}
\]

\noindent
Da qui, possiamo definire la nostra funzione di costo come:
\[
J(\beta) = - \sum_{i=1}^N \left[ y^{(i)} \log f_\beta \left( \mathbf{x}^{(i)} \right) + (1 - y^{(i)}) \log \left( 1 - f_\beta \left( \mathbf{x}^{(i)} \right) \right) \right]
\]

Ora abbiamo una funzione di costo che possiamo minimizzare utilizzando metodi di ottimizzazione numerica, come la discesa del gradiente. La minimizzazione di questa funzione di costo ci permetterà di trovare i valori ottimali dei parametri $\beta$ che meglio si adattano ai dati osservati.

\subsection{Visualizzazione della cross-entropy loss}
\begin{figure}[htbp]
    \centering
    \includegraphics[width=\textwidth]{images/loss_landscape_logistic.png}
    \caption{Visualizzazione della superficie della cross-entropy loss per un modello di regressione logistica con due parametri ($\beta_0$ e $\beta_1$). A sinistra è rappresentata la superficie tridimensionale della loss, che evidenzia la presenza di un unico minimo globale. A destra sono riportate le curve di livello della stessa funzione, che mostrano come la loss aumenti progressivamente man mano che ci si allontana dal minimo. Il punto rosso indica la combinazione di parametri che minimizza la loss, corrispondente alla stima $\hat{\beta}$ ottenuta tramite massima verosimiglianza.}
    \label{fig:loss_landscape_logistic}
\end{figure}

Per comprendere meglio il comportamento della funzione di costo della regressione logistica, è utile analizzare come la \emph{cross-entropy loss} vari al variare dei parametri del modello. Nel caso di un modello con una sola variabile esplicativa, la funzione di costo dipende da due soli parametri, $\beta_0$ (intercetta) e $\beta_1$ (coefficiente), e può quindi essere rappresentata graficamente nello spazio bidimensionale dei parametri. La superficie tridimensionale mostra come la loss cresca rapidamente quando i parametri si allontanano dalla combinazione ottimale, mentre il grafico delle curve di livello (contour plot) permette di individuare più chiaramente la regione in cui la funzione assume i valori minimi.

Questa rappresentazione aiuta a visualizzare un concetto fondamentale: la funzione di costo della regressione logistica è \emph{convessa} rispetto ai parametri (in particolare alla massima verosimiglianza), il che garantisce l'esistenza di un unico minimo globale. Di conseguenza, metodi iterativi come la \emph{gradient descent} o il \emph{Newton-Raphson} convergeranno sempre alla stessa soluzione ottimale, rendendo l'ottimizzazione stabile ed efficace. La forma “a scodella” della superficie conferma che piccoli cambiamenti nei parametri vicino al minimo producono variazioni limitate della loss, mentre modifiche più grandi portano rapidamente ad un aumento significativo dell'errore.

\section{Interpretazione statistica dei coefficienti}
Come detto in precedenza, i coefficienti della regressione logistica possono essere interpretati in termini di log-odds. In particolare, ogni coefficiente $\beta_i$ rappresenta il cambiamento nei log-odds associato a un incremento unitario nella variabile indipendente $X_i$, mantenendo costanti tutte le altre variabili. Questo perché la regressione logistica modella i log-odds come una combinazione lineare delle variabili indipendenti.

\subsection{Interpretazione dell'intercetta}
L'intercetta $\beta_0$ può essere interpretata come i log-odds quando tutte le variabili indipendenti sono uguali a zero. Infatti, ponendo tutte le variabili indipendenti a zero, possiamo scrivere:
\[
\log(\frac{p}{1-p}) = \beta_0 + \beta \cdot \mathbf{x}
\]
da qui:
\[
x = 0 \Rightarrow \log(\frac{p}{1-p}) = \beta_0 \Rightarrow \frac{p}{1-p} = e^{\beta_0}
\]
Ricordando che $\frac{p}{1-p}$ sono gli odds, possiamo affermare che $e^{\beta_0}$ rappresenta gli odds quando tutte le variabili indipendenti sono uguali a zero. Quindi, per $x=0$ possiamo dire che sono $e^{\beta_0}$ volte più probabili gli eventi di successo rispetto agli eventi di insuccesso.

\subsection{Interpretazione dei coefficienti delle variabili indipendenti}
Ogni coefficiente $\beta_i$ rappresenta il cambiamento nei log-odds associato a un incremento unitario nella variabile indipendente $X_i$, mantenendo costanti tutte le altre variabili. Possiamo scrivere che:
\[
odds(p \mid x) = \frac{P(Y=1 \mid X=x)}{1 - P(Y=1 \mid X=x)} = e^{\beta_0 + \beta \mathbf{x}}
\]
quindi
\[
\log (odds(p \mid x+1)) - \log (odds (p \mid x)) = \beta_i
\]
e da qui:
\[
\begin{aligned}
e^{\,\log \mathrm{odds}(p \mid x + \mathbf{e}_i) - \log \mathrm{odds}(p \mid x)}
&= e^{\beta_i}
\;\Rightarrow\;
\frac{e^{\,\log \mathrm{odds}(p \mid x + \mathbf{e}_i)}}%
     {e^{\,\log \mathrm{odds}(p \mid x)}}
= e^{\beta_i}
\\[6pt]
\Rightarrow\;
\frac{\mathrm{odds}(p \mid x + \mathbf{e}_i)}%
     {\mathrm{odds}(p \mid x)}
&= e^{\beta_i}
\;\Rightarrow\;
\mathrm{odds}(p \mid x + \mathbf{e}_i)
= e^{\beta_i}\,\mathrm{odds}(p \mid x)
\end{aligned}
\]

Questo significa che per ogni incremento unitario nella variabile indipendente $X_i$, gli odds di successo vengono moltiplicati per un fattore di $e^{\beta_i}$. Se $\beta_i$ è positivo, allora un aumento di $X_i$ aumenta gli odds di successo, mentre se $\beta_i$ è negativo, un aumento di $X_i$ diminuisce gli odds di successo.

Nel caso della regressione logistica semplice (con una sola variabile indipendente), l'aumento di un'unità in $X$ comporta un cambiamento di $e^{\beta_1}$ negli odds di successo. Ad esempio, se $\beta_1 = 0.5$, allora un incremento unitario in $X$ moltiplica gli odds di successo per $e^{0.5} \approx 1.65$, indicando che gli odds aumentano del 65\%.

\section{Valutazione della regressione logistica}
Per valutare un modello di regressione lineare si utilizzava $R^2$, ma questo non è adatto per la regressione logistica, in quanto la variabile dipendente è binaria. Oltretutto noi non andiamo a minimizzare l'errore quadratico medio, ma la cross-entropy loss.

\subsection{Pseudo $R^2$}
Per valutare la bontà di adattamento di un modello di regressione logistica, si possono utilizzare diverse misure di pseudo $R^2$. Una delle più comuni è il \emph{McFadden's $\mathbf{R^2}$}, definito come:
\[
R_{McFadden}^2 = 1 - \frac{\log L_{model}}{\log L_{null}}
\]
dove $\log L_{model}$ è il logaritmo della verosimiglianza del modello stimato e $\log L_{null}$ è il logaritmo della verosimiglianza del modello nullo (un modello senza variabili indipendenti).

\section{Regressione logistica multiclasse}
Esistono dei casi dove la variabile dipendente non è binaria, ma può assumere più di due valori (ad esempio, classificazione di immagini in più categorie). In questi casi, si può utilizzare la \emph{regressione logistica multiclasse} (o multinomiale), che estende il concetto di regressione logistica a più classi.

\subsection{Modello di regressione logistica multiclasse}
Nel caso della regressione logistica multiclasse per $K$ classi, il modello stima la probabilità che un'osservazione appartenga a ciascuna delle $K$ classi. La probabilità che un'osservazione $\mathbf{x}$ appartenga alla classe $k$ è data da:
\[
P(Y = k \mid X = x) =
\frac{e^{\beta_k^T x}}{\sum_{j=1}^K e^{\beta_j^T x}}
\]
per $k = 1, 2, \ldots, K$, dove $\beta_k$ sono i coefficienti associati alla classe $k$.

\subsection{Interpretazione geometrica dei coefficienti}
In un modello di regressione logistica multiclasse, ogni classe $k$ è associata a un vettore di coefficienti $\beta_k$. Questi vettori definiscono delle iperpiani nello spazio delle caratteristiche che separano le diverse classi. La decisione su quale classe assegnare a una nuova osservazione dipende dalla distanza dell'osservazione da questi iperpiani.
In particolare, per una nuova osservazione $\mathbf{x}$, si calcolano i prodotti scalari $\beta_k^T \mathbf{x}$ per ogni classe $k$. L'osservazione viene assegnata alla classe con il valore massimo di questo prodotto scalare, che corrisponde alla massima probabilità stimata.

\paragraph{Decision boundary.}
La \emph{decision boundary} tra due classi $k$ e $j$ è definita dall'insieme dei punti $\mathbf{x}$ per i quali le probabilità stimate delle due classi sono uguali, ovvero quando è indeciso (quindi quando la probabilità è $0.5$):
\[
P(Y=k \mid X = x) = P(Y=j \mid X = x)
\]

Ricordando che la regressione logistica definisce iperpiani nello spazio delle caratteristiche, la decision boundary tra due classi sarà un iperpiano che separa le regioni dello spazio in cui il modello predice ciascuna delle due classi:
\[
P(y = 1 \mid x) = 0.5 \Leftrightarrow e^{-(\beta_0 + \beta_1 x_1 + ... + \beta_n x_n)} = 1 \Leftrightarrow \beta_0 + \beta_1 x_1 + ... + \beta_n x_n = 0
\]
e questa è l'equazione di un iperpiano nello spazio delle caratteristiche. Si può rendere in forma esplicita come:
\[
x_n = -\frac{\beta_0}{\beta_n} - \frac{\beta_1}{\beta_n} x_1 - ... - \frac{\beta_{n-1}}{\beta_n} x_{n-1}
\]
dove il coefficiente angolare di ciascuna variabile $x_i$ è dato da $-\frac{\beta_i}{\beta_n}$ e l'intercetta è $-\frac{\beta_0}{\beta_n}$.

\begin{figure}
    \centering
    \includegraphics[width=0.7\textwidth]{images/multiclass_logistic_regression.png}
    \caption{Esempio di classificazione binaria ottenuto tramite regressione logistica lineare. I punti rappresentano le osservazioni appartenenti alle due classi, mentre la linea rossa indica la \emph{decision boundary}, ovvero l'insieme dei punti per i quali il modello assegna una probabilità pari a 0.5. Tale frontiera separa le regioni dello spazio delle caratteristiche in cui il modello predice rispettivamente la classe 0 o la classe 1.}
    \label{fig:multiclass_logistic_regression}
\end{figure}

\section{Regressione softmax}
La regressione softmax è un'estensione della regressione logistica multiclasse che viene utilizzata quando la variabile dipendente può assumere più di due classi. Mentre la regressione logistica binaria utilizza la funzione sigmoide per modellare la probabilità di appartenenza a una classe, la regressione softmax utilizza la funzione softmax per modellare le probabilità di appartenenza a ciascuna delle $K$ classi.

\subsection{Funzione softmax}
A differenza della regressione logistica multiclasse, che può essere vista come una serie di modelli binari indipendenti, la regressione softmax considera tutte le classi contemporaneamente, garantendo che le probabilità stimate per tutte le classi sommino a 1. La funzione softmax è definita come:
\[
P(Y = k \mid X = x) = \frac{e^{\beta_k^T x}}{\sum_{j=1}^K e^{\beta_j^T x}}
\]
per $k = 1, 2, \ldots, K$, dove $\beta_k$ sono i coefficienti associati alla classe $k$.

\subsection{Interpretazione geometrica dei coefficienti}
La regressione Softmax estende la regressione logistica al caso multiclasse, e ciò comporta una serie di implicazioni geometriche particolarmente interessanti. In questo modello, a ciascuna classe $k$ è associato un vettore di coefficienti $\beta_k$, che definisce una direzione nello spazio delle caratteristiche. L'osservazione viene assegnata alla classe per la quale il prodotto scalare $\beta_k^\top \mathbf{x}$ risulta maggiore, poiché questo valore determina la probabilità stimata per la classe stessa.

Un concetto centrale è quello della \emph{decision boundary}. La frontiera di decisione tra due classi $i$ e $j$ è formata dall'insieme dei punti $\mathbf{x}$ per cui il modello risulta perfettamente indeciso, cioè quando la probabilità di appartenere alla classe $i$ è esattamente uguale a quella della classe $j$. In questo caso, l'equazione che determina la decision boundary assume una forma molto semplice:
\[
\beta_i^\top \mathbf{x} = \beta_j^\top \mathbf{x}
\]
che può essere riscritta come:
\[
(\beta_i - \beta_j)^\top \mathbf{x} = 0.
\]

\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.7\textwidth]{images/softmax_regression.png}
    \caption{Esempio di regressione logistica multiclasse con tre classi, ottenuta tramite softmax regression. Le regioni colorate rappresentano le aree dello spazio delle caratteristiche in cui il modello assegna la probabilità più alta rispettivamente alla classe 0 (blu), alla classe 1 (arancione) e alla classe 2 (verde). Le linee tratteggiate rappresentano le \emph{decision boundary} analitiche tra ciascuna coppia di classi (0 vs 1, 0 vs 2, 1 vs 2), ottenute imponendo l'uguaglianza delle probabilità predette $P(Y=i\mid x)=P(Y=j\mid x)$. Le tre frontiere, combinate, suddividono lo spazio in tre regioni lineari, ciascuna associata alla classe con il valore massimo di $\beta_k^\top x$.}
    \label{fig:softmax_regression}
\end{figure}

Questa relazione mostra chiaramente che la frontiera tra due classi è un \emph{iperpiano} nello spazio delle caratteristiche. Dal punto di vista geometrico, la regressione Softmax suddivide lo spazio in regioni lineari, ognuna delle quali è associata a una delle classi. Ogni iperpiano separa due regioni adiacenti, definendo quindi la struttura complessiva del processo di classificazione.

In questo contesto, i vettori dei coefficienti $\beta_k$ non vanno interpretati solo come parametri numerici, ma come vere e proprie “direzioni preferenziali”: un valore elevato di $\beta_k$ in una certa direzione indica che, muovendosi lungo quella direzione, aumenta la probabilità di assegnare l'osservazione alla classe $k$. Allo stesso modo, la posizione relativa degli iperpiani determinati dai vari vettori $\beta_k$ stabilisce come lo spazio venga partizionato. Pertanto, variazioni nei coefficienti influiscono direttamente sulla forma e sull'inclinazione delle regioni decisionali.

\section{Altri modi di regressione multiclasse}
Esistono altri modi per estendere la regressione logistica al caso multiclasse, come il \emph{one-vs-rest} (OvR) e il \emph{one-vs-one} (OvO). Questi approcci suddividono il problema multiclasse in una serie di problemi binari, che possono essere risolti utilizzando la regressione logistica standard.

\subsection{One-vs-Rest (OvR)}
Nell'approccio One-vs-Rest, per ogni classe $k$, si addestra un modello di regressione logistica binaria che distingue la classe $k$ da tutte le altre classi. In questo modo, si ottengono $K$ modelli binari, uno per ciascuna classe. Durante la fase di predizione, si calcolano le probabilità per ciascun modello e si assegna l'osservazione alla classe con la probabilità più alta.

Per esempio, se abbiamo tre classi (A, B, C), si addestrano tre modelli:
\begin{itemize}
    \item Modello 1: Classe A vs Non-A (B e C)
    \item Modello 2: Classe B vs Non-B (A e C)
    \item Modello 3: Classe C vs Non-C (A e B)
\end{itemize}

\subsection{One-vs-One (OvO)}
Nell'approccio One-vs-One, si addestra un modello di regressione logistica binaria per ogni coppia di classi. Questo significa che per $K$ classi, si addestrano $\frac{K(K-1)}{2}$ modelli binari. Durante la fase di predizione, si utilizza un sistema di voto: ogni modello vota per una delle due classi che distingue, e l'osservazione viene assegnata alla classe che riceve il maggior numero di voti.

Per esempio, se abbiamo tre classi (A, B, C), si addestrano tre modelli:
\begin{itemize}
    \item Modello 1: Classe A vs Classe B
    \item Modello 2: Classe A vs Classe C
    \item Modello 3: Classe B vs Classe C
\end{itemize}
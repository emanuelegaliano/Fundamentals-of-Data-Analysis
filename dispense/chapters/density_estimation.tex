\chapter{Stima della densità}
L'obiettivo degli algoritmi di clustering è quello di raggruppare le strutture interne dei dati in insiemi omogenei. K-means, per esempio, assume che i cluster abbiano una forma sferica e che siano separati da confini lineari. 

Un altro approccio, più flessibile, è quello basato sulla stima della densità di probabilità che ha generato i dati: i cluster vengono identificati come regioni ad alta densità separate da regioni a bassa densità. Questo approccio è particolarmente utile quando i cluster hanno forme complesse o non lineari.

\section{Densità di probabilità}
La densità di probabilità (PDF, Probability Density Function) è una \textbf{funzione continua} che descrive la probabilità relativa di una variabile casuale continua di assumere un certo valore. Per stimarla esistono due metodi principali: Metodi non parametrici e Metodi parametrici.
\begin{description}
    \item[Metodi non parametrici.] - 
    \item[Metodi parametrici.] - Questi metodi provano a stimare i parametri della \textbf{distribuzione di partenza} che si assume abbia generato i dati. Un vantaggio di questi metodi è che, una volta stimati i parametri, abbiamo una forma chiusa della funzione di densità. Uno svantaggio è che spesso richiedono molti iperparametri da regolare e che le assunzioni fatte sulla distribuzione di partenza potrebbero non essere corrette.
\end{description}

\section{Metodi non parametrici}
Questi metodi cercano di stimarla direttamente \textbf{dai dati}, senza forti assunzioni o la distribuzione di partenza. Il più grande vantaggio di questi metodi è che hanno pochi iperparametri da regolare, uno svantaggio è che, anche se possiamo computare numericamente $f(x)$, non abbiamo una forma chiusa della funzione.

\subsection{Istogrammi}
Un istogramma è una rappresentazione grafica della distribuzione di un insieme di dati (vedere sezione \ref{sec:istogramma}). I dati non vengono semplicemente contati, in quanto potrebbe dire che un \emph{bin} 4 volte più largo di un altro contiene 4 volte più dati, ma viene calcolata la PDF $f(x)$ soddisfando una regola: il volume totale sotto la curva deve essere uguale a 1:
\[
\int f(x) dx = 1
\]

Per fare questo, dobbiamo definire la densità $f(R_i)$, per ogni \emph{bin} $R_i$, come la probabilità di quel \emph{bin} diviso il suo \emph{volume} $V_i$:
\[
f(x \in R_i) = \frac{P(R_i)}{V(R_i)} = \frac{|R_i|/N}{V(R_i)}
\]
dove $|R_i|$ è il numero di punti nel \emph{bin} $R_i$ e $N$ è il numero totale di punti. Dividendo per il volume ci assicuriamo che un \emph{bin} più largo non abbia una densità più alta solo perché è più largo e ci assicuriamo che l'integrale della densità sia uguale a 1:
\[
P(x \in \bigcup_i R_i) = \sum_i P(R_i) = \sum_i f(x \in R_i) V(R_i) = \sum_i \frac{|R_i|/N}{V(R_i)} = \frac{1}{N} \sum_i |R_i| = 1
\]

Questo approccio ha 2 \emph{iperparametri} principali:
\begin{itemize}
    \item Numero di \emph{bin}: Un numero troppo basso di \emph{bin} porta a una stima troppo grossolana della densità, mentre un numero troppo alto porta a una stima troppo rumorosa.
    \item Posizione della griglia dei \emph{bin}: La posizione della griglia può influenzare significativamente la stima della densità, specialmente se i dati hanno strutture a scale diverse.
\end{itemize}

Un'alternativa ai bin quadrati sono gli \emph{hexbin} (vedere sezione \ref{sec:hexbinplot}), che usano esagoni invece di quadrati per i bin. Gli esagoni hanno il vantaggio di avere una distanza più uniforme tra il centro e i lati rispetto ai quadrati, riducendo così la varianza nella stima della densità.

\subsection{Kernel Density Estimation (KDE)}
Gli istogrammi hanno due problemi:
\begin{itemize}
    \item I confini rigidi dei \emph{bin} possono introdurre discontinuità nella stima della densità.
    \item La stima della densità dipende fortemente dalla posizione della griglia dei \emph{bin} e, per il motivo sopra, "a quadretti".
\end{itemize}

Un'approccio più sofisticato è quello della \textbf{stima della densità con kernel} (KDE, Kernel Density Estimation). Invece di contare i punti in ogni \emph{bin}, KDE posiziona una funzione \textbf{kernel} centrata su ogni punto dati e somma i contributi di tutti i kernel per ottenere la stima della densità.

\subsubsection*{Kernel circolare}
Un modo "naive" di implementare KDE è quello di centrare una finestra circolare di raggio $h$, la \emph{bandwith}, su un certo punto $x$ e calcolare la densità basata su quanti punti $N(x, h)$:
\[
N(x, h) = \{x' \in X\ s.t.\ ||x' - x||_2 \leq h \}
\]

cadono all'interno di questa finestra (la logica è la stessa dell'istogramma):
\[
f(x) = \frac{|N(x, h)|/N}{V(h)}
\]
dove $V(h)$ è il volume della finestra circolare (in 2D, l'area del cerchio di raggio $h$: $V(h) = \pi h^2$) e $N$ è il numero totale di punti. Dalla definizione dell'area del cerchio, possiamo vedere che:
\[
\int f(x) dx = \int \frac{|N(x, h)|/N}{V(h)} dx = \frac{1}{N V(h)} \int |N(x, h)| dx = \frac{1}{N V(h)} \cdot N \cdot V(h) = 1
\]

\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.8\textwidth]{images/kde_circle.png}
    \caption{Esempio di KDE con kernel circolare. Ogni punto dati contribuisce alla stima della densità all'interno della sua finestra circolare.}
    \label{fig:kde_circle}
\end{figure}

\paragraph{Kernel view.}
Un modo più generale di vedere KDE è quello di scrivere l'espressione $f(x)$ come
\[
f(x) = \frac{1}{|X|} \sum_{i=1}^{|X|} K_h (x_i - x)
\]
dove $x_i$ sono i punti dati, $|X|$ è il numero totale di punti e $K_h$ è definita come
\[
K_h (\mathbf{x}_i - \mathbf{x}) = \begin{cases}
\frac{1}{V(h)} & \text{se } ||x_i - x||_2 \leq h \\
0 & \text{altrimenti}
\end{cases}
\]
ovvero la funzione kernel che assegna un peso uniforme $\frac{1}{V(h)}$ ai punti all'interno della finestra circolare di raggio $h$ e 0 altrimenti. Questa dipende dal parametro $h$, che controlla la larghezza della finestra (la \emph{bandwith}). 

Uno dei problemi con questo tipo di kernel è che è molto \textbf{sensibile} alla posizione dei punti dati: piccoli cambiamenti nella posizione dei punti possono portare a grandi cambiamenti nella stima della densità. Possiamo notare che, poiché il kernel fa un salto netto da $\frac{1}{V(h)}$ a 0 al bordo della finestra, la stima della densità sarà discontinua in quei punti.

\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.8\textwidth]{images/kde_kernel.png}
    \caption{Kernel View nella stima di densità tramite Kernel Density Estimation (KDE). 
    Il grafico mostra il valore del kernel $K_h(\lVert x - x_i \rVert)$ in funzione della distanza normalizzata 
    $\lVert (x - x_i)/h \rVert_2$. I punti verdi contribuiscono alla stima con peso non nullo, mentre i punti rossi,
    posti oltre il supporto compatto del kernel, hanno peso nullo.}
    \label{fig:kde_kernel}
\end{figure}

\paragraph{Problemi del kernel circolare.}
Il kernel circolare ha un problema principale: fa decisioni \textbf{nette}, il punto si trova o non si trova all'interno della finestra. Questo porta a stime della densità che sono \textbf{discontinue} e \textbf{sensibili} alla posizione dei punti dati. 

Un modo di risolvere questo è utilizzare dei kernel più \emph{smooth} (morbidi), che assegnano pesi decrescenti ai punti man mano che si allontanano dal centro del kernel, invece di un peso uniforme all'interno di una finestra rigida. Un esempio è il \textbf{Kernel Gaussiano}:
\[
K_h(\mathbf{x}_i - \mathbf{x}) \propto \exp\left(-\frac{\|\mathbf{x}_i - \mathbf{x}\|^2}{2h^2}\right)
\]
Questo kernel assegna pesi più alti ai punti vicini al centro e pesi più bassi ai punti lontani, producendo una stima della densità più liscia e meno sensibile alla posizione dei punti dati.

\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.8\textwidth]{images/kde_gaussian.png}
    \caption{Kernel View per la Kernel Density Estimation (KDE) con kernel gaussiano. 
    Il valore del kernel $K_h(\lVert x - x_i \rVert)$ decresce in modo continuo all’aumentare della distanza 
    normalizzata $\lVert (x - x_i)/h \rVert_2$. A differenza dei kernel a supporto compatto, tutti i punti
    contribuiscono alla stima con peso non nullo, evidenziato dalla scala cromatica.}
    \label{fig:kde_gaussian}
\end{figure}

\subsection{Epanechnikov Kernel}
Il kernel guassiano risolve il problema della discontinuità, ma ha un supporto infinito, il che significa che ogni punto dati contribuisce alla stima della densità in ogni punto dello spazio, anche se con un peso molto piccolo. Questo può essere computazionalmente inefficiente.

Un'alternativa è l'\textbf{Epanechnikov Kernel}, che pone a 0 il peso dei punti oltre una certa distanza, mantenendo però una transizione più morbida rispetto al kernel circolare:
\[
K_h (\mathbf{x}_i - \mathbf{x}) = \frac{3}{4h^2} \left(  1- \frac{||\mathbf{x}_i - \mathbf{x}||^2}{h^2} \right) \mathbb{I}\left( ||\mathbf{x}_i - \mathbf{x}||_2 \leq h \right)
\]
dove $\mathbb{I}$ è la funzione indicatrice che vale 1 se la condizione è vera e 0 altrimenti. Questo kernel assegna pesi decrescenti ai punti man mano che si allontanano dal centro, ma pone a 0 il peso dei punti oltre la distanza $h$.

\begin{figure}[htbp]
    \centering
    \includegraphics[width=1\textwidth]{images/kde_nonparametric_comparison.png}
    \caption{Confronto in Kernel View tra diversi kernel utilizzati nella Kernel Density Estimation (KDE).
    Da sinistra a destra: kernel uniforme (radiale), kernel di Epanechnikov e kernel gaussiano.
    Il grafico mostra il peso $K_h(\lVert x - x_i \rVert)$ in funzione della distanza normalizzata
    $\lVert (x - x_i)/h \rVert_2$. I kernel uniforme ed Epanechnikov presentano supporto compatto,
    assegnando peso nullo ai punti oltre il raggio unitario, mentre il kernel gaussiano assegna
    peso non nullo a tutti i punti con contributo decrescente all’aumentare della distanza.}
    \label{fig:kde_nonparametric_comparison}
\end{figure}

\begin{figure}[htbp]
    \centering
    \includegraphics[width=1\textwidth]{images/kde_nonparametric_comparison2.png}
    \caption{Visualizzazione geometrica della Kernel Density Estimation (KDE) in $\mathbb{R}^2$ per diversi kernel con banda $h=1$.
    Da sinistra a destra: kernel circolare (uniforme), kernel di Epanechnikov e kernel gaussiano.
    Il punto di valutazione $x$ è indicato con una croce blu, mentre il cerchio rappresenta la regione
    $\lVert x - x_i \rVert_2 \le h$. I punti all’interno del supporto compatto contribuiscono alla stima
    con peso non nullo per i kernel circolare ed Epanechnikov, mentre il kernel gaussiano assegna un peso
    decrescente a tutti i punti, inclusi quelli esterni al raggio $h$.}
    \label{fig:kde_nonparametric_comparison2}
\end{figure}

\subsection{Tradeoff bias-varianza: sccelta della bandwith}
La scelta del kernel non è l'unico aspetto importante nella KDE, l'unico iperparametro della KDE è la \textbf{bandwith} $h$, che controlla la larghezza del kernel. La scelta di $h$ ha un impatto significativo sulla stima della densità:
\begin{itemize}
    \item Un valore di $h$ troppo piccolo porta a una stima della densità rumorosa, con molte fluttuazioni (alta varianza).
    \item Un valore di $h$ troppo grande porta a una stima della densità troppo liscia, che può perdere dettagli importanti (alto bias).
\end{itemize}

\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.9\textwidth]{images/kde_bandwidth_comparison.png}
    \caption{Effetto del parametro di banda $h$ nella Kernel Density Estimation (KDE) e relativo trade-off bias–varianza.
    Per valori piccoli di $h$ (in alto a sinistra) la stima presenta bassa distorsione ma alta varianza,
    con strutture molto frammentate e sensibili al rumore.
    All’aumentare di $h$ la varianza diminuisce e la stima diventa più liscia,
    ma un eccessivo smoothing (in basso a destra) introduce bias,
    fondendo strutture distinte e perdendo dettagli locali.
    La scelta del bandwidth governa quindi l’equilibrio tra fedeltà ai dati e regolarità della stima.}
    \label{fig:kde_bandwidth_comparison}
\end{figure}
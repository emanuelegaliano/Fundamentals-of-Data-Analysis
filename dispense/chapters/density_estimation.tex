\chapter{Stima della densità}
L'obiettivo degli algoritmi di clustering è quello di raggruppare le strutture interne dei dati in insiemi omogenei. K-means, per esempio, assume che i cluster abbiano una forma sferica e che siano separati da confini lineari. 

Un altro approccio, più flessibile, è quello basato sulla stima della densità di probabilità che ha generato i dati: i cluster vengono identificati come regioni ad alta densità separate da regioni a bassa densità. Questo approccio è particolarmente utile quando i cluster hanno forme complesse o non lineari.

\section{Densità di probabilità}
La densità di probabilità (PDF, Probability Density Function) è una \textbf{funzione continua} che descrive la probabilità relativa di una variabile casuale continua di assumere un certo valore. Per stimarla esistono due metodi principali: Metodi non parametrici e Metodi parametrici.

\section{Metodi non parametrici}
Questi metodi cercano di stimarla direttamente \textbf{dai dati}, senza forti assunzioni o la distribuzione di partenza. Il più grande vantaggio di questi metodi è che hanno pochi iperparametri da regolare, uno svantaggio è che, anche se possiamo computare numericamente $f(x)$, non abbiamo una forma chiusa della funzione.

\subsection{Istogrammi}
Un istogramma è una rappresentazione grafica della distribuzione di un insieme di dati (vedere sezione \ref{sec:istogramma}). I dati non vengono semplicemente contati, in quanto potrebbe dire che un \emph{bin} 4 volte più largo di un altro contiene 4 volte più dati, ma viene calcolata la PDF $f(x)$ soddisfando una regola: il volume totale sotto la curva deve essere uguale a 1:
\[
\int f(x) dx = 1
\]

Per fare questo, dobbiamo definire la densità $f(R_i)$, per ogni \emph{bin} $R_i$, come la probabilità di quel \emph{bin} diviso il suo \emph{volume} $V_i$:
\[
f(x \in R_i) = \frac{P(R_i)}{V(R_i)} = \frac{|R_i|/N}{V(R_i)}
\]
dove $|R_i|$ è il numero di punti nel \emph{bin} $R_i$ e $N$ è il numero totale di punti. Dividendo per il volume ci assicuriamo che un \emph{bin} più largo non abbia una densità più alta solo perché è più largo e ci assicuriamo che l'integrale della densità sia uguale a 1:
\[
P(x \in \bigcup_i R_i) = \sum_i P(R_i) = \sum_i f(x \in R_i) V(R_i) = \sum_i \frac{|R_i|/N}{V(R_i)} = \frac{1}{N} \sum_i |R_i| = 1
\]

Questo approccio ha 2 \emph{iperparametri} principali:
\begin{itemize}
    \item Numero di \emph{bin}: Un numero troppo basso di \emph{bin} porta a una stima troppo grossolana della densità, mentre un numero troppo alto porta a una stima troppo rumorosa.
    \item Posizione della griglia dei \emph{bin}: La posizione della griglia può influenzare significativamente la stima della densità, specialmente se i dati hanno strutture a scale diverse.
\end{itemize}

Un'alternativa ai bin quadrati sono gli \emph{hexbin} (vedere sezione \ref{sec:hexbinplot}), che usano esagoni invece di quadrati per i bin. Gli esagoni hanno il vantaggio di avere una distanza più uniforme tra il centro e i lati rispetto ai quadrati, riducendo così la varianza nella stima della densità.

\subsection{Kernel Density Estimation (KDE)}
Gli istogrammi hanno due problemi:
\begin{itemize}
    \item I confini rigidi dei \emph{bin} possono introdurre discontinuità nella stima della densità.
    \item La stima della densità dipende fortemente dalla posizione della griglia dei \emph{bin} e, per il motivo sopra, "a quadretti".
\end{itemize}

Un'approccio più sofisticato è quello della \textbf{stima della densità con kernel} (KDE, Kernel Density Estimation). Invece di contare i punti in ogni \emph{bin}, KDE posiziona una funzione \textbf{kernel} centrata su ogni punto dati e somma i contributi di tutti i kernel per ottenere la stima della densità.

\subsubsection*{Kernel circolare}
Un modo "naive" di implementare KDE è quello di centrare una finestra circolare di raggio $h$, la \emph{bandwith}, su un certo punto $x$ e calcolare la densità basata su quanti punti $N(x, h)$:
\[
N(x, h) = \{x' \in X\ s.t.\ ||x' - x||_2 \leq h \}
\]

cadono all'interno di questa finestra (la logica è la stessa dell'istogramma):
\[
f(x) = \frac{|N(x, h)|/N}{V(h)}
\]
dove $V(h)$ è il volume della finestra circolare (in 2D, l'area del cerchio di raggio $h$: $V(h) = \pi h^2$) e $N$ è il numero totale di punti. Dalla definizione dell'area del cerchio, possiamo vedere che:
\[
\int f(x) dx = \int \frac{|N(x, h)|/N}{V(h)} dx = \frac{1}{N V(h)} \int |N(x, h)| dx = \frac{1}{N V(h)} \cdot N \cdot V(h) = 1
\]

\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.8\textwidth]{images/kde_circle.png}
    \caption{Esempio di KDE con kernel circolare. Ogni punto dati contribuisce alla stima della densità all'interno della sua finestra circolare.}
    \label{fig:kde_circle}
\end{figure}

\paragraph{Kernel view.}
Un modo più generale di vedere KDE è quello di scrivere l'espressione $f(x)$ come
\[
f(x) = \frac{1}{|X|} \sum_{i=1}^{|X|} K_h (x_i - x)
\]
dove $x_i$ sono i punti dati, $|X|$ è il numero totale di punti e $K_h$ è definita come
\[
K_h (\mathbf{x}_i - \mathbf{x}) = \begin{cases}
\frac{1}{V(h)} & \text{se } ||x_i - x||_2 \leq h \\
0 & \text{altrimenti}
\end{cases}
\]
ovvero la funzione kernel che assegna un peso uniforme $\frac{1}{V(h)}$ ai punti all'interno della finestra circolare di raggio $h$ e 0 altrimenti. Questa dipende dal parametro $h$, che controlla la larghezza della finestra (la \emph{bandwith}). 

Uno dei problemi con questo tipo di kernel è che è molto \textbf{sensibile} alla posizione dei punti dati: piccoli cambiamenti nella posizione dei punti possono portare a grandi cambiamenti nella stima della densità. Possiamo notare che, poiché il kernel fa un salto netto da $\frac{1}{V(h)}$ a 0 al bordo della finestra, la stima della densità sarà discontinua in quei punti.

\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.8\textwidth]{images/kde_kernel.png}
    \caption{Kernel View nella stima di densità tramite Kernel Density Estimation (KDE). 
    Il grafico mostra il valore del kernel $K_h(\lVert x - x_i \rVert)$ in funzione della distanza normalizzata 
    $\lVert (x - x_i)/h \rVert_2$. I punti verdi contribuiscono alla stima con peso non nullo, mentre i punti rossi,
    posti oltre il supporto compatto del kernel, hanno peso nullo.}
    \label{fig:kde_kernel}
\end{figure}

\paragraph{Problemi del kernel circolare.}
Il kernel circolare ha un problema principale: fa decisioni \textbf{nette}, il punto si trova o non si trova all'interno della finestra. Questo porta a stime della densità che sono \textbf{discontinue} e \textbf{sensibili} alla posizione dei punti dati. 

Un modo di risolvere questo è utilizzare dei kernel più \emph{smooth} (morbidi), che assegnano pesi decrescenti ai punti man mano che si allontanano dal centro del kernel, invece di un peso uniforme all'interno di una finestra rigida. Un esempio è il \textbf{Kernel Gaussiano}:
\[
K_h(\mathbf{x}_i - \mathbf{x}) \propto \exp\left(-\frac{\|\mathbf{x}_i - \mathbf{x}\|^2}{2h^2}\right)
\]
Questo kernel assegna pesi più alti ai punti vicini al centro e pesi più bassi ai punti lontani, producendo una stima della densità più liscia e meno sensibile alla posizione dei punti dati.

\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.8\textwidth]{images/kde_gaussian.png}
    \caption{Kernel View per la Kernel Density Estimation (KDE) con kernel gaussiano. 
    Il valore del kernel $K_h(\lVert x - x_i \rVert)$ decresce in modo continuo all’aumentare della distanza 
    normalizzata $\lVert (x - x_i)/h \rVert_2$. A differenza dei kernel a supporto compatto, tutti i punti
    contribuiscono alla stima con peso non nullo, evidenziato dalla scala cromatica.}
    \label{fig:kde_gaussian}
\end{figure}

\subsection{Epanechnikov Kernel}
Il kernel guassiano risolve il problema della discontinuità, ma ha un supporto infinito, il che significa che ogni punto dati contribuisce alla stima della densità in ogni punto dello spazio, anche se con un peso molto piccolo. Questo può essere computazionalmente inefficiente.

Un'alternativa è l'\textbf{Epanechnikov Kernel}, che pone a 0 il peso dei punti oltre una certa distanza, mantenendo però una transizione più morbida rispetto al kernel circolare:
\[
K_h (\mathbf{x}_i - \mathbf{x}) = \frac{3}{4h^2} \left(  1- \frac{||\mathbf{x}_i - \mathbf{x}||^2}{h^2} \right) \mathbb{I}\left( ||\mathbf{x}_i - \mathbf{x}||_2 \leq h \right)
\]
dove $\mathbb{I}$ è la funzione indicatrice che vale 1 se la condizione è vera e 0 altrimenti. Questo kernel assegna pesi decrescenti ai punti man mano che si allontanano dal centro, ma pone a 0 il peso dei punti oltre la distanza $h$.

\begin{figure}[htbp]
    \centering
    \includegraphics[width=1\textwidth]{images/kde_nonparametric_comparison.png}
    \caption{Confronto in Kernel View tra diversi kernel utilizzati nella Kernel Density Estimation (KDE).
    Da sinistra a destra: kernel uniforme (radiale), kernel di Epanechnikov e kernel gaussiano.
    Il grafico mostra il peso $K_h(\lVert x - x_i \rVert)$ in funzione della distanza normalizzata
    $\lVert (x - x_i)/h \rVert_2$. I kernel uniforme ed Epanechnikov presentano supporto compatto,
    assegnando peso nullo ai punti oltre il raggio unitario, mentre il kernel gaussiano assegna
    peso non nullo a tutti i punti con contributo decrescente all’aumentare della distanza.}
    \label{fig:kde_nonparametric_comparison}
\end{figure}

\begin{figure}[htbp]
    \centering
    \includegraphics[width=1\textwidth]{images/kde_nonparametric_comparison2.png}
    \caption{Visualizzazione geometrica della Kernel Density Estimation (KDE) in $\mathbb{R}^2$ per diversi kernel con banda $h=1$.
    Da sinistra a destra: kernel circolare (uniforme), kernel di Epanechnikov e kernel gaussiano.
    Il punto di valutazione $x$ è indicato con una croce blu, mentre il cerchio rappresenta la regione
    $\lVert x - x_i \rVert_2 \le h$. I punti all’interno del supporto compatto contribuiscono alla stima
    con peso non nullo per i kernel circolare ed Epanechnikov, mentre il kernel gaussiano assegna un peso
    decrescente a tutti i punti, inclusi quelli esterni al raggio $h$.}
    \label{fig:kde_nonparametric_comparison2}
\end{figure}

\subsection{Tradeoff bias-varianza: sccelta della bandwith}
La scelta del kernel non è l'unico aspetto importante nella KDE, l'unico iperparametro della KDE è la \textbf{bandwith} $h$, che controlla la larghezza del kernel. La scelta di $h$ ha un impatto significativo sulla stima della densità:
\begin{itemize}
    \item Un valore di $h$ troppo piccolo porta a una stima della densità rumorosa, con molte fluttuazioni (alta varianza).
    \item Un valore di $h$ troppo grande porta a una stima della densità troppo liscia, che può perdere dettagli importanti (alto bias).
\end{itemize}

\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.9\textwidth]{images/kde_bandwidth_comparison.png}
    \caption{Effetto del parametro di banda $h$ nella Kernel Density Estimation (KDE) e relativo trade-off bias–varianza.
    Per valori piccoli di $h$ (in alto a sinistra) la stima presenta bassa distorsione ma alta varianza,
    con strutture molto frammentate e sensibili al rumore.
    All’aumentare di $h$ la varianza diminuisce e la stima diventa più liscia,
    ma un eccessivo smoothing (in basso a destra) introduce bias,
    fondendo strutture distinte e perdendo dettagli locali.
    La scelta del bandwidth governa quindi l’equilibrio tra fedeltà ai dati e regolarità della stima.}
    \label{fig:kde_bandwidth_comparison}
\end{figure}

\section{Metodi parametrici}
Questi metodi provano a stimare i parametri della \textbf{distribuzione di partenza} che si assume abbia generato i dati. Un vantaggio di questi metodi è che, una vxolta stimati i parametri, abbiamo una forma chiusa della funzione di densità. Uno svantaggio è che spesso richiedono molti iperparametri da regolare e che le assunzioni fatte sulla distribuzione di partenza potrebbero non essere corrette. Ci sono diversi vantaggi per fare questo tipo di stima rispetto a quella non parametrica:
\begin{itemize}
    \item \textbf{Forma analitica}: Una volta stimati i parametri, abbiamo una forma chiusa della funzione di densità, che può essere utile per ulteriori analisi.
    \item \textbf{Modelli compatti}: I modelli parametrici spesso richiedono meno memoria per memorizzare i parametri rispetto a memorizzare tutti i dati.
    \item \textbf{Modelli interpretabili}: I parametri stimati possono avere un significato interpretabile, che può essere utile per comprendere la struttura dei dati.
    \item \textbf{Limiti di dati}: I modelli parametrici possono essere più robusti quando si dispone di pochi dati, poiché fanno assunzioni sulla distribuzione di partenza.
\end{itemize}

\noindent
Esistono, tuttavia, anche degli svantaggi:
\begin{itemize}
    \item \textbf{Processo di ottimizzazione}: La stima dei parametri spesso richiede l'ottimizzazione di una funzione obiettivo, che può essere computazionalmente costosa e sensibile ai valori iniziali.
    \item \textbf{Assunzioni sulla distribuzione}: I modelli parametrici fanno assunzioni sulla distribuzione di partenza, che potrebbero non essere corrette per i dati reali.
    \item \textbf{Iperparametri}: I modelli parametrici spesso richiedono la scelta di diversi iperparametri, che possono influenzare significativamente la stima della densità.
\end{itemize}

\subsection{Distribuzione Gaussiana Multivariata}
Un metodo parametrico molto comune è assumere che i dati seguano una \textbf{doistribuzione guassiana multivariata}. Ricordando che la PDF per una guassiana multivariata a $d$ dimensioni è:
\[
N\left( \mathbf{x;\mu,}\mathbf{\Sigma} \right) = \frac{1}{\sqrt{(2\pi)^{d}\det(\Sigma)}} e^{ - \frac{1}{2}\left( \mathbf{x} - \mathbf{\mu} \right)^{T}\Sigma^{- 1}\left( \mathbf{x} - \mathbf{\mu} \right)}
\]
quindi questa distribuzione è definita unicamente da due valori: la media $\mu$ e la matrice di covarianza $\Sigma$, i valori da trovare.

\paragraph{MLE: Maximum Likelihood Estimation.}
Per stimare i parametri della distribuzione, possiamo usare il metodo della \textbf{massima verosimiglianza} (MLE, Maximum Likelihood Estimation). L'idea è di trovare i parametri che massimizzano la probabilità di osservare i dati dati i parametri stessi. Sia $\mathbf{X = \{x_1, x_2, \ldots, x_N\}}$ il nostro insieme di dati, la funzione di verosimiglianza è definita come:
\[
P(\mathbf{X} | \mu, \Sigma) = \prod_{i=1}^{N} N\left( \mathbf{x_i;\mu,}\mathbf{\Sigma} \right)
\]
Per semplificare i calcoli, spesso lavoriamo con il logaritmo della funzione di verosimiglianza, chiamato \textbf{log-likelihood} (log-verosimiglianza):
\[
\log P(\mathbf{X} | \mu, \Sigma) = \sum_{i=1}^{N} \log N\left( \mathbf{x_i;\mu,}\mathbf{\Sigma} \right)
\]

Per quanto questa soluzione sembri computazionalmente costosa, per una distribuzione gaussiana esistono delle forme chiuse. Si prende la derivata della log-verosimiglianza rispetto a $\mu$ e $\Sigma$, la si pone uguale a 0 e si risolve per i parametri. 

\begin{figure}[htbp]
    \centering
    \includegraphics[width=\textwidth]{images/mle_gaussian.png}
    \caption{Esempio di stima parametrica tramite \textbf{Maximum Likelihood Estimation} per una distribuzione gaussiana unidimensionale. 
    In ciascun pannello è mostrata una distribuzione normale $N(\mu,\sigma^2)$ con diversi valori di media $\mu$ e deviazione standard $\sigma$. 
    I punti blu sull’asse orizzontale rappresentano i dati osservati, mentre le linee verticali tratteggiate indicano il contributo di ciascun campione alla log-verosimiglianza. 
    La curva arancione è la densità di probabilità associata ai parametri considerati e il valore della log-verosimiglianza totale $\log P(\mathbf{X} \mid \mu, \sigma)$ è riportato in legenda. 
    Il confronto tra i diversi pannelli evidenzia come la log-verosimiglianza vari al cambiare dei parametri e come essa venga massimizzata quando la distribuzione stimata è coerente con i dati osservati.}
    \label{fig:mle-gaussian}
\end{figure}

\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.8\textwidth]{images/mle_gaussian_2d.png}
    Confronto della funzione di verosimiglianza per una distribuzione gaussiana unidimensionale in funzione dei parametri. A sinistra è mostrata la log-verosimiglianza $\log P(\mathbf{X}\mid \mu,\sigma=1)$ al variare della media $\mu$, che evidenzia un massimo in corrispondenza della media campionaria. Al centro è riportata la log-verosimiglianza $\log P(\mathbf{X}\mid \mu=0,\sigma)$ in funzione della deviazione standard $\sigma$, che mostra un massimo per un valore ottimale di dispersione dei dati. A destra è visualizzata la superficie di verosimiglianza $P(\mathbf{X}\mid \mu,\sigma)$ nel piano $(\mu,\sigma)$, che presenta un unico massimo globale corrispondente alla stima di massima verosimiglianza dei parametri.
    \label{fig:mle-gaussian-2d}
\end{figure}

\paragraph{Media e covarianza campionaria.}
Un modo per risolvere il problema di stima dei parametri è massimizzare direttamente la log-likelihood rispetto a $\mu$ e $\Sigma$. 
In generale, questo comporta la risoluzione di un problema di ottimizzazione potenzialmente complesso; tuttavia, nel caso della distribuzione gaussiana, la particolare forma analitica della log-likelihood rende il problema trattabile in forma chiusa.

Sviluppando la log-likelihood della gaussiana multivariata e derivandola rispetto ai parametri, si ottiene che il massimo è raggiunto quando la media della distribuzione coincide con la media empirica dei dati e la covarianza coincide con la covarianza campionaria. In particolare:
\[
\hat{\mu} = \frac{1}{N} \sum_{i=1}^{N} x_i
\]
\[
\hat{\Sigma} = \frac{1}{N} \sum_{i=1}^{N} (x_i - \hat{\mu})(x_i - \hat{\mu})^T
\]

\noindent
Questi stimatori sono noti come \textbf{stimatori di massima verosimiglianza} per la media e la covarianza di una distribuzione gaussiana.

Dal punto di vista intuitivo, la MLE sceglie i parametri che rendono i dati osservati il più probabili possibile sotto il modello assunto. 
Nel caso gaussiano, massimizzare la verosimiglianza equivale a:
\begin{itemize}
    \item centrare la distribuzione nel punto che minimizza la distanza quadratica media dai dati (la media campionaria);
    \item scegliere una dispersione che rifletta esattamente la variabilità osservata nei dati (la covarianza campionaria).
\end{itemize}

Questo risultato evidenzia un aspetto chiave dei metodi parametrici: una volta fissata la forma della distribuzione, la stima dei parametri diventa un problema ben definito e, per alcuni modelli come la gaussiana, ammette soluzioni analitiche semplici e interpretabili.

\paragraph{Limiti dell’approccio MLE.}
È importante sottolineare che questi risultati valgono \emph{solo} sotto l’assunzione che i dati siano effettivamente generati da una distribuzione gaussiana. 
Se tale ipotesi è violata, la MLE restituisce comunque una media e una covarianza, ma la densità stimata può rappresentare male la struttura reale dei dati, specialmente in presenza di multimodalità, outlier o distribuzioni fortemente asimmetriche.

\subsection{Gaussian Mixture Model}
La distribuzone gaussiana ma fa un'assunzione forte: tutti i dati derivano da \textbf{un'unico} cluster (è unimodale). Questa assunzione fallisce per molti dataset reali. Una soluzione potrebbe essere utilizzare una \textbf{GMM: Guassian Mixture Model}: invece di utilizzare una gaussiana, modelliamo i dati come una combinazione di $K$ gaussiane, ognuna con la propria media e covarianza. La PDF di un GMM è data da:
\[
P(x) = \sum_{k=1}^{K} \pi_k N(x; \mu_k, \Sigma_k)
\]
dove $\pi_k$ sono i pesi delle singole gaussiane (che sommano a 1), e $N(x; \mu_k, \Sigma_k)$ è la PDF della gaussiana con media $\mu_k$ e covarianza $\Sigma_k$.

\begin{figure}[htbp]
    \centering
    \includegraphics[width=1\textwidth]{images/gmm_example.png}
    \caption{Esempio di Gaussian Mixture Model (GMM) unidimensionale con 3 componenti gaussiane. 
    I punti blu sull'asse orizzontale rappresentano i dati osservati. Le curve colorate (blu, arancione e verde) 
    indicano le tre distribuzioni gaussiane componenti con i rispettivi pesi $\pi_k$. 
    La curva tratteggiata rossa mostra la densità complessiva risultante dalla somma ponderata delle singole gaussiane. 
    Questo esempio illustra come un GMM possa modellare efficacemente dati multimodali, catturando strutture 
    complesse che una singola gaussiana non potrebbe rappresentare.}
    \label{fig:gmm_example}
\end{figure}

\paragraph{Soft clustering con GMM.}
Un vantaggio importante dei GMM è che permettono un \textbf{soft clustering}: ogni punto dati ha una probabilità di appartenere a ciascuna delle $K$ componenti gaussiane, invece di essere assegnato rigidamente a un singolo cluster. Per fare questo, introduce una \textbf{variabile latente} $Z$: una variabile latente è una variabile che non è osservata direttamente nei dati, ma che influenza il processo di generazione dei dati. In questo caso, $Z$ indica quale componente gaussiana ha generato ciascun punto dati.

In GMM, per ogni variabile $x_i$ esiste una variabile latente $z_i$ che indica quale delle $K$ componenti gaussiane ha generato quel punto. Si può utilizzare la probabilità a posteriori, con il teorema di Bayes, per calcolare la probabilità che un punto $x_i$ appartenga alla componente $k$:
\[
P(Z = k | x) = \frac{P(x | Z = k) P(Z = k)}{P(x)}
\]
dove $P(x | Z = k)$ è la PDF della componente gaussiana $k$, $P(Z = k) = \pi_k$ è il peso della componente, e $P(x)$ è la PDF complessiva del GMM. Da questo ricaviamo la \textbf{responsabilità} $\gamma_k$ che un punto $x_i$ appartiene alla componente $k$:
\[
\gamma_k = P(Z=k|\mathbf{x}) = \frac{\pi_k \mathcal{N}(\mathbf{x}|\mathbf{\mu}_k, \mathbf{\Sigma}_k)}{\sum_{j=1}^K \pi_j \mathcal{N}(\mathbf{x}|\mathbf{\mu}_j, \mathbf{\Sigma}_j)}
\]

\paragraph{Stima dei parametri con MLE.}
Per ottimizzare i parametri del GMM ($\pi_k$, $\mu_k$, $\Sigma_k$), possiamo usare ancora una volta la MLE. Tuttavia, a causa della presenza delle variabili latenti, la log-likelihood diventa più complessa:
\[
\log P(\mathbf{X} | \{\pi_k, \mu_k, \Sigma_k\}_{k=1}^K) = \sum_{i=1}^{N} \log \left( \sum_{k=1}^{K} \pi_k N(x_i; \mu_k, \Sigma_k) \right)
\]
Per massimizzare questa funzione, si usa spesso l'algoritmo di \textbf{Expectation-Maximization} (EM).

\paragraph{GMM vs K-means.}
Un confronto interessante è tra GMM e K-means. Entrambi gli algoritmi cercano di raggruppare i dati in $K$ cluster, ma lo fanno in modi diversi:
\begin{itemize}
    \item K-means assegna ogni punto dati al cluster più vicino, basandosi sulla distanza euclidea, producendo un \textbf{hard clustering}.
    \item GMM assegna a ogni punto dati una probabilità di appartenenza a ciascun cluster, basandosi sulla stima della densità, producendo un \textbf{soft clustering}.
\end{itemize}

\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.9\textwidth]{images/gmm_vs_kmeans.png}
    \caption{Confronto tra clustering con \textbf{K-means} e \textbf{Modello a miscela di gaussiane (GMM)} su dati bidimensionali. 
    Confronto tra clustering con \textbf{K-means} e \textbf{Modello a miscela di gaussiane (GMM)} su dati bidimensionali. 
    A sinistra, K-means assegna i punti ai cluster in base alla distanza euclidea dai centroidi, producendo regioni di decisione sferiche e di uguale dimensione (cerchi tratteggiati). 
    A destra, il GMM modella ciascun cluster come una distribuzione gaussiana con media e covarianza proprie, permettendo regioni ellittiche orientate (contorni rossi) e una rappresentazione più flessibile della struttura dei dati.}
    \label{fig:gmm_vs_kmeans}
\end{figure}
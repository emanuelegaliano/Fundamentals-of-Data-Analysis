\chapter{Analisi predittiva}
L'analisi predittiva è una branca dell'analisi dei dati che utilizza tecniche statistiche, di machine learning e di data mining per prevedere eventi futuri basandosi su dati storici.

I principali obiettivi sono:
\begin{itemize}
    \item Fare inferenza sulle relazioni tra variabili.
    \item Costruire modelli che possono essere usati per fare predizioni su nuovi dati.
\end{itemize}

\section{Modello}
Prima di continuare, è necessario dare la definizione di "modello".

\begin{nicequote}
    Un modello è una rappresentazione semplificata di un sistema complesso, che cattura le caratteristiche essenziali del sistema per permettere l'analisi e la previsione del suo comportamento.
\end{nicequote}

Si può vedere una analogia di un modello come una cartina geografica: non rappresenta ogni dettaglio del territorio, ma fornisce informazioni sufficienti per orientarsi e pianificare un percorso. Ma questo porta a una conclusione, quella cartina è tecnicamente \textbf{sbagliata}, poiché non rappresenta a pieno il territorio.

\noindent
Lo statistico George Box ha detto:
\begin{quote}
    "All models are wrong, but some are useful."
\end{quote}

Ovvero, tutti i modelli sono sbagliati in quanto semplificazioni della realtà, ma alcuni possono essere utili per fare previsioni accurate e prendere decisioni informate. Per esempio, utilizzando un modello che predice il BMI (Body Mass Index) di una popolazione possiamo fare delle previsioni solo su una parte della popolazione, ma non su ogni singolo individuo (come si vede in figura \ref{fig:bmi_model}).

\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.9\textwidth]{images/bmi_model.png}
    \caption{Distribuzione del BMI in una popolazione. Istogramma reale composto da $n=768$ individui (barre grigie) a confronto con un modello gaussiano (linea nera). La Gaussiana riassume con media e deviazione una parte della popolazione in modo corretto, ma assegna probabilità anche a BMI negativi.}
    \label{fig:bmi_model}
\end{figure}

\subsection{Modelli predittivi}
Quando facciamo analisi predittiva possiamo identificare:
\begin{itemize}
    \item Una variabile $Y$ detta \textbf{Variabile dipendente} o \emph{variabile di risposta}, che rappresenta l'output che vogliamo prevedere.
    \item Un vettore di variabili $X = (X_1, X_2, \ldots, X_p)$ dette \textbf{Variabili indipendenti} o \emph{predittori}, che rappresentano gli input utilizzati per fare la previsione.
\end{itemize}

Possiamo esprimere il modello predittivo come una funzione matematica, con una forma generale:
\[
Y = f(X) + \epsilon
\]
dove $f$ è il modello che descrive la relazione tra le variabili indipendenti e la variabile dipendente, ed $\epsilon$ è un termine di errore che rappresenta la variabilità non spiegata dal modello.

\section{Predizione vs Spiegazione}
Un aspetto importante dell'analisi predittiva è la distinzione tra \textbf{predizione} e \textbf{spiegazione}.

\subsection{Predizione}
\begin{nicequote}
    La predizione si concentra sulla capacità del modello di fare previsioni accurate su nuovi dati, indipendentemente dal fatto che il modello sia interpretabile o meno.
\end{nicequote}

Quindi risponde a una semplice domanda: "Cosa avverrà?". Nel caso particolare della predizione, l'accuratezza del modello è la metrica più importante.

Per fare un esempio, si pensi a un modello per prevedere il prezzo delle case basandosi su caratteristiche come la posizione, la dimensione e il numero di stanze. Anche se la rete neurale può essere complessa e difficile da interpretare, se riesce a fare previsioni accurate sui prezzi delle case, allora è considerata un buon modello predittivo.

\subsection{Spiegazione}
\begin{nicequote}
    La spiegazione si concentra sulla comprensione delle relazioni tra le variabili e sull'interpretabilità del modello.
\end{nicequote}

In questo caso il modello deve essere al 100\% interpretabile, rispondendo alla domanda: "Perché avverrà?". Qui l'accuratezza del modello è meno importante rispetto alla capacità di spiegare i fenomeni osservati.

Per fare un esempio, si consideri un modello per analizzare come una malattia e da quali fattori essa dipende (età, stile di vita, genetica, ecc.). In questo caso, un modello semplice è preferibile, anche se meno accurato, perché permette ai medici di comprendere i fattori di rischio e di prendere decisioni informate sui trattamenti.

\subsection{Compromesso tra Predizione e Spiegazione}
Spesso esiste un compromesso tra predizione e spiegazione, in quanto i due obiettivi non sono spesso mutuamente esclusivi. Un modello potente infatti, se il problema lo permette, deve sia fare inferenza che predizione.

Si pensi a un dataset di rischio del diabete di tipo 1: un modello potrebbe essere utilizzato per prevedere la probabilità che un individuo sviluppi la malattia (predizione), ma potrebbe anche essere utile per identificare i fattori di rischio associati alla malattia (spiegazione).

\section{Statistica vs Machine Learning}
Un altro aspetto importante dell'analisi predittiva è la distinzione tra \textbf{statistica} e \textbf{machine learning}.

\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.9\textwidth]{images/statistics_vs_ml.jpeg}
    \caption{Confronto tra \emph{glass box} statistica e \emph{black box} di ML: la statistica privilegia interpretabilità e spiegazione del \textit{perché}, mentre il machine learning privilegia accuratezza predittiva sul \textit{che cosa} a partire dai dati, spesso con modelli opachi.}
    \label{fig:stats_vs_ml}
\end{figure}

\subsection{Approccio statistico}
L'approccio statistico è concentrato sul capire il modello e sull'inferenza. Si comporta come in figura \ref{fig:stats_vs_ml}, come una "glass box", dove il funzionamento interno del modello è trasparente e interpretabile. Le sue metodologie includono:
\begin{itemize}
    \item Affidarsi alle assunzioni fatte sui dati (distribuzioni, linearità, indipendenza, ecc.).
    \item Utilizzare molto il testo di ipotesi, con p-values e intervalli di confidenza.
    \item Prediligere modelli semplici e interpretabili.
\end{itemize}

\subsection{Approccio di Machine Learning}
L'approccio di Machine Learning ha un focus primario sulla predizione accurata. Si comporta come in figura \ref{fig:stats_vs_ml}, come una "black box", dove il funzionamento interno del modello può essere complesso e difficile da interpretare. Le sue metodologie includono:
\begin{itemize}
    \item Le performance del modello si vedono sui dati, senza fare molte assunzioni a priori.
    \item Si affida allo split dei dati in training, validation e test set per simulare la generalizzazione.
    \item Non vengono misurate le performance con p-values, ma con metriche di accuratezza predittiva sul test-set.
    \item L'intepretabilità è una cosa in più, non un requisito.
\end{itemize}

\subsection{Trade-Off di Complessità-Interpretabilità}
Quando si lavora su modelli predittivi, statistici o di machine learning, spesso si deve affrontare un trade-off tra complessità e interpetabilità del modello: 
\begin{description}
    \item[Modelli semplici]: un modello semplice (come la regressione lineare) è facile da interpretare e spiegare, ma potrebbe non catturare tutte le complessità dei dati, portando a una minore accuratezza predittiva. Come già detto prima, è esattamente la prerogativa della statistica.
    \item[Modelli complessi]: un modello complesso (come le reti neurali profonde) può catturare meglio le complessità dei dati e fornire previsioni più accurate, ma spesso è difficile da interpretare e spiegare. Questo è il punto di forza del machine learning.
\end{description}

\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.9\textwidth]{images/complexity_interpretability_tradeoff.png}
    \caption{Accuratezza vs complessità del modello: dalla Regressione Lineare a Decision Tree e Random Forest fino alle Reti Neurali, l’errore tende a diminuire man mano che cresce la complessità (ma aumenta il costo/opaquezza del modello).}
    \label{fig:complexity_interpretability_tradeoff}
\end{figure}

\section{Tipologie di problema}
Una volta stabilito se vogliamo capire i dati o fare predizione, dobbiamo identificare il tipo di problema che stiamo affrontando. I principali tipi di problemi nel machine learning sono:
\begin{itemize}
    \item \textbf{Supervised learning}: il modello viene addestrato su un dataset etichettato, dove ogni esempio di input ha una corrispondente etichetta di output. L'obiettivo è imparare una funzione che mappa gli input agli output corretti.
    \item \textbf{Unsupervised learning}: il modello viene addestrato su un dataset non etichettato, dove non ci sono etichette di output. L'obiettivo è trovare strutture nascoste nei dati, come cluster o pattern.
\end{itemize}

\subsection{Regressione}
La regressione è un tipo di problema di supervised learning in cui l'obiettivo è prevedere un valore $y$ continuo. Dobbiamo trovare quindi il miglior "fit" di una funzione all'interno dei nostri dati, per descrivere a pieno la relazione tra le variabili indipendenti e la variabile dipendente.

\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.9\textwidth]{images/regression_example.png}
    \caption{Esempio di problema di regressione: prevedere il prezzo di una casa in base ai metri quadri.}
    \label{fig:regression_example}
\end{figure}

Per fare un esempio legato alla figura \ref{fig:regression_example}, si consideri un dataset che contiene informazioni sulle case, come la dimensione in metri quadri, il numero di stanze e il prezzo di vendita. L'obiettivo è prevedere il prezzo di una casa basandosi sulle sue caratteristiche. Un modello di regressione potrebbe essere utilizzato per trovare la relazione tra la dimensione della casa e il suo prezzo, permettendo di fare previsioni sui prezzi delle case in base alle loro dimensioni.

\subsection{Classificazione}
La classificazione è un altro tipo di problema di supervised learning in cui l'obiettivo è prevedere una categoria o classe discreta $y$(etichetta). In questo caso, durante il training, il modello impara a mappare gli input alle classi corrette.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{images/classification_example.png}
    \caption{Esempio di problema di classificazione: capire se una malattia è presente oppure no in base all'età e a un'analisi del sangue.}
    \label{fig:classification_example}
\end{figure}

Continuando l'esempio in figura \ref{fig:classification_example}, si consideri un dataset che contiene informazioni sui pazienti, come l'età, il sesso e i risultati di un'analisi del sangue. L'obiettivo è prevedere se un paziente ha una certa malattia (ad esempio, diabete) basandosi sulle sue caratteristiche. Nel caso particolare dell'immagine, notiamo che può essere separata da una retta per risolvere il problema.

\subsection{Clustering}
Il clustering è un problema di unsupervised learning in cui l'obiettivo è raggruppare i dati in cluster basandosi sulla somiglianza tra gli esempi. In questo caso, il modello cerca di identificare strutture nascoste nei dati senza l'uso di etichette di output.

\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.9\textwidth]{images/clustering_example.png}
    \caption{Esempio di problema di clustering: raggruppare i clienti in segmenti basandosi sui loro comportamenti di acquisto.}
    \label{fig:clustering_example}
\end{figure}

Il modello, unsupervised, viene addestrato ad identificare gruppi con caratteristiche simili. Nella figura \ref{fig:clustering_example} si considera un esempio dove si vogliono raggruppare quei gruppi di persone in base all'età e a una feature chiamata "Spending Score", che indica quanto una persona spende in un negozio. 

\section{Modelli parametrici vs Modelli non parametrici}
I modelli possono differirsi anche in base a come rappresentano la funzione $f(X)$ che mappa gli input agli output. Possiamo distinguere tra modelli parametrici, che assumono una forma funzionale specifica, e modelli non parametrici, che non fanno assunzioni rigide sulla forma della funzione.

\subsection{Modelli parametrici}
Nello specifico, un modello parametrico è caratterizzato da un numero fisso di parametri che definiscono la funzione $f(X)$. Questi modelli sono spesso più semplici e veloci da addestrare, ma possono essere limitati nella loro capacità di catturare la complessità dei dati. Qui il compito principale è quello di modificare i valori dei parametri per adattare il modello ai dati di training.

Un esempio comune di modello parametrico è la regressione lineare, dove la funzione $f(X)$ è una combinazione lineare delle variabili indipendenti:
\[
f(X) = \beta_0 + \beta_1 X_1 + \beta_2 X_2 + \ldots + \beta_p X_p
\]
dove $\beta_0, \beta_1, \ldots, \beta_p$ sono i parametri del modello.

Ha come pro il fatto che è semplice, veloce e richiede meno dati. Tuttavia questi modelli sono "biased": le assunzioni iniziali possono portare a errori sistematici se i dati non seguono quelle assunzioni, inoltre sono più propensi all'\emph{underfitting}\footnote{L'underfitting è un fenomeno causato da un modello troppo "semplice", che non riesce a catturare la complessità dei dati.}.

\subsection{Modelli non parametrici}
Un modello non parametrico, invece, non assume una forma funzionale specifica per la funzione $f(X)$. Questi modelli sono più flessibili e possono adattarsi meglio alla complessità dei dati, ma possono essere più lenti da addestrare e richiedere più dati. Qui il compito principale è quello di memorizzare i dati di training e utilizzare questi dati per fare predizioni sui nuovi input.

Un esempio comune di modello non parametrico è il k-Nearest Neighbors (k-NN), dove la predizione per un nuovo input viene fatta basandosi sui k esempi più vicini nel dataset di training.

Ha come pro il fatto che è flessibile e può catturare relazioni complesse nei dati. Tuttavia questi modelli sono "low-bias": non fanno assunzioni rigide sui dati, ma possono essere più propensi all'\emph{overfitting}\footnote{L'overfitting è un fenomeno causato da un modello troppo "complesso", che si adatta troppo bene ai dati di training, ma non generalizza bene su nuovi dati.}.

\section{Learning}
Alla base dei problemi di Machine Learning c'è il concetto di \textbf{learning}, ovvero l'apprendimento di una funzione $f(X)$ dai dati. 

\subsection{Definizione formale}
\begin{nicequote}
    Siano $\mathcal{X}$ e $\mathcal{Y}$ gli spazi, rispettivamente, di input e di output al modello. Possono essere visti come spazi di variabili aleatorie, in particolare una variabile $X$ e una $Y$ con osservazioni $x \in \mathcal{X}, y \in \mathcal{Y}$. L'obiettivo è trovare una funzione che risponde alla nostra ipotesi $h \in \mathcal{H}$, dove $\mathcal{H}$ è lo spazio delle ipotesi (funzioni candidate), ovvero una funzione:
    \[
    h: \mathcal{X} \to \mathcal{Y}
    \]
    che approssima la relazione tra $X$ e $Y$. Possiamo scrivere una funzione che lo fa non in modo preciso, ma approssimando a una predizione $\hat{y} \approx y \in \mathcal{Y}$:
    \[
    \hat{y} = h(x)
    \]
\end{nicequote}

\paragraph{Esempio 1: non parametrico.} Per fare un esempio, immaginiamo $\mathcal{X} = \mathbb{R}^m$ e $\mathcal{Y} = \{0, 1\}$, ovvero uno spazio di input a $m$ variabili che rappresentano lo spazio dei risultati di un'analisi del sangue e uno spazio di output binario che rappresenta la presenza o meno di una malattia. L'obiettivo è trovare una funzione $h$ che mappa i risultati dell'analisi del sangue alla presenza o meno della malattia.

\paragraph{Esempio 2: parametrico.} In un altro esempio, immaginiamo di dover classificare le email in spam e non: in questo caso possiamo definire $\mathcal{X}$ come lo spazio delle email rappresentate da vettori di caratteristiche (come la frequenza di certe parole chiave) e $\mathcal{Y} = \{0, 1\}$ come lo spazio delle etichette (spam o non spam). L'obiettivo è trovare una funzione $h$ che mappa le caratteristiche delle email alle etichette corrette:
\[
h(x) =
\begin{cases}
1 & f(x) > \theta \\
0 & f(x) \leq \theta
\end{cases}
\]
Dove $\theta$ è esattamente il nostro parametro, in questo caso una \emph{soglia}.


\subsection{Il processo di Learning}
Per trovare la funzione $h$ che meglio approssima la relazione tra $X$ e $Y$, dobbiamo avere un modo per valutare quanto bene una funzione $h$ si adatta ai dati. Questo viene fatto utilizzando una funzione di perdita (loss function) $L(y, \hat{y})$, che misura l'errore tra il valore reale $y$ e la predizione $\hat{y}$ fatta dal modello.

In realtà vorremmo calcolare l'errore atteso (o rischio) del modello su tutta la distribuzione dei dati, ma non avendo accesso a questa distribuzione, possiamo solo stimare l'errore empirico $R(h)$ definita sulla nostra ipotesi $h^*$\footnote{Indichiamo la nostra ipotesi con un asterisco $h^*$ perché rappresenta la migliore funzione di approssimazione possibile dati i dati a disposizione.} come \textbf{l'errore atteso} sotto la distribuzione di dati $P(X, Y)$:
\[
R(h) = \mathbb{E}_{(X,Y) \sim P} [L(Y, h(X))]
\]
In questo caso, si definisce \textbf{obiettivo del learning statistico} come la risoluzione del problema di ottimizzazione seguente:
\[
h^* = \arg\min_{h \in \mathcal{H}} R(h)
\]

\subsection{ERM: Empirical Risk Minimization}
Esiste un problema: non possiamo calcolare la funzione di rischio $R(h)$ perché non conosciamo la vera distribuzione $P(X, Y)$. Tuttavia, possiamo stimare il rischio empirico $R_{emp}(h)$ utilizzando un dataset di training\footnote{Il dataset di training è un insieme di dati utilizzati per addestrare il modello.} di $N$ esempi che sono un campione rappresentativo della popolazione. Si definisce $\text{TR}$ training set, come l'insieme delle coppie:
\[
\text{TR} = \{(x_i, y_i)\}^N_{i=1}
\]
Grazie a questo e alla \textbf{legge dei grandi numeri}\footnote{La legge dei grandi numeri afferma che, al crescere del numero di osservazioni, la media campionaria converge alla media della popolazione.}, possiamo stimare il rischio empirico come:
\[
R_{\text{emp}}(h) = \frac{1}{N} \sum_{i=1}^{N} L(y_i, h(x_i))
\]
Quindi, possiamo risolvere il problema di ottimizzazione empirica:
\[
h_{\text{emp}} = \arg\min_{h \in \mathcal{H}} R_{\text{emp}}(h)
\]
Questo approccio è noto come \textbf{Empirical Risk Minimization} (ERM), ovvero la minimizzazione del rischio empirico.

\section{Capacità del modello}
Un aspetto cruciale nell'analisi predittiva è la capacità del modello, ovvero la sua capacità di adattarsi ai dati di training e di generalizzare a nuovi dati. Possiamo definire formalmente la capacità come una relazione allo spazio e alla "ricchezza" (intesa come complessità) dello spazio delle ipotesi $\mathcal{H}$ dal quale il modello può scegliere la funzione $h$. Esistono modelli:
\begin{description}
    \item[A bassa capacità]: modelli con uno spazio delle ipotesi limitato, che possono adattarsi solo a funzioni semplici. Questi modelli sono meno propensi all'overfitting, ma possono soffrire di underfitting. Un esempio è una classificazione lineare con una retta: sicuramente andrà bene per dati linearmente separabili, ma nei dati più complessi non riuscirà a catturare le relazioni tra le variabili.
    \item[Ad alta capacità]: modelli con uno spazio delle ipotesi ampio, che possono adattarsi a funzioni complesse. Questi modelli sono più propensi all'overfitting, ma possono catturare meglio le relazioni nei dati. Un esempio è una classificazione non lineare con un polinomio di grado 10: sicuramente riuscirà a catturare le relazioni nei dati complessi, ma rischia di adattarsi troppo ai dati di training e di non generalizzare bene su nuovi dati. 
\end{description}

\subsection{Misurare la capacità del modello}
Per misurare la capacità de modello si deve misurare quanto bene il modello generalizza sui nuovi dati. Questa metrica è solitamente una funzione che valuta l'accuratezza del modello su un dataset di test separato dal training set. Un modo comune, nel caso della regressione, è utilizzare l'errore quadratico medio (Mean Squared Error, MSE):
\[
R_{\text{emp}}(h) = \frac{1}{M} \sum_{j=1}^{M} (y_j - h(x_j))^2
\]

Generalmente però, questa metrica, non basta. Perché si potrebbe pensare di utilizzare come mappatura di una regressione il valore del training set:
\[
\hat{h}(x) = y \qquad (x, y) \in \text{TR}
\]
ottenendo un errore di 0 sul training set, ma un errore altissimo sul test set. Per questo motivo si usano tecniche di validazione incrociata (cross-validation) per stimare la capacità del modello in modo più robusto.

Questo porta alla definizione di due concetti già visti in breve, \textbf{underfitting} e \textbf{overfitting}:
\begin{description}
    \item[Underfitting] Si verifica quando un modello è troppo semplice per catturare le relazioni nei dati, portando a prestazioni scadenti sia sul training set che sul test set. Per esempio, dei dati che seguono una distribuzione quadratica vengono approssimati con una retta (\ref{fig:ovfit_underfit_rightfit}, sinistra).
    \item[Overfitting] Si verifica quando un modello è troppo complesso e si adatta troppo ai dati di training, catturando il rumore invece della vera relazione tra le variabili. Questo porta a buone prestazioni sul training set ma scarse sul test set. Per esempio, dei dati che seguono una distribuzione quadratica vengono approssimati con un polinomio di grado 5 (\ref{fig:ovfit_underfit_rightfit}, destra).
\end{description}

\subsection{Bias e Varianza}
Per valutare le prestazioni di un modello possiamo usare i valori del bias e della varianza per fare delle stime su come il modello si comporta sui dati.

\begin{figure}[htbp]
	\centering
	\includegraphics[width=\textwidth]{images/ovfit_underfit_rightfit.png}
	\caption{Underfitting---capacità adeguata---overfitting in regressione polinomiale: punti blu \(=\) dati di training (\(\mathrm{TR}\)), punti rossi \(=\) fuori da \(\mathrm{TR}\); la linea verde mostra il fit del modello: lineare (sinistra), polinomio di grado \(2\) (centro) e polinomio di grado \(5\) (destra).}
	\label{fig:ovfit_underfit_rightfit}
\end{figure}

\paragraph{Bias.} Il bias è \textbf{l'errore sistematico} che il modello commette sui dati. Un modello con alto bias tende a sottostimare la complessità del problema, portando a errori elevati sia sul training set che sul test set. Questo fenomeno è noto come \textbf{underfitting}. Nell'immagine \ref{fig:ovfit_underfit_rightfit}, il grafico a sinistra mostra un esempio di underfitting, dove il modello lineare non riesce a catturare la relazione tra le variabili e commette un errore sistematico sia sul training set (punti blu) che sul test set (punti rossi).

\paragraph{Varianza.} La varianza rappresenta, invece, la sensibilità del modello alle variazioni nei dati di training. Un modello con alta varianza tende a sovradattarsi ai dati di training, catturando il rumore invece della vera relazione tra le variabili. Questo porta a errori bassi sul training set ma elevati sul test set, fenomeno noto come \textbf{overfitting}. Nell'immagine \ref{fig:ovfit_underfit_rightfit}, il grafico a destra mostra un esempio di overfitting, dove il modello polinomiale è troppo complesso e si adatta troppo strettamente ai dati di training, risultando in prestazioni scadenti sui dati di test\footnote{Si noti che basterebbe rimuovere un punto del training set per far cambiare completamente il modello, in quanto altamente sensibile ai dati di input.}.

\begin{figure}[htbp]
	\centering
	\includegraphics[width=0.8\textwidth]{images/bias_variance_tradeoff_curves.png}
	\caption{Trade-off tra bias e varianza in funzione della complessità del modello.}
	\label{fig:bias_variance_tradeoff_curves}
\end{figure}

\subsection{Parametri vs Iperparametri}
Per controllare la capacità del modello, possiamo agire su due tipi di parametri:
\begin{description}
    \item[Parametri] I parametri sono i valori che il modello impara durante il processo di training. Questi parametri definiscono la funzione $h$ che mappa gli input agli output. Per esempio, in una regressione lineare, i parametri sono i coefficienti della retta.
    \item[Iperparametri] Gli iperparametri sono i valori che vengono impostati prima del processo di training e controllano il comportamento del modello. Questi iperparametri influenzano la capacità del modello e il modo in cui viene addestrato. Sono coefficienti che hanno fattori esterni, come il tasso di apprendimento, la profondità di un albero decisionale o il grado del polinomio nella funzione di regressione.
\end{description}

\section{Selezione del modello}
Il problema a questo punto diventa: come facciamo a far sì che l'algoritmo di learning (ERM) trovi i migliori parametri per il modello fornito un set di iperparametri.

\subsection{Approccio 1: selezione statistica}
Quando il nostro obiettivo principale è l'\emph{inferenza} (il modello glass-box), selezionamo un modello in base a quanto spiega bene i dati in favore della semplicità. Questo approccio utilizza l'intero dataset \textbf{in una volta}, perché non cerchiamo di "predire" il futuro, ma di trovare la migliore spiegazione per i dati che osserviamo.

Si usano misure statistiche che bilanciano bontà di fit e complessità del modello, come:
\begin{itemize}
    \item \textbf{p-values}: Facciamo un test sul valore di ogni variabile e potremmo rimuovere quelle con un p-value alto (ovvero quelle che creano rumore).
    \item $\mathbf{R^2}$: Questa misura indica la proporzione di varianza nella variabile dipendente che è spiegata dalle variabili indipendenti nel modello. Un valore di $R^2$ più alto indica un modello che spiega meglio i dati.
\end{itemize}

\subsection{Approccio 2: selezione predittiva}
Quando il nostro obiettivo principale è la \emph{predizione} (il modello black-box), selezioniamo un modello in base a quanto bene predice nuovi dati, bilanciando accuratezza e complessità. Questo approccio utilizza tecniche di validazione incrociata per stimare la capacità del modello di generalizzare a nuovi dati.

Esiste infatti una "golden rule" (reegola d'oro) nei modelli predittivi:
\begin{quote}
    "The performance of a model on the data it was trained on is \emph{irrelevant}. The only measure that matters is its performance on new, unseen data."
\end{quote}
Ovvero, è inutile valutare un modello in base a quanto bene si adatta ai dati di training: l'unica metrica che conta è quanto bene si comporta su nuovi dati mai visti prima.

Per valutare le prestazioni dobbiamo ricordare che il rischio non è calcolabile, quindi usiamo il rischio empirico sul test set. Generalmente si utilizzano \emph{misure di performance} oppure \emph{misura di errori} (loss functon):
\begin{itemize}
    \item \textbf{Loss Function}: la loss function è molto utile nei casi di training, in quanto misura l'errore tra la predizione del modello e il valore reale. Per esempio, nella regressione si può usare l'errore quadratico medio (MSE), mentre nella classificazione si può usare la log-loss.
    \item \textbf{Metriche di performance}: le metriche di performance sono utilizzate per valutare le prestazioni del modello su un dataset di test.
\end{itemize}

\noindent
Una misura di performance $p$ misura l'insieme delle verità $Y$ e delle predizioni corrispondenti $\hat{Y}$ per un certo dataset:
\[
p: \mathcal{Y}^N \times \mathcal{Y}^N \to \mathbb{R}
\]

\noindent
La differenza con il rischio empirico è che questa misura valuta direttamente le prestazioni del modello, senza passare per una funzione di perdita.

\subsection{Validazione Holdout}
La validazione holdout (o single split) è una tecnica semplice per stimare la capacità di generalizzazione di un modello. Consiste nel dividere il dataset in due parti: un training set e un test set. Il modello viene addestrato sul training set e poi valutato sul test set. 

\begin{figure}[htbp]
    \centering
    \includegraphics[width=\textwidth]{images/holdout_validation.png}
    \caption{Validazione Holdout: il dataset viene prima mescolato, poi diviso in training set (70\%) e test set (30\%). Il modello viene addestrato sul training set e valutato sul test set per stimare la capacità di generalizzazione.}
    \label{fig:holdout_validation}
\end{figure}

Questo paradigma è utile per modelli semplici e dataset grandi, in quanto un dataset più piccolo restituirebbe con qualità inferiore le stime delle prestazioni del modello.

\subsection{K-Fold Cross-Validation}
La K-Fold Cross-Validation risolve il problema di dataset piccoli: divide il dataset in diversi sottogruppi (folds) e utilizza ogni sottogruppo sia come test che come training set in diverse iterazioni. In particolare, il dataset viene diviso in \(K\) folds di dimensioni approssimativamente uguali. In ogni iterazione, uno dei folds viene utilizzato come test set, mentre gli altri \(K-1\) folds vengono utilizzati come training set. Questo processo viene ripetuto \(K\) volte, in modo che ogni sottogruppo venga utilizzato come test set una volta.

\begin{figure}
    \centering
    \includegraphics[width=\textwidth]{images/k_fold_cross_validation.png}
    \caption{K-Fold Cross-Validation: il dataset viene diviso in \(K\) folds. In ogni iterazione, un fold viene utilizzato come test set e gli altri \(K-1\) folds come training set. Questo processo viene ripetuto \(K\) volte, e le prestazioni del modello vengono mediate su tutte le iterazioni per ottenere una stima più robusta della capacità di generalizzazione.}
    \label{fig:k_fold_cross_validation}
\end{figure}

Alla fine del processo, le prestazioni del modello vengono mediate su tutte le iterazioni per ottenere una stima più robusta della capacità di generalizzazione del modello. Questo metodo è particolarmente utile quando si lavora con dataset di dimensioni limitate, in quanto consente di utilizzare tutti i dati disponibili sia per l'addestramento che per la valutazione del modello.

Rimangono due problemi:
\begin{itemize}
    \item É una tecnica computazionalmente costosa, in quanto se il training di un modello richiede 1 giorno, questo paradigma ne fa impiegare 4.
    \item Gli iperparametri non vengono ottimizzati durante il processo di training.
\end{itemize}

\subsection{Leave-One-Out Cross-Validation (LOOCV)}
La Leave-One-Out Cross Validation (LOOCV) è una variante estrema della K-Fold Cross-Validation, in cui il numero di folds \(K\) è uguale al numero di esempi nel dataset. In altre parole, in ogni iterazione, un singolo esempio viene utilizzato come test set, mentre tutti gli altri esempi vengono utilizzati come training set. Questo processo viene ripetuto per ogni esempio nel dataset.

\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.8\textwidth]{images/loocv.png}
    \caption{Leave-One-Out Cross-Validation (LOOCV): in ogni iterazione, un singolo esempio viene utilizzato come test set, mentre tutti gli altri esempi vengono utilizzati come training set. Questo processo viene ripetuto per ogni esempio nel dataset.}
    \label{fig:loocv}
\end{figure}

Questo approccio è molto utile quando si lavora con dataset molto piccoli, in quanto consente di utilizzare quasi tutti i dati disponibili per l'addestramento del modello in ogni iterazione. Tuttavia, la LOOCV soffre degli stessi problemi della K-Fold Cross Validation.

\subsection{Ottimizzazione degli iperparametri}
Il problema degli iperaparametri, con questi paradigmi, comunque persiste. 

\paragraph{Grid Search.}
Alcuni algoritmi suggeriscono di utilizzare una \textbf{grid search}: un tipo di ricerca esaustiva sugli iperparametri, in cui si definisce una griglia di valori possibili per ogni iperparametro e si valuta il modello per ogni combinazione di valori nella griglia. Questo approccio risolve indubbiamente l'ottimizzazione degli iperparametri, ma può essere computazionalmente costoso, specialmente con un gran numero di iperparametri e valori da esplorare.

\paragraph{Validation set.}
Un altro approccio è quello di utilizzare un \textbf{validation set}: un sottoinsieme del dataset separato dal training set e dal test set, utilizzato per ottimizzare gli iperparametri. Per esempio, in una Validazione Holdout, si potrebbe dividere il dataset in tre parti: training set, validation set e test set. Il modello viene addestrato sul training set, gli iperparametri vengono ottimizzati sul validation set e infine le prestazioni del modello vengono valutate sul test set.

\begin{figure}
    \centering
    \includegraphics[width=\textwidth]{images/validation_set.png}
    \caption{Utilizzo di un validation set per l'ottimizzazione degli iperparametri: il dataset viene diviso in training set (60\%), validation set (20\%) e test set (20\%). Il modello viene addestrato sul training set, gli iperparametri vengono ottimizzati sul validation set e infine le prestazioni del modello vengono valutate sul test set.}
    \label{fig:validation_set}
\end{figure}

Ci sono tuttavia, combinazioni migliori. Si potrebbe effettuare, per esempio, una K-Fold Cross Validation dove si divide inizialmente il dataset in training/validation set e test set: il training/validation set dopo è sottoposto a una variante di K-Fold Cross Validation per ottimizzare gli iperparametri, mentre il test set viene usato solo alla fine per valutare le prestazioni finali del modello.

\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.9\textwidth]{images/k_fold_with_hyperparameter_tuning.png}
    \caption{K-Fold Cross Validation con ottimizzazione degli iperparametri: il dataset viene diviso in training/validation set e test set. Il training/validation set viene sottoposto a K-Fold Cross Validation per ottimizzare gli iperparametri, mentre il test set viene utilizzato solo alla fine per valutare le prestazioni finali del modello.}
    \label{fig:k_fold_with_hyperparameter_tuning}
\end{figure}
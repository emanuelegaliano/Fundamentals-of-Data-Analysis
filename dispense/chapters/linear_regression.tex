\chapter{Regressione lineare}
La regressione lineare è una tecnica utilizzata per modellare la relazione tra una variabile dipendente e una o più variabili indipendenti. In particolare si rileva utile in tutti quei casi dove si vuole studiare l'effetto di una o più variabili esplicative su una variabile di interesse, oppure si vogliono fare previsioni sui valori futuri della variabile di interesse basandosi sui valori delle variabili esplicative.

\section{Formalizzazione della regressione}
\begin{nicequote}
La regressione lineare mira a studiare \emph{l'associazione} tra una variabile dipendente \(Y\) e una o più variabili indipendenti \(X_1, X_2, \ldots, X_p\) definendo un modello matematico $f$ tale che:
\[
Y = f(X) + \epsilon
\]
Dove \(\epsilon\) rappresenta l'errore o il \textbf{rumore} nel modello. L'obiettivo della regressione è stimare la funzione \(f\) in modo da minimizzare l'errore tra i valori osservati di \(Y\) e i valori predetti dal modello.
\end{nicequote}

Si parla di \emph{rumore} con il valore di \(\epsilon\) per indicare tutte quelle variabili che influenzano \(Y\) ma che non sono state incluse nel modello. Anche perché basti pensare al fatto che $f$ non è un modello deterministico ma probabilistico, quindi non è possibile prevedere con certezza il valore di \(Y\) dato \(X\). 

\section{Regressione lineare semplice}
Si parla di \textbf{regressione lineare semplice} quando si ha una sola variabile indipendente \(X\). In questo caso, il modello di regressione lineare può essere espresso come:
\[
Y = \beta_0 + \beta_1 X + \epsilon
\]

\paragraph{Esempio.} 
Ipotizziamo di voler prendere in considerazione un dataset che contiene informazioni sul numero di ore di studio e i voti ottenuti dagli studenti in un esame. Vogliamo capire se esiste una relazione tra il numero di ore di studio (variabile indipendente \(X\)) e il voto ottenuto (variabile dipendente \(Y\)), con un modello circa così:
\[
\text{voto} = \beta_0 + \beta_1 \cdot \text{ore\_studio} + \epsilon
\]

\subsection{Analogia geometrica con una retta}
In realtà la forma del regressore lineare è, in questo caso, quella di una retta:
\[
y = mx + q \longrightarrow Y = \beta_1 X + \beta_0
\]
Dove \(m\) rappresenta la pendenza della retta (coefficiente angolare) e \(q\) rappresenta l'intercetta con l'asse delle ordinate (coefficiente lineare).

Si può facilmente notare, ricordando un po' di algebra (figura \ref{fig:rette_regressione}), che:
\begin{itemize}
    \item Al variare del parametro $\beta_1$ si forma un fascio di rette proprio\footnote{Un fascio di rette proprio è un insieme di rette che passano tutte per uno stesso punto.} passanti per il punto \((0, \beta_0)\), ovvero l'intercetta con l'asse delle ordinate (a sinistra in figura \ref{fig:rette_regressione}).
    \item Al variare del parametro $\beta_0$ si forma un fascio di rette improprio\footnote{Un fascio di rette improprio è un insieme di rette parallele tra loro.} con pendenza $\beta_1$ (a destra in figura \ref{fig:rette_regressione}).
\end{itemize}

\begin{figure}[htbp]
    \centering
    \includegraphics[width=\textwidth]{images/regression_lines.png}
    \caption{Esempio di rette di regressione al variare del coefficiente: a sinistra varia \(\beta_1\) mantenendo fisso \(\beta_0\), a destra varia \(\beta_0\) mantenendo fisso \(\beta_1\).}
    \label{fig:rette_regressione}
\end{figure}

\subsection{OLS: Ordinary Least Squares}\label{subsec:ols}
Per stimare i coefficienti \(\beta_0\) e \(\beta_1\) del modello "ottimale" bisogna definire cos'è un modello \emph{ottimale}. Si può dire ottimale un modello se predice bene la variabile $Y$ dalla variabile $X$. Per misurare questo, partiamo con il definire il nostro insieme di osservazioni:
\[
\{(x_i, y_i)\}_{i=1}^{n}
\]
Dove \(n\) è il numero di osservazioni, \(x_i\) è il valore della variabile indipendente per l'osservazione \(i\) e \(y_i\) è il valore della variabile dipendente per l'osservazione \(i\). 

\noindent
Sia:
\[
\hat{y}_i = \hat{\beta}_0 + \hat{\beta}_1 x_i
\]
il valore predetto dal modello per l'osservazione \(i\), dove \(\hat{\beta}_0\) e \(\hat{\beta}_1\) sono le stime dei coefficienti. Per ogni punto dati \((x_i, y_i)\), definiamo la deviazione della predizione dal valore corretto $y_i$ come:
\[
e_i = y_i - \hat{y}_i
\]
Questa quantità è chiamata \textbf{residuo} o \textbf{errore di predizione}. Ovviamente questo valore può essere positivo o negativo, a seconda che la predizione sia inferiore o superiore al valore reale. Da questo valore definiamo la \textbf{RSS: Residual Sum of Squares} (Somma dei quadrati dei residui) come:
\[
RSS = \sum_{i=1}^{n} e_i^2 = \sum_{i=1}^{n} (y_i - \hat{y}_i)^2
\]

Con questo valore possiamo avere una misura di un modello ottimale: un modello è tanto più ottimale quanto più piccolo è il valore di RSS. Quindi l'obiettivo diventa minimizzare il valore di RSS trovando i valori ottimali di \(\hat{\beta}_0\) e \(\hat{\beta}_1\):
\[
(\hat{\beta}_0, \hat{\beta}_1) = \arg\min_{\beta_0, \beta_1} \sum_{i=1}^{n} (y_i - (\beta_0 + \beta_1 x_i))^2
\]
che viene chiamata \textbf{loss (o cost) function}, ovvero la funzione che misura l'errore del modello ed è quella da minimizzare. 

Per minimizzare questa funzione si può usare il calcolo differenziale ricordando che per trovare i punti di minimo di una funzione basta trovare i punti in cui la derivata prima è nulla\footnote{Quando la derivata prima è nulla, la crescenza della curva cambia e quindi si ha un punto di massimo o minimo locale.}. Calcoliamo le derivate parziali\footnote{I parametri sono due, non si può derivare rispetto a entrambi contemporaneamente, quindi si calcolano le derivate parziali.} della funzione di loss rispetto a \(\beta_0\) e \(\beta_1\) e le poniamo uguali a zero:
\[
\frac{\partial RSS}{\partial \beta_0} = 0, \qquad \frac{\partial RSS}{\partial \beta_1} = 0
\]

Risolvendo il sistema di equazioni si ottengono le seguenti formule per i coefficienti stimati:
\[
\begin{aligned}
\hat{\beta}_1 &= 
\frac{
\displaystyle\sum_{i=1}^{n} (x_i - \bar{x})(y_i - \bar{y})
}{
\displaystyle\sum_{i=1}^{n} (x_i - \bar{x})^2
}, \qquad
\hat{\beta}_0 &= 
\bar{y} - \hat{\beta}_1 \, \bar{x}
\end{aligned}
\]
Dove \(\bar{x}\) e \(\bar{y}\) sono le medie dei valori di \(X\) e \(Y\) rispettivamente:
\[
\bar{x} = \frac{1}{n} \sum_{i=1}^{n} x_i, \qquad \bar{y} = \frac{1}{n} \sum_{i=1}^{n} y_i
\]

\paragraph{Varianza dalla popolazione ideale.}
Considerando una popolazione ideale dove:
\[
Y = 2x+1
\]
ci si aspetta che i campioni estratti da questa popolazione abbiano coefficienti \(\hat{\beta}_0 \approx 1\) e \(\hat{\beta}_1 \approx 2\), ma a causa del rumore e della variabilità dei dati, i valori stimati possono differire leggermente da questi valori ideali.

\begin{figure}[htbp]
    \centering
    \includegraphics[width=\textwidth]{images/pop_variance_lin_regr.png}
    \caption{Esempio di regressione lineare e variabilità campionaria. Ogni pannello mostra un campione diverso estratto dalla stessa popolazione con la rispettiva retta di regressione stimata (\(\hat{\beta}_0, \hat{\beta}_1\)). La linea blu rappresenta la retta di regressione della popolazione, mentre l’ultimo grafico illustra la media delle rette di regressione campionarie (in rosso), evidenziando come, in media, essa coincida con la vera retta di regressione della popolazione.}
    \label{fig:pop_variance_lin_regr}
\end{figure}

Dalla figura \ref{fig:pop_variance_lin_regr} si può notare come, nonostante la variabilità dei campioni, la media delle rette di regressione campionarie (in rosso nell'ultimo grafico) tende a coincidere con la vera retta di regressione della popolazione (in blu), indicando che il metodo OLS potrebbe avere una varianza bassa, ma rimane comunque uno stimatore\footnote{Ricordiamo che uno stimatore è qualcosa che ci fornisce una stima dei parametri della popolazione basata sui dati campionari.} \emph{unbiased}.

\subsection{Intervalli di confidenza per i coefficienti}
Poiché i coefficienti stimati \(\hat{\beta}_0\) e \(\hat{\beta}_1\) sono calcolati su un campione di dati, possonoe ssere visti come \emph{stimatori} dei veri coefficienti della popolazione. Da qui possiamo calcolare gli \textbf{intervalli di confidenza} e gli \textbf{standard errors} (SE) per questi coefficienti, che ci danno un'idea della precisione delle nostre stime.

\paragraph{Esempio.}
Prendiamo per esempio un dataset così formato:

\begin{table}[H]
\centering
\begin{tabular}{lcccccccc}
\hline
 & displacement & cylinders & horsepower & weight & acceleration & model\_year & origin & mpg \\
\hline
0   & 307.0 & 8 & 130.0 & 3504 & 12.0 & 70 & 1 & 18.0 \\
1   & 350.0 & 8 & 165.0 & 3693 & 11.5 & 70 & 1 & 15.0 \\
2   & 318.0 & 8 & 150.0 & 3436 & 11.0 & 70 & 1 & 18.0 \\
3   & 304.0 & 8 & 150.0 & 3433 & 12.0 & 70 & 1 & 16.0 \\
4   & 302.0 & 8 & 140.0 & 3449 & 10.5 & 70 & 1 & 17.0 \\
\vdots & \vdots & \vdots & \vdots & \vdots & \vdots & \vdots & \vdots & \vdots \\
393 & 140.0 & 4 & 86.0 & 2790 & 15.6 & 82 & 1 & 27.0 \\
394 & 97.0 & 4 & 52.0 & 2130 & 24.6 & 82 & 2 & 44.0 \\
395 & 135.0 & 4 & 84.0 & 2295 & 11.6 & 82 & 1 & 32.0 \\
396 & 120.0 & 4 & 79.0 & 2625 & 18.6 & 82 & 1 & 28.0 \\
397 & 119.0 & 4 & 82.0 & 2720 & 19.4 & 82 & 1 & 31.0 \\
\hline
\multicolumn{9}{c}{298 rows $\times$ 8 columns} \\
\hline
\end{tabular}
\caption{Esempio di dataset con caratteristiche delle automobili}
\label{tab:car_dataset}
\end{table}

\noindent
Da qui creiamo il modello:
\[
\text{mpg} \approx = \beta_0 + \text{horsepower} \cdot \beta_1 + \epsilon
\]
E otteniamo i seguenti risultati:
\begin{table}[htbp]
\centering
\begin{tabular}{lcc}
\textbf{COEFFICIENT} & \textbf{SE} & \textbf{INTERVALLI DI CONFIDENZA} \\
\hline
\(\hat{\beta}_0 = 39.94 \) & 0.717 & [38.53, 41.35] \\
\hline
\(\hat{\beta}_1 = -0.1578 \)  & 0.006 & [-0.17, -0.15] \\
\hline
\end{tabular}
\caption{Stima dello standard error e degli intervalli di confidenza per i coefficienti della regressione}
\label{tab:coef_conf_intervals}
\end{table}

Da gli intervalli di confidenza nella tabella \ref{tab:coef_conf_intervals} si può notare come il coefficiente \(\hat{\beta}_1\) abbia un intervallo di confidenza che non include lo zero, suggerendo che esiste una relazione significativa tra la variabile indipendente (horsepower) e la variabile dipendente (mpg). Inoltre, lo standard error relativamente basso indica che la stima del coefficiente è precisa.

Un tipo di plot a cui si può fare riferimento a uno scatter plot con la retta di regressione e gli intervalli di confidenza intorno alla retta stessa, come mostrato in figura \ref{fig:conf_intervals_plot} (più approfondito nella sotto-sezione \ref{subsec:scatter_ci}).

\begin{figure}
    \centering
    \includegraphics[width=0.7\textwidth]{images/conf_intervals_plot.png}
    \caption{Scatter plot della relazione tra \textit{horsepower} e \textit{mpg} con retta di regressione stimata e relativo intervallo di confidenza.}
    \label{fig:conf_intervals_plot}
\end{figure}

\subsection{Test statistici per la significatività dei coefficienti}
Nell'esempio di prima abbiamo considerato i coefficienti come stimatori della popolazione. Quindi possiamo eseguire dei test statistici per verificare se questi coefficienti sono significativamente diversi da zero. Questo perché per $\beta_1 = 0$ non esiste correlazione tra le variabili $X$ e $Y$ e il regressore diventa:
\[
Y = \beta_0 + \epsilon
\]
che non dipende da $X$. Eseguiamo dunque un test di ipotesi, definendo l'\emph{ipotesi nulla}:
\[
H_0: \text{Non c'è associazione tra } X \text{ e } Y \Leftrightarrow \beta_1 = 0
\]
e l'\emph{ipotesi alternativa}:
\[
H_a: \text{C'è associazione tra } X \text{ e } Y \Leftrightarrow \beta_1 \neq 0
\]

Per testare queste ipotesi, si può utilizzare il test $t$-Student, calcolando il valore t come:
\[
t = \frac{\hat{\beta}_1 - 0}{SE(\hat{\beta}_1)}
\]
Dove \(SE(\hat{\beta}_1)\) è lo standard error del coefficiente stimato. Confrontando il valore t calcolato con la distribuzione $t$-Student con \(n-2\) gradi di libertà (dove \(n\) è il numero di osservazioni), si può determinare il valore p associato. Se il $p$-value è inferiore a una soglia di significatività predefinita (ad esempio, 0.05), si rifiuta l'ipotesi nulla, suggerendo che esiste una relazione significativa tra \(X\) e \(Y\).

Si può eseguire lo stesso test anche per l'intercetta \(\beta_0\), anche se il modello non cambierà molto perché non è un coefficiente di una variabile indipendente. Si possono comunque aggiornare i risultati:

\begin{table}[htbp]
\centering
\begin{tabular}{lcccc}
\textbf{COEFFICIENT} & \textbf{SE} & $\mathbf{t}$ & $\mathbf{P > |t|}$ & \textbf{INTERVALLI DI CONFIDENZA} \\
\hline
\(\hat{\beta}_0 = 39.94 \) & 0.717 & 55.66 & 0.000 & [38.53, 41.35] \\
\hline
\(\hat{\beta}_1 = -0.1578 \)  & 0.006 & -26.30 & 0.000 & [-0.17, -0.15] \\
\hline
\end{tabular}
\caption{Stima dello standard error e degli intervalli di confidenza per i coefficienti della regressione}
\label{tab:coef_conf_intervals_with_t}
\end{table}

\section{Valutazione del modello di regressione}
Dopo aver stimato i coefficienti del modello di regressione, è importante valutare la bontà del modello. I test statistici ci dicono se una relazione statistica significativa esiste, ma non ci dicono quanto bene il modello si \textbf{adatta} ai dati o quanto sono \textbf{accurate} le predizioni del modello, per le quali si usano metriche diverse:
\begin{description}
    \item[Metriche per la bontà del modello.] Queste metriche misurano quanto bene il modello si adatta ai dati osservati. Un esempio comune è il coefficiente di determinazione \(R^2\), che indica la proporzione della varianza nella variabile dipendente che è spiegata dalle variabili indipendenti nel modello. Un valore di \(R^2\) vicino a 1 indica un buon adattamento del modello ai dati.
    \item[Metriche per l'accuratezza delle predizioni.] Queste metriche valutano quanto accurate sono le predizioni del modello sui dati nuovi o non visti. Esempi comuni includono l'Errore Quadratico Medio (MSE) e l'Errore Assoluto Medio (MAE). Queste metriche misurano la differenza tra i valori predetti dal modello e i valori effettivi osservati. Un valore più basso di MSE o MAE indica una maggiore accuratezza delle predizioni del modello.
\end{description}

\paragraph{Residui e RSS.}
Tutte le metriche di regressione sono costruite sui \textbf{residui} (sotto-sezione \ref{subsec:ols}), che rappresentano la differenza tra i valori osservati e i valori predetti dal modello. La somma dei quadrati dei residui (RSS) è una misura comune della bontà del modello, che quantifica la quantità totale di errore nelle predizioni del modello. Un valore più basso di RSS indica un miglior adattamento del modello ai dati.

\subsection{Metriche per la bontà del modello}
Generalmente il nostro obiettivo in queste metriche è capire\footnote{Nel caso della statistica, capire spesso significa \emph{inferire}: ovvero stimare parametri e testare ipotesi} quanto il modello spiega i dati osservati, ovvero quanto della variabilità totale della variabile dipendente \(Y\) è spiegata dalle variabili indipendenti \(X\).

\paragraph{RSE: Residual Standard Error.}
Il Residual Standard Error (RSE) è una misura della dispersione dei residui intorno alla retta di regressione stimata. Si calcola come la radice quadrata della somma dei quadrati dei residui divisa per i gradi di libertà:
\[
RSE = \sqrt{\frac{RSS}{n - p - 1}} = \sqrt{\frac{1}{n - p - 1} \sum_{i=1}^n (y_i - \hat{y}_i)^2}
\]
dove \(n\) è il numero di osservazioni e \(p\) è il numero di variabili indipendenti nel modello, ovvero i "gradi di libertà" del modello, nel costro caso di regressione semplice $p = 1 \Rightarrow$ il denominatore diventa $n-2$.

Poiché l'RSE è una misura della dispersione dei residui, un valore più basso di RSE indica che i residui sono più vicini alla retta di regressione stimata, suggerendo un miglior adattamento del modello ai dati. Inoltre, l'RSE è espresso nelle stesse unità della variabile dipendente \(Y\), rendendolo interpretabile nel contesto del problema di regressione. Ha tuttavia un difetto: non è normalizzato, quindi non permette di confrontare modelli con variabili dipendenti diverse e per capire se è buono bisogna confrontarlo con la scala dei valori di $Y$.

\paragraph{Statistica di $\mathbf{R^2}$.}
La statistica di $R^2$ (coefficiente di determinazione) è una misura assoluta della bontà del modello di regressione. Essa quantifica la proporzione della varianza nella variabile dipendente \(Y\) che è spiegata dalle variabili indipendenti \(X\) nel modello. Per calcolare, dobbiamo prima calcolare la somma totale dei quadrati (TSS), che rappresenta la variabilità totale nella variabile dipendente:
\[
TSS = \sum_{i=1}^{n} (y_i - \bar{y})^2
\]
Dove \(\bar{y}\) è la media dei valori osservati di \(Y\). A questo punto, possiamo calcolare \(R^2\) come:
\[
R^2 = 1 - \frac{RSS}{TSS} = \frac{TSS - RSS}{TSS}
\]

Il valore di \(R^2\) varia tra 0 e 1, dove un valore di 0 indica che il modello non spiega alcuna variabilità nella variabile dipendente, mentre un valore di 1 indica che il modello spiega tutta la variabilità. In generale, un valore più alto di \(R^2\) indica un miglior adattamento del modello ai dati. Tuttavia, è importante notare che un alto valore di \(R^2\) non implica necessariamente che il modello sia appropriato o che le variabili indipendenti siano causalmente correlate alla variabile dipendente, perché questa misura spiega la correlazione ma non la causalità.

\begin{figure}[htbp]
    \centering
    \includegraphics[width=.9\textwidth]{images/r2_example.png}
    \caption{Confronto tra tre modelli di regressione lineare con livelli diversi di varianza del rumore: \textit{High Fit} con $R^2 = 0.97$, \textit{Medium Fit} con $R^2 = 0.78$ e \textit{Low Fit} con $R^2 = 0.50$. All'aumentare della variabilità dei dati attorno alla retta di regressione, la bontà dell'adattamento diminuisce.}
    \label{fig:r2_example}
\end{figure}

\subsection{Grafici di diagnostica}
Le metriche sono unicamente numeriche e quantificano l'errore, senza però spiegare \emph{perché} l'errore esiste. Anche qui, si utilizzano i residui per fare dei grafici di diagnostica che aiutano a capire se il modello è adatto o meno ai dati.

\paragraph{Grafico dei residui vs valori predetti.}
Un grafico comune è il grafico dei residui contro i valori predetti. In questo grafico, i residui \(e_i\) sono tracciati sull'asse delle ordinate contro i valori predetti \(\hat{y}_i\) sull'asse delle ascisse. Questo grafico aiuta a identificare eventuali pattern nei residui che potrebbero indicare problemi con il modello di regressione. Esistono tre casi possibili:
\begin{itemize}
    \item I residui sono distribuiti casualmente intorno allo zero, senza pattern evidente. Questo indica che il modello di regressione è appropriato per i dati.
    \item Si forma una "U" o una $\Cap$ nei residui, suggerendo che il modello di regressione lineare non cattura una relazione non lineare tra \(X\) e \(Y\). In questo caso, potrebbe essere necessario considerare un modello di regressione non lineare.
    \item La dispersione dei residui aumenta o diminuisce con i valori predetti, indicando eteroschedasticità\footnote{L'eteroschedasticità è una forma di non costanza della varianza degli errori.}. Questo suggerisce che la variabilità dei residui non è costante e potrebbe richiedere una trasformazione delle variabili o l'uso di un modello di regressione più robusto.
\end{itemize}

\begin{figure}[htbp]
    \centering
    \includegraphics[width=\textwidth]{images/residuals_vs_fitted.png}
    \caption{Confronto tra diversi pattern nei grafici Residuals vs.\ Fitted. A sinistra un caso ``buono'', in cui i residui formano una nube casuale attorno allo zero, indicando che il modello lineare è appropriato. Al centro un caso ``cattivo'', dove i residui mostrano una chiara struttura a U, segnalando non-linearità. A destra un altro caso ``cattivo'', in cui la dispersione dei residui aumenta con i valori stimati, evidenziando eteroschedasticità (effetto ``fanning'').}
    \label{fig:residuals_vs_fitted}
\end{figure}

\paragraph{Grafici Q-Q (Quantile-Quantile).}
Un altro grafico utile è il grafico Q-Q (Quantile-Quantile), che confronta la distribuzione dei residui con una distribuzione normale teorica. In questo grafico, i quantili dei residui sono tracciati contro i quantili di una distribuzione normale. Se i residui seguono una distribuzione normale, i punti nel grafico Q-Q dovrebbero allinearsi lungo una linea retta diagonale. Eventuali deviazioni significative da questa linea indicano che i residui non seguono una distribuzione normale, suggerendo che il modello di regressione potrebbe non essere appropriato per i dati.

\begin{figure}[htbp]
    \centering
    \includegraphics[width=\textwidth]{images/qq_plot.png}
    \caption{Confronto tra due Q--Q plot dei residui. A sinistra un caso ``buono'', in cui i punti seguono approssimativamente la linea rossa, indicando che i residui possono essere considerati normalmente distribuiti. A destra un caso ``cattivo'', dove le code pesanti (heavy tails) producono deviazioni marcate dalla linea teorica, mostrando che i residui non seguono una distribuzione normale.}
    \label{fig:qq_plot}
\end{figure}

\section{Regressione lineare multivariata}
La regressione lineare multivariata estende il concetto di regressione lineare semplice includendo più variabili indipendenti. In questo caso, il modello di regressione può essere espresso come:
\[
Y = \beta_0 + \beta_1 X_1 + \beta_2 X_2 + \ldots + \beta_p X_p + \epsilon
\]
Dove \(X_1, X_2, \ldots, X_p\) sono le variabili indipendenti e \(p\) è il numero di variabili indipendenti nel modello. I coefficienti \(\beta_1, \beta_2, \ldots, \beta_p\) rappresentano l'effetto di ciascuna variabile indipendente sulla variabile dipendente \(Y\), tenendo conto dell'effetto delle altre variabili indipendenti nel modello.

Questa generalizzazione viene utilizzata quando si vuole studiare l'effetto di più variabili esplicative su una variabile di interesse, permettendo di controllare per l'influenza di altre variabili e di ottenere stime più accurate degli effetti delle singole variabili indipendenti.

Nell'esempio della tabella \ref{tab:car_dataset}, si potrebbe costruire un modello:
\[
mpg \approx = \beta_0 + \beta_1 \cdot \text{horsepower} + \beta_2 \cdot \text{weight} + \beta_3 \cdot \text{model\_year} + \epsilon
\]
e ottenere un valore di $R^2 = 0.808$ rispetto a $R^2 = 0.682$ del modello semplice con solo \textit{horsepower} come variabile indipendente, suggerendo che l'inclusione di più variabili indipendenti migliora la capacità del modello di spiegare la variabilità nella variabile dipendente \(mpg\).

\subsection{Interpretazione geometrica}
In un contesto multivariato, la regressione lineare può essere interpretata geometricamente come la ricerca di un iperpiano che meglio si adatta ai dati in uno spazio a più dimensioni. Ogni variabile indipendente \(X_j\) rappresenta una dimensione nello spazio, e la variabile dipendente \(Y\) rappresenta l'altezza dell'iperpiano in quella posizione.

\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.7\textwidth]{images/multivariate_regression_plane.png}
    \caption{Rappresentazione tridimensionale della regressione lineare multipla sul dataset Auto MPG. I punti indicano i valori osservati di \textit{mpg} in funzione di \textit{horsepower} e \textit{weight}, mentre il piano rosso rappresenta il piano di regressione stimato. Le linee verticali mostrano la distanza tra i valori osservati e quelli predetti dal modello, evidenziando l’errore di regressione.}
    \label{fig:multivariate_regression_plane}
\end{figure}

\subsection{Interpretazione statistica}
Generalmente interpretare statisticamente un modello di regressione lineare multivariata è simile all'interpretazione della regressione lineare semplice. Dato un modello:
\[
Y = \beta_0 + \beta \cdot x
\]
dove \(x\) è un vettore di variabili indipendenti e \(\beta\) è un vettore di coefficienti associati a ciascuna variabile indipendente. Si può interpretare:
\begin{itemize}
    \item Il valore di $\beta_0$ come l'intercetta del modello, ovvero il valore atteso di \(Y\) quando tutte le variabili indipendenti sono uguali a zero.
    \item Il valore di un certo $\beta_i$, con $i \in [1, p]$, come il cambiamento atteso in \(Y\) associato a un aumento unitario della variabile indipendente \(X_i\), mantenendo costanti tutte le altre variabili indipendenti nel modello. Questo permette di isolare l'effetto di ciascuna variabile indipendente sulla variabile dipendente, tenendo conto dell'influenza delle altre variabili nel modello.
\end{itemize}

\paragraph{Come variano i coefficienti al variare delle variabili indipendenti.}
Da questa interpretazione statistica, possiamo dire che: aggiungere nuove variabili al regressore cambia il modo in cui la varianza di \(Y\) viene “spiegata” dai coefficienti. Nel modello semplice
\[
mpg = \beta_0 + \beta_1 \cdot \text{horsepower}
\]
otteniamo ad esempio \(\hat{\beta}_0 \approx 39.94\) e \(\hat{\beta}_1 \approx -0.16\). Quando introduciamo anche il \textit{weight},
\[
mpg = \beta_0 + \beta_1 \cdot \text{horsepower} + \beta_2 \cdot \text{weight},
\]
le stime diventano circa \(\hat{\beta}_0 \approx 45.64\) e \(\hat{\beta}_1 \approx -0.05\). Questo non significa che uno dei due modelli sia “sbagliato”, ma che nel primo caso l'effetto di \textit{horsepower} ingloba anche parte della variabilità dovuta al \textit{weight}\footnote{Ecco perché il test $R^2$ non spiega completamente la bontà del modello.} (e ad altre variabili non osservate), mentre nel secondo caso ogni coefficiente descrive l'effetto della propria variabile a parità delle altre.

\subsection{Stima dei coeffficienti di regressione}
Dato il modello generale di regressione lineare multivariata:
\[
Y = \beta_0 + \beta_1 X_1 + \beta_2 X_2 + \ldots + \beta_p X_p + \epsilon
\]
Possiamo definire la \textbf{funzione di costo} come la somma dei quadrati dei residui (RSS):
\[
RSS(\beta_0, \beta_1, \ldots, \beta_p) = \sum_{i=1}^{n} (y_i - (\beta_0 + \beta_1 x_{i1} + \beta_2 x_{i2} + \ldots + \beta_p x_{ip}))^2
\]
dove \(n\) è il numero di osservazioni, \(y_i\) è il valore osservato della variabile dipendente per l'osservazione \(i\), e \(x_{ij}\) è il valore della variabile indipendente \(X_j\) per l'osservazione \(i\).

I valori $\hat{\beta}_0, \hat{\beta}_1, \ldots, \hat{\beta}_p$ che minimizzano la funzione di costo RSS possono essere trovati risolvendo il seguente sistema di equazioni:
\[
\begin{cases}
y^{(1)} = \hat{\beta}_0 + \hat{\beta}_1 x_1^{(1)} + \hat{\beta}_2 x_2^{(1)} + \ldots + \hat{\beta}_p x_p^{(1)} \\
y^{(2)} = \hat{\beta}_0 + \hat{\beta}_1 x_1^{(2)} + \hat{\beta}_2 x_2^{(2)} + \ldots + \hat{\beta}_p x_p^{(2)} \\
\vdots \\
y^{(n)} = \hat{\beta}_0 + \hat{\beta}_1 x_1^{(n)} + \hat{\beta}_2 x_2^{(n)} + \ldots + \hat{\beta}_p x_p^{(n)}
\end{cases}
\]
trasformato in forma matriciale come:
\[
\mathbf{Y} = \mathbf{X} \boldsymbol{\beta} + \boldsymbol{\epsilon}
\]
Dove:
\begin{itemize}
    \item \(\mathbf{Y}\) è il vettore colonna dei valori osservati della variabile dipendente.
    \item \(\mathbf{X}\) è la matrice delle variabili indipendenti, con una colonna di 1s per l'intercetta, chiamata anche \textbf{design matrix}.
    \item \(\boldsymbol{\beta}\) è il vettore colonna dei coefficienti di regressione da stimare.
    \item \(\boldsymbol{\epsilon}\) è il vettore colonna degli errori.
\end{itemize}

Dalla notazione sopra, possiamo derivare la soluzione per i coefficienti stimati ricordando che la somma dei quadrati dei residui può essere espressa come:
\[
RSS(\beta) = \sum_{i=1}^n (e^{(i)})^2 = e^T e
\]

Minimizzando la funzione di costo, utilizzando sempre il metodo dei \emph{minimi quadrati ordinari} (OLS), otteniamo la seguente formula per i coefficienti stimati:
\[
\hat{\boldsymbol{\beta}} = (\mathbf{X}^T \mathbf{X})^{-1} \mathbf{X}^T \mathbf{Y}
\]

\subsection{F-Test}
Per valutare la significatività complessiva di un modello di regressione lineare multivariata, si può utilizzare il test F. Questo test confronta un modello completo (con tutte le variabili indipendenti) con un modello ridotto (senza alcune o tutte le variabili indipendenti) per determinare se l'inclusione delle variabili indipendenti migliora significativamente l'adattamento del modello ai dati. Nella realtà, non è altro che definire un test d'ipotesi:
\[
H_0: \text{Tutte le variabili indipendenti non hanno effetto su } Y \Leftrightarrow \beta_1 = \beta_2 = \ldots = \beta_p = 0
\]
\[
H_a: \text{Almeno una variabile indipendente ha un effetto su } Y \Leftrightarrow \exists i : \beta_i \neq 0
\]

\subsection{Eliminazione backward delle variabili}
L'eliminazione backward è una tecnica di selezione delle variabili utilizzata nella regressione lineare multivariata per identificare un sottoinsieme ottimale di variabili indipendenti da includere nel modello. Questo metodo inizia con un modello completo che include tutte le variabili indipendenti disponibili e procede rimuovendo iterativamente le variabili meno significative fino a quando non si raggiunge un criterio di arresto predefinito. In particolare, si elimina quella variabile il cui valore p associato al test t è il più alto (quindi la meno significativa) e si ripete il processo finché tutte le variabili rimanenti sono statisticamente significative secondo una soglia di significatività scelta (ad esempio, 0.05).

\subsection{Colinearità e instabilità di OLS}
\begin{nicequote}
    La colinearità si verifica quando due o più variabili indipendenti in un modello di regressione lineare sono altamente correlate tra loro. Questo può portare a problemi nell'interpretazione dei coefficienti di regressione e a instabilità nelle stime dei coefficienti stessi. In presenza di colinearità, le stime dei coefficienti possono diventare molto sensibili a piccole variazioni nei dati, rendendo difficile determinare l'effetto individuale di ciascuna variabile indipendente sulla variabile dipendente. Inoltre, la colinearità può aumentare lo standard error delle stime dei coefficienti, riducendo la precisione delle stime e rendendo più difficile rilevare relazioni significative tra le variabili indipendenti e la variabile dipendente.
\end{nicequote}

\noindent
Prendiamo per esempio la soluzione di OLS per i coefficienti stimati:
\[
\hat{\boldsymbol{\beta}} = (X^T X)^{-1} X^T Y
\]

Se due o più variabili indipendenti sono altamente correlate, la matrice \(X^T X\) può diventare prossima alla \textbf{singolarità}, ovvero una matrice non invertibile poiché le righe sono linearmente \emph{dipendenti} e non è applicabile il teorema della matrice inversa\footnote{Il teorema della matrice inversa afferma che una matrice quadrata è invertibile se e solo se ha rango massimo, ovvero il rango è uguale alla dimensione della matrice.}, rendendo il calcolo dell'inversa numericamente instabile. Questo può portare a stime dei coefficienti che variano notevolmente con piccole modifiche nei dati, rendendo difficile interpretare i risultati del modello di regressione.

Da qui, nasce il problema su OLS: in presenza di colinearità, le stime dei coefficienti possono essere instabili e poco affidabili. Per affrontare questo problema si può fare la computazione della matrice \textbf{pseudo-inversa} di Moore-Penrose \((X^TX)^+\), che permette di calcolare una soluzione approssimata anche quando la matrice non è invertibile. Tuttavia, questa soluzione può ancora essere sensibile alla colinearità e potrebbe non fornire stime affidabili dei coefficienti.

\subsection{Adjusted $\mathbf{R^2}$} 
Nel caso della regressione lineare semplice $R^2 = \pi(x, y)^2$ (con $\pi$ coefficiente di correlazione di Pearson). Nel caso della multivariata, si può scrivere come:
\[
R^2 = \pi(Y, \hat{Y})^2
\]
dove \(\hat{Y}\) sono i valori predetti dal modello di regressione. Tuttavia, un problema con \(R^2\) nella regressione multivariata è che tende ad aumentare con l'aggiunta di più variabili indipendenti, anche se queste variabili non migliorano realmente il modello. Per affrontare questo problema, si utilizza una versione modificata di \(R^2\) chiamata \textbf{Adjusted \(R^2\)} (o \(R^2\) aggiustato), che tiene conto del numero di variabili indipendenti nel modello. Da notare che anche questo è un problema di \textbf{bias-varianza trade-off}, in quanto aggiungere più variabili riduce il bias ma aumenta la varianza.

Si può esprimere come:
\[
\bar{R}^2 = 1 - \left(1 - R^2\right) \frac{m - 1}{m - p - 1}
\]
dove \(m\) è il numero di osservazioni e \(p\) è il numero di variabili indipendenti nel modello. L'Adjusted \(R^2\) penalizza l'aggiunta di variabili indipendenti che non migliorano significativamente il modello, fornendo una misura più accurata della bontà del modello in presenza di più variabili indipendenti.

\section{Predittori qualitativi}
In molti casi, le variabili indipendenti in un modello di regressione possono essere di natura qualitativa (categorica) piuttosto che quantitativa (numerica). Ad esempio, una variabile qualitativa potrebbe rappresentare il genere (maschio/femmina), il colore (rosso/blu/verde) o la categoria di un prodotto (A/B/C). Per includere queste variabili qualitative in un modello di regressione lineare, è necessario convertirle in una forma numerica.

\subsection{Variabili dummy}
Un metodo comune per includere variabili qualitative in un modello di regressione è l'uso di \textbf{variabili dummy}. Le variabili dummy sono variabili binarie (0 o 1) che indicano la presenza o l'assenza di una particolare categoria di una variabile qualitativa. Per una variabile qualitativa con \(k\) categorie, si creano \(k-1\) variabili dummy, dove ciascuna variabile dummy rappresenta una categoria specifica, e la categoria rimanente viene utilizzata come categoria di riferimento (baseline). Continiuamo l'esempio del dataset \ref{tab:car_dataset} e supponiamo di voler introdurre la variabile qualitativa:
\[
\text{fuel\_type[T.gas]} = 
\begin{cases}
1 & \text{se il tipo di carburante è gas} \\
0 & \text{altrimenti}
\end{cases}
\]
da questo poi fittiamo il modello:
\[
mpg = \beta_0 + \beta_1 \cdot \text{horsepower} + \beta_2 \cdot \text{fuel\_type[T.gas]} + \epsilon
\]
ottenendo i seguenti risultati:
\begin{table}[htbp]
\centering
\begin{tabular}{lrrrrrr}
\hline
 & coef & std err & t & P$>|t|$ & [0.025 & 0.975] \\
\hline
Intercept       & 41.2379  & 1.039 & 39.705  & 0.000 & 39.190 & 43.286 \\
fueltype[T.gas] & -2.7658  & 0.918 & -3.013  & 0.003 & -4.576 & -0.956 \\
horsepower      & -0.1295  & 0.007 & -18.758 & 0.000 & -0.143 & -0.116 \\
\hline
\end{tabular}
\caption{Risultati della regressione}
\end{table}
Da qui si può notare come il coefficiente associato alla variabile dummy \textit{fuel\_type[T.gas]} sia negativo, suggerendo che, a parità di \textit{horsepower}, le automobili che utilizzano gas come carburante tendono ad avere un valore di \textit{mpg} inferiore rispetto a quelle che utilizzano altri tipi di carburante (categoria di riferimento). Inoltre, il valore p associato a questo coefficiente è inferiore a 0.05, indicando che l'effetto del tipo di carburante sulla variabile dipendente è statisticamente significativo.

\paragraph{Predittori con più di due categorie}
Considerato un predittore a $n$ categorie, si possono creare $n-1$ variabili dummy per rappresentare le categorie. Per esempio, si potrebbe avere una variabile qualitativa \textit{color} con tre categorie: rosso, blu e verde. Si possono creare due variabili dummy:
\[
\text{color\_red} = 
\begin{cases}
1 & \text{se il colore è rosso} \\
0 & \text{altrimenti}
\end{cases}
\]
\[
\text{color\_blue} = 
\begin{cases}
1 & \text{se il colore è blu} \\
0 & \text{altrimenti}
\end{cases}
\]
La categoria verde viene utilizzata come categoria di riferimento. Includendo queste variabili dummy in un modello di regressione, si può valutare l'effetto di ciascun colore sulla variabile dipendente rispetto alla categoria di riferimento (verde).